{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T00:14:52.638801Z","iopub.execute_input":"2022-01-08T00:14:52.639526Z","iopub.status.idle":"2022-01-08T00:14:52.663054Z","shell.execute_reply.started":"2022-01-08T00:14:52.639427Z","shell.execute_reply":"2022-01-08T00:14:52.662369Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n * 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nprint(boston)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:14:58.494452Z","iopub.execute_input":"2022-01-08T00:14:58.494713Z","iopub.status.idle":"2022-01-08T00:14:59.301376Z","shell.execute_reply.started":"2022-01-08T00:14:58.494684Z","shell.execute_reply":"2022-01-08T00:14:59.300628Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n        4.9800e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n        9.1400e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n        4.0300e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        5.6400e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n        6.4800e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\", 'filename': '/opt/conda/lib/python3.7/site-packages/sklearn/datasets/data/boston_house_prices.csv'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target #target값\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:08.118187Z","iopub.execute_input":"2022-01-08T00:15:08.118792Z","iopub.status.idle":"2022-01-08T00:15:08.164901Z","shell.execute_reply.started":"2022-01-08T00:15:08.118753Z","shell.execute_reply":"2022-01-08T00:15:08.164203Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(506, 14)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  PRICE  \n0     15.3  396.90   4.98   24.0  \n1     17.8  396.90   9.14   21.6  \n2     17.8  392.83   4.03   34.7  \n3     18.7  394.63   2.94   33.4  \n4     18.7  396.90   5.33   36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n* w1은 RM(방의 계수) 피처의 Weight 값\n* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n* bias는 Bias\n* N은 입력 데이터 건수\n![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n","metadata":{}},{"cell_type":"code","source":"# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨. \n# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n'''\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n    # 데이터 건수\n    N = len(target)\n    # 예측 값. \n    predicted = w1 * rm + w2*lstat + bias\n    # 실제값과 예측값의 차이 \n    diff = target - predicted\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산. (위 내용 참고, 미분 취하는 계산임)\n    w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))\n    \n    # Mean Squared Error값을 계산. \n    mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환. \n    return bias_update, w1_update, w2_update, mse_loss\n'''\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate):\n    # rm, lstat의 영향이 가장 커서 이 두 개로만 연습할 것임\n    # w1, w2가 들어오는 이유는 예측값 계산하기 위함\n    \n    N = len(target) # def의 인자로 들어오는 건 전부 array(배열) => sum 속도빠름. loop돌리지 않고 numpy 써서 더 좋음.\n    # 주택가격 전체 건수\n    \n    predicted = w1*rm + w2*lstat + bias\n    # 예측값i\n    \n    diff = target-predicted # array 인수 506개를 한번에 계산\n    # 차이i = 실제값i-예측값i\n    \n    w1_update = -learning_rate*(2/N)*np.dot(rm.T, diff) # np.dot()위해 앞에있는 rm 전치행렬로 바꿈\n    w2_update = -learning_rate*(2/N)*np.dot(lstat.T, diff)\n    # η(dLoss(W)/dw1)\n    # η = learning rate = step\n    \n    bias_update1 = -learning_rate*(2/N)*np.sum(diff)\n    bias_factors = np.ones((N,))\n    bias_update = -learning_rate*(2/N)*np.dot(bias_factors.T, diff)\n    bias_update1\n    bias_update\n    \n    mse_loss = np.mean(np.square(diff)) # 차이의 제곱의 평균\n    \n    return bias_update, w1_update, w2_update, mse_loss # 업데이트할 bias와 weight, mse 리턴","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:16.962590Z","iopub.execute_input":"2022-01-08T00:15:16.962893Z","iopub.status.idle":"2022-01-08T00:15:16.971645Z","shell.execute_reply.started":"2022-01-08T00:15:16.962860Z","shell.execute_reply":"2022-01-08T00:15:16.970884Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 를 적용하는 함수 생성\n* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. ","metadata":{}},{"cell_type":"code","source":"np.zeros((3,)), np.ones((2,))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:20.379756Z","iopub.execute_input":"2022-01-08T00:15:20.380317Z","iopub.status.idle":"2022-01-08T00:15:20.386615Z","shell.execute_reply.started":"2022-01-08T00:15:20.380278Z","shell.execute_reply":"2022-01-08T00:15:20.385680Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(array([0., 0., 0.]), array([1., 1.]))"},"metadata":{}}]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.ones((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0] # 현재 features 안에는 bostonDF에서 rm, lstat 두 피처만 뽑혀있었음\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행 (get_update_weights_value 함수) \n    for i in range(iter_epochs):\n        # weight/bias update 값 계산 \n        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n        # weight/bias의 update 적용 (이전 셀 참고)\n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', loss) #loss값이 계속 줄어드나 확인용\n        \n    return w1, w2, bias","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:23.997853Z","iopub.execute_input":"2022-01-08T00:15:23.998549Z","iopub.status.idle":"2022-01-08T00:15:24.006595Z","shell.execute_reply.started":"2022-01-08T00:15:23.998511Z","shell.execute_reply":"2022-01-08T00:15:24.005797Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 적용\n* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() # default값 안 주면 0~1 사이 값으로 바꿔줌\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']]) #bostonDF에서 rm, lstat 2개의 피처만 가져올것임\nprint(scaled_features[:15])\nprint(bostonDF['PRICE'])\nprint(bostonDF['PRICE'].values)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:34.848464Z","iopub.execute_input":"2022-01-08T00:15:34.848719Z","iopub.status.idle":"2022-01-08T00:15:34.873322Z","shell.execute_reply.started":"2022-01-08T00:15:34.848691Z","shell.execute_reply":"2022-01-08T00:15:34.872596Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[[0.57750527 0.08967991]\n [0.5479977  0.2044702 ]\n [0.6943859  0.06346578]\n [0.65855528 0.03338852]\n [0.68710481 0.09933775]\n [0.54972217 0.09602649]\n [0.4696302  0.29525386]\n [0.50028741 0.48068433]\n [0.39662771 0.7781457 ]\n [0.46809734 0.424117  ]\n [0.53956697 0.51655629]\n [0.46905537 0.31843267]\n [0.44606246 0.38576159]\n [0.45755892 0.18018764]\n [0.48572523 0.23537528]]\n0      24.0\n1      21.6\n2      34.7\n3      33.4\n4      36.2\n       ... \n501    22.4\n502    20.6\n503    23.9\n504    22.0\n505    11.9\nName: PRICE, Length: 506, dtype: float64\n[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n 22.  11.9]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() # default값 안 주면 0~1 사이 값으로 바꿔줌\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']]) #bostonDF에서 rm, lstat 2개의 피처만 가져올것임\n\nw1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\n# bostonDF['PRICE'].values를 해줘서 value값만 들어있는 array배열이 됨\n# iter_epochs=1000 => 1000번돌리는것. \nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)\n'''\n최초 w1, w2, bias: [0.] [0.] [1.]\nEpoch: 1 / 1000\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\nEpoch: 2 / 1000\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\nEpoch: 3 / 1000\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\n~~~\nEpoch: 998 / 1000\nw1: [18.53484608] w2: [-12.98953596] bias: [16.7777374] loss: 38.49782159620822\nEpoch: 999 / 1000\nw1: [18.54107799] w2: [-13.00014391] bias: [16.77768681] loss: 38.48266313399792\nEpoch: 1000 / 1000\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\n\n##### 최종 w1, w2, bias #######\n[18.54730416] [-13.01074165] [16.77763613]\n'''","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:15:49.500727Z","iopub.execute_input":"2022-01-08T00:15:49.501014Z","iopub.status.idle":"2022-01-08T00:15:51.846309Z","shell.execute_reply.started":"2022-01-08T00:15:49.500982Z","shell.execute_reply":"2022-01-08T00:15:51.845599Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"최초 w1, w2, bias: [0.] [0.] [1.]\nEpoch: 1 / 1000\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\nEpoch: 2 / 1000\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\nEpoch: 3 / 1000\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\nEpoch: 4 / 1000\nw1: [0.93124998] w2: [0.39159032] bias: [2.65334123] loss: 476.7031232605375\nEpoch: 5 / 1000\nw1: [1.14941104] w2: [0.48110116] bias: [3.03885015] loss: 455.41666565492966\nEpoch: 6 / 1000\nw1: [1.36202867] w2: [0.56744065] bias: [3.41383227] loss: 435.2718794853261\nEpoch: 7 / 1000\nw1: [1.56925388] w2: [0.65069613] bias: [3.7785751] loss: 416.20726135905875\nEpoch: 8 / 1000\nw1: [1.77123356] w2: [0.73095251] bias: [4.13335831] loss: 398.1646216743121\nEpoch: 9 / 1000\nw1: [1.96811059] w2: [0.8082924] bias: [4.47845392] loss: 381.0889060727257\nEpoch: 10 / 1000\nw1: [2.16002396] w2: [0.88279616] bias: [4.81412652] loss: 364.9280265121499\nEpoch: 11 / 1000\nw1: [2.34710885] w2: [0.95454195] bias: [5.14063347] loss: 349.6327014412229\nEpoch: 12 / 1000\nw1: [2.52949675] w2: [1.02360578] bias: [5.45822511] loss: 335.1563045853578\nEpoch: 13 / 1000\nw1: [2.70731556] w2: [1.0900616] bias: [5.76714493] loss: 321.45472188016\nEpoch: 14 / 1000\nw1: [2.8806897] w2: [1.15398133] bias: [6.06762979] loss: 308.4862161132872\nEpoch: 15 / 1000\nw1: [3.04974017] w2: [1.21543494] bias: [6.35991006] loss: 296.21129885942213\nEpoch: 16 / 1000\nw1: [3.21458467] w2: [1.27449047] bias: [6.64420982] loss: 284.59260931540365\nEpoch: 17 / 1000\nw1: [3.37533771] w2: [1.33121411] bias: [6.92074705] loss: 273.594799663735\nEpoch: 18 / 1000\nw1: [3.53211064] w2: [1.38567023] bias: [7.18973376] loss: 263.1844266127188\nEpoch: 19 / 1000\nw1: [3.6850118] w2: [1.43792147] bias: [7.45137615] loss: 253.32984878042316\nEpoch: 20 / 1000\nw1: [3.83414653] w2: [1.48802872] bias: [7.70587484] loss: 244.00112960761027\nEpoch: 21 / 1000\nw1: [3.97961734] w2: [1.53605124] bias: [7.95342492] loss: 235.16994550172836\nEpoch: 22 / 1000\nw1: [4.1215239] w2: [1.58204665] bias: [8.19421617] loss: 226.80949893011575\nEpoch: 23 / 1000\nw1: [4.2599632] w2: [1.626071] bias: [8.4284332] loss: 218.89443619575283\nEpoch: 24 / 1000\nw1: [4.39502954] w2: [1.66817882] bias: [8.65625556] loss: 211.40076964326613\nEpoch: 25 / 1000\nw1: [4.52681466] w2: [1.70842313] bias: [8.87785789] loss: 204.30580405648178\nEpoch: 26 / 1000\nw1: [4.65540781] w2: [1.74685552] bias: [9.09341009] loss: 197.58806702168727\nEpoch: 27 / 1000\nw1: [4.78089579] w2: [1.78352615] bias: [9.30307739] loss: 191.22724304292856\nEpoch: 28 / 1000\nw1: [4.90336302] w2: [1.81848384] bias: [9.50702053] loss: 185.20411120718256\nEpoch: 29 / 1000\nw1: [5.02289163] w2: [1.85177604] bias: [9.70539583] loss: 179.50048620813746\nEpoch: 30 / 1000\nw1: [5.13956149] w2: [1.88344893] bias: [9.89835537] loss: 174.0991625476179\nEpoch: 31 / 1000\nw1: [5.25345032] w2: [1.91354741] bias: [10.08604706] loss: 168.9838617434446\nEpoch: 32 / 1000\nw1: [5.36463368] w2: [1.94211517] bias: [10.26861478] loss: 164.13918238174\nEpoch: 33 / 1000\nw1: [5.4731851] w2: [1.96919469] bias: [10.44619847] loss: 159.55055286042136\nEpoch: 34 / 1000\nw1: [5.57917608] w2: [1.99482731] bias: [10.61893425] loss: 155.20418667888\nEpoch: 35 / 1000\nw1: [5.68267616] w2: [2.01905321] bias: [10.78695453] loss: 151.0870401366579\nEpoch: 36 / 1000\nw1: [5.78375302] w2: [2.0419115] bias: [10.95038809] loss: 147.18677231132398\nEpoch: 37 / 1000\nw1: [5.88247245] w2: [2.06344021] bias: [11.10936022] loss: 143.49170719274747\nEpoch: 38 / 1000\nw1: [5.97889846] w2: [2.08367632] bias: [11.26399275] loss: 139.99079785758036\nEpoch: 39 / 1000\nw1: [6.07309332] w2: [2.10265582] bias: [11.4144042] loss: 136.6735925740235\nEpoch: 40 / 1000\nw1: [6.16511758] w2: [2.12041371] bias: [11.56070987] loss: 133.53020273287106\nEpoch: 41 / 1000\nw1: [6.25503017] w2: [2.13698402] bias: [11.70302189] loss: 130.55127250643451\nEpoch: 42 / 1000\nw1: [6.34288837] w2: [2.15239986] bias: [11.84144932] loss: 127.72795014224695\nEpoch: 43 / 1000\nw1: [6.42874791] w2: [2.16669344] bias: [11.97609827] loss: 125.05186080346593\nEpoch: 44 / 1000\nw1: [6.51266302] w2: [2.17989608] bias: [12.10707193] loss: 122.51508087263836\nEpoch: 45 / 1000\nw1: [6.59468643] w2: [2.19203825] bias: [12.23447067] loss: 120.11011363998173\nEpoch: 46 / 1000\nw1: [6.67486943] w2: [2.20314959] bias: [12.35839213] loss: 117.82986630158322\nEpoch: 47 / 1000\nw1: [6.7532619] w2: [2.21325891] bias: [12.47893128] loss: 115.66762819693854\nEpoch: 48 / 1000\nw1: [6.82991239] w2: [2.22239427] bias: [12.59618049] loss: 113.61705021905469\nEpoch: 49 / 1000\nw1: [6.90486809] w2: [2.23058292] bias: [12.71022962] loss: 111.6721253339386\nEpoch: 50 / 1000\nw1: [6.97817492] w2: [2.2378514] bias: [12.82116606] loss: 109.82717014969782\nEpoch: 51 / 1000\nw1: [7.04987754] w2: [2.2442255] bias: [12.92907483] loss: 108.07680747870056\nEpoch: 52 / 1000\nw1: [7.1200194] w2: [2.24973032] bias: [13.03403861] loss: 106.41594983928779\nEpoch: 53 / 1000\nw1: [7.18864275] w2: [2.25439027] bias: [13.13613783] loss: 104.83978384641568\nEpoch: 54 / 1000\nw1: [7.25578868] w2: [2.25822908] bias: [13.23545073] loss: 103.34375544333147\nEpoch: 55 / 1000\nw1: [7.3214972] w2: [2.26126986] bias: [13.33205341] loss: 101.9235559289684\nEpoch: 56 / 1000\nw1: [7.38580717] w2: [2.26353506] bias: [13.42601987] loss: 100.57510873818623\nEpoch: 57 / 1000\nw1: [7.44875643] w2: [2.26504653] bias: [13.51742212] loss: 99.29455693429395\nEpoch: 58 / 1000\nw1: [7.51038178] w2: [2.26582552] bias: [13.6063302] loss: 98.07825137547643\nEpoch: 59 / 1000\nw1: [7.57071899] w2: [2.2658927] bias: [13.6928122] loss: 96.9227395188156\nEpoch: 60 / 1000\nw1: [7.62980289] w2: [2.26526817] bias: [13.7769344] loss: 95.82475482755214\nEpoch: 61 / 1000\nw1: [7.68766733] w2: [2.26397149] bias: [13.85876124] loss: 94.78120674908494\nEpoch: 62 / 1000\nw1: [7.74434526] w2: [2.26202167] bias: [13.9383554] loss: 93.7891712329572\nEpoch: 63 / 1000\nw1: [7.7998687] w2: [2.25943721] bias: [14.01577787] loss: 92.8458817597344\nEpoch: 64 / 1000\nw1: [7.85426883] w2: [2.25623611] bias: [14.09108795] loss: 91.94872085324727\nEpoch: 65 / 1000\nw1: [7.90757595] w2: [2.25243586] bias: [14.16434333] loss: 91.09521205015614\nEpoch: 66 / 1000\nw1: [7.95981955] w2: [2.2480535] bias: [14.23560012] loss: 90.2830123021959\nEpoch: 67 / 1000\nw1: [8.01102831] w2: [2.24310557] bias: [14.30491291] loss: 89.50990478778891\nEpoch: 68 / 1000\nw1: [8.06123013] w2: [2.23760818] bias: [14.37233478] loss: 88.77379211096891\nEpoch: 69 / 1000\nw1: [8.11045214] w2: [2.23157699] bias: [14.43791738] loss: 88.07268986674761\nEpoch: 70 / 1000\nw1: [8.15872074] w2: [2.22502725] bias: [14.50171094] loss: 87.40472055318024\nEpoch: 71 / 1000\nw1: [8.20606159] w2: [2.21797376] bias: [14.56376431] loss: 86.76810781144925\nEpoch: 72 / 1000\nw1: [8.25249968] w2: [2.21043094] bias: [14.62412502] loss: 86.16117097629328\nEpoch: 73 / 1000\nw1: [8.2980593] w2: [2.2024128] bias: [14.68283929] loss: 85.58231992005933\nEpoch: 74 / 1000\nw1: [8.34276406] w2: [2.193933] bias: [14.73995209] loss: 85.03005017455823\nEpoch: 75 / 1000\nw1: [8.38663696] w2: [2.18500477] bias: [14.79550715] loss: 84.50293831575453\nEpoch: 76 / 1000\nw1: [8.42970036] w2: [2.17564104] bias: [14.84954701] loss: 83.99963759713033\nEpoch: 77 / 1000\nw1: [8.47197598] w2: [2.16585433] bias: [14.90211305] loss: 83.51887381832368\nEpoch: 78 / 1000\nw1: [8.513485] w2: [2.15565685] bias: [14.95324552] loss: 83.05944141636493\nEpoch: 79 / 1000\nw1: [8.55424798] w2: [2.14506048] bias: [15.00298356] loss: 82.62019976751809\nEpoch: 80 / 1000\nw1: [8.59428494] w2: [2.13407676] bias: [15.05136527] loss: 82.20006968837905\nEpoch: 81 / 1000\nw1: [8.63361534] w2: [2.12271691] bias: [15.09842767] loss: 81.79803012549498\nEpoch: 82 / 1000\nw1: [8.67225811] w2: [2.11099186] bias: [15.14420679] loss: 81.41311502334757\nEpoch: 83 / 1000\nw1: [8.71023168] w2: [2.09891223] bias: [15.18873769] loss: 81.04441036108942\nEpoch: 84 / 1000\nw1: [8.74755397] w2: [2.08648835] bias: [15.23205444] loss: 80.69105134894139\nEpoch: 85 / 1000\nw1: [8.78424239] w2: [2.07373027] bias: [15.2741902] loss: 80.35221977564838\nEpoch: 86 / 1000\nw1: [8.8203139] w2: [2.06064776] bias: [15.31517722] loss: 80.02714149885448\nEpoch: 87 / 1000\nw1: [8.85578499] w2: [2.04725033] bias: [15.35504688] loss: 79.71508407069689\nEpoch: 88 / 1000\nw1: [8.8906717] w2: [2.03354722] bias: [15.39382968] loss: 79.4153544913329\nEpoch: 89 / 1000\nw1: [8.92498962] w2: [2.01954743] bias: [15.4315553] loss: 79.12729708350739\nEpoch: 90 / 1000\nw1: [8.95875394] w2: [2.0052597] bias: [15.46825261] loss: 78.85029148163835\nEpoch: 91 / 1000\nw1: [8.99197941] w2: [1.99069254] bias: [15.5039497] loss: 78.58375072925126\nEpoch: 92 / 1000\nw1: [9.02468039] w2: [1.97585422] bias: [15.53867387] loss: 78.32711947892314\nEpoch: 93 / 1000\nw1: [9.05687085] w2: [1.9607528] bias: [15.57245169] loss: 78.07987228921439\nEpoch: 94 / 1000\nw1: [9.08856437] w2: [1.94539609] bias: [15.605309] loss: 77.8415120133617\nEpoch: 95 / 1000\nw1: [9.11977417] w2: [1.92979171] bias: [15.63727095] loss: 77.6115682747885\nEpoch: 96 / 1000\nw1: [9.1505131] w2: [1.91394707] bias: [15.66836197] loss: 77.38959602475494\nEpoch: 97 / 1000\nw1: [9.18079366] w2: [1.89786936] bias: [15.69860585] loss: 77.17517417772181\nEpoch: 98 / 1000\nw1: [9.21062801] w2: [1.8815656] bias: [15.72802573] loss: 76.96790432024122\nEpoch: 99 / 1000\nw1: [9.240028] w2: [1.86504258] bias: [15.7566441] loss: 76.76740948941219\nEpoch: 100 / 1000\nw1: [9.26900511] w2: [1.84830694] bias: [15.78448284] loss: 76.57333301715327\nEpoch: 101 / 1000\nw1: [9.29757055] w2: [1.83136512] bias: [15.81156325] loss: 76.38533743674583\nEpoch: 102 / 1000\nw1: [9.32573521] w2: [1.81422338] bias: [15.83790603] loss: 76.20310344829278\nEpoch: 103 / 1000\nw1: [9.35350967] w2: [1.79688783] bias: [15.86353133] loss: 76.02632893991846\nEpoch: 104 / 1000\nw1: [9.38090424] w2: [1.77936439] bias: [15.88845873] loss: 75.85472806170637\nEpoch: 105 / 1000\nw1: [9.40792893] w2: [1.76165882] bias: [15.91270728] loss: 75.68803034953318\nEpoch: 106 / 1000\nw1: [9.4345935] w2: [1.74377674] bias: [15.93629554] loss: 75.52597989611056\nEpoch: 107 / 1000\nw1: [9.46090743] w2: [1.7257236] bias: [15.95924151] loss: 75.36833456669144\nEpoch: 108 / 1000\nw1: [9.48687994] w2: [1.70750469] bias: [15.98156274] loss: 75.2148652570338\nEpoch: 109 / 1000\nw1: [9.51252] w2: [1.68912519] bias: [16.0032763] loss: 75.06535519134565\nEpoch: 110 / 1000\nw1: [9.53783634] w2: [1.67059011] bias: [16.02439876] loss: 74.91959925805673\nEpoch: 111 / 1000\nw1: [9.56283744] w2: [1.65190433] bias: [16.04494626] loss: 74.77740338137866\nEpoch: 112 / 1000\nw1: [9.58753157] w2: [1.63307258] bias: [16.06493452] loss: 74.63858392672564\nEpoch: 113 / 1000\nw1: [9.61192676] w2: [1.6140995] bias: [16.08437878] loss: 74.50296713817104\nEpoch: 114 / 1000\nw1: [9.63603082] w2: [1.59498957] bias: [16.10329391] loss: 74.37038860621374\nEpoch: 115 / 1000\nw1: [9.65985134] w2: [1.57574714] bias: [16.12169436] loss: 74.24069276422141\nEpoch: 116 / 1000\nw1: [9.68339573] w2: [1.55637648] bias: [16.13959417] loss: 74.11373241200526\nEpoch: 117 / 1000\nw1: [9.70667116] w2: [1.53688171] bias: [16.15700701] loss: 73.98936826506468\nEpoch: 118 / 1000\nw1: [9.72968465] w2: [1.51726684] bias: [16.17394618] loss: 73.86746852811852\nEpoch: 119 / 1000\nw1: [9.75244299] w2: [1.49753578] bias: [16.1904246] loss: 73.74790849161448\nEpoch: 120 / 1000\nw1: [9.7749528] w2: [1.47769233] bias: [16.20645486] loss: 73.63057014997872\nEpoch: 121 / 1000\nw1: [9.79722052] w2: [1.45774018] bias: [16.2220492] loss: 73.51534184043389\nEpoch: 122 / 1000\nw1: [9.81925241] w2: [1.43768293] bias: [16.2372195] loss: 73.40211790127799\nEpoch: 123 / 1000\nw1: [9.84105457] w2: [1.41752407] bias: [16.25197736] loss: 73.29079834857474\nEpoch: 124 / 1000\nw1: [9.86263292] w2: [1.39726699] bias: [16.26633401] loss: 73.18128857026421\nEpoch: 125 / 1000\nw1: [9.88399322] w2: [1.37691501] bias: [16.28030043] loss: 73.0734990367546\nEpoch: 126 / 1000\nw1: [9.90514108] w2: [1.35647133] bias: [16.29388726] loss: 72.9673450271072\nEpoch: 127 / 1000\nw1: [9.92608196] w2: [1.33593907] bias: [16.30710486] loss: 72.86274636997472\nEpoch: 128 / 1000\nw1: [9.94682116] w2: [1.31532129] bias: [16.31996332] loss: 72.75962719849768\nEpoch: 129 / 1000\nw1: [9.96736385] w2: [1.29462093] bias: [16.33247243] loss: 72.65791571840701\nEpoch: 130 / 1000\nw1: [9.98771504] w2: [1.27384088] bias: [16.34464173] loss: 72.55754398862112\nEpoch: 131 / 1000\nw1: [10.00787962] w2: [1.25298391] bias: [16.3564805] loss: 72.45844771366467\nEpoch: 132 / 1000\nw1: [10.02786234] w2: [1.23205276] bias: [16.36799776] loss: 72.36056604727145\nEpoch: 133 / 1000\nw1: [10.04766782] w2: [1.21105006] bias: [16.37920228] loss: 72.26384140656936\nEpoch: 134 / 1000\nw1: [10.06730056] w2: [1.18997839] bias: [16.3901026] loss: 72.16821929627682\nEpoch: 135 / 1000\nw1: [10.08676492] w2: [1.16884025] bias: [16.40070703] loss: 72.07364814237184\nEpoch: 136 / 1000\nw1: [10.10606517] w2: [1.14763806] bias: [16.41102363] loss: 71.98007913472244\nEpoch: 137 / 1000\nw1: [10.12520544] w2: [1.12637419] bias: [16.42106027] loss: 71.88746607819654\nEpoch: 138 / 1000\nw1: [10.14418976] w2: [1.10505095] bias: [16.43082459] loss: 71.79576525179384\nEpoch: 139 / 1000\nw1: [10.16302204] w2: [1.08367056] bias: [16.44032401] loss: 71.70493527536757\nEpoch: 140 / 1000\nw1: [10.18170609] w2: [1.0622352] bias: [16.44956577] loss: 71.61493698352743\nEpoch: 141 / 1000\nw1: [10.20024562] w2: [1.04074698] bias: [16.45855689] loss: 71.52573330633642\nEpoch: 142 / 1000\nw1: [10.21864423] w2: [1.01920796] bias: [16.46730423] loss: 71.4372891564357\nEpoch: 143 / 1000\nw1: [10.23690542] w2: [0.99762014] bias: [16.47581443] loss: 71.34957132225128\nEpoch: 144 / 1000\nw1: [10.25503262] w2: [0.97598546] bias: [16.48409395] loss: 71.26254836695449\nEpoch: 145 / 1000\nw1: [10.27302913] w2: [0.95430581] bias: [16.49214911] loss: 71.17619053286658\nEpoch: 146 / 1000\nw1: [10.29089819] w2: [0.93258303] bias: [16.49998601] loss: 71.09046965101383\nEpoch: 147 / 1000\nw1: [10.30864293] w2: [0.91081891] bias: [16.50761062] loss: 71.00535905555587\nEpoch: 148 / 1000\nw1: [10.32626641] w2: [0.88901518] bias: [16.51502873] loss: 70.92083350282455\nEpoch: 149 / 1000\nw1: [10.34377161] w2: [0.86717353] bias: [16.52224597] loss: 70.83686909472493\nEpoch: 150 / 1000\nw1: [10.3611614] w2: [0.84529561] bias: [16.52926782] loss: 70.75344320626338\nEpoch: 151 / 1000\nw1: [10.37843861] w2: [0.82338301] bias: [16.53609961] loss: 70.67053441698039\nEpoch: 152 / 1000\nw1: [10.39560596] w2: [0.80143729] bias: [16.54274653] loss: 70.58812244607789\nEpoch: 153 / 1000\nw1: [10.41266613] w2: [0.77945995] bias: [16.54921363] loss: 70.50618809104164\nEpoch: 154 / 1000\nw1: [10.42962169] w2: [0.75745247] bias: [16.5555058] loss: 70.42471316957067\nEpoch: 155 / 1000\nw1: [10.44647518] w2: [0.73541627] bias: [16.56162782] loss: 70.34368046463578\nEpoch: 156 / 1000\nw1: [10.46322902] w2: [0.71335275] bias: [16.56758434] loss: 70.26307367249775\nEpoch: 157 / 1000\nw1: [10.47988562] w2: [0.69126324] bias: [16.57337986] loss: 70.18287735352673\nEpoch: 158 / 1000\nw1: [10.49644729] w2: [0.66914907] bias: [16.57901878] loss: 70.10307688567114\nEpoch: 159 / 1000\nw1: [10.51291628] w2: [0.6470115] bias: [16.58450536] loss: 70.02365842043385\nEpoch: 160 / 1000\nw1: [10.52929479] w2: [0.62485178] bias: [16.58984377] loss: 69.94460884122006\nEpoch: 161 / 1000\nw1: [10.54558496] w2: [0.60267112] bias: [16.59503805] loss: 69.86591572392965\nEpoch: 162 / 1000\nw1: [10.56178885] w2: [0.58047068] bias: [16.60009213] loss: 69.78756729967246\nEpoch: 163 / 1000\nw1: [10.5779085] w2: [0.55825162] bias: [16.60500982] loss: 69.70955241949265\nEpoch: 164 / 1000\nw1: [10.59394585] w2: [0.53601502] bias: [16.60979486] loss: 69.63186052099356\nEpoch: 165 / 1000\nw1: [10.60990284] w2: [0.51376198] bias: [16.61445085] loss: 69.55448159676082\nEpoch: 166 / 1000\nw1: [10.62578132] w2: [0.49149354] bias: [16.61898132] loss: 69.47740616448675\nEpoch: 167 / 1000\nw1: [10.64158309] w2: [0.46921071] bias: [16.62338969] loss: 69.40062523870435\nEpoch: 168 / 1000\nw1: [10.65730992] w2: [0.44691449] bias: [16.62767928] loss: 69.3241303040444\nEpoch: 169 / 1000\nw1: [10.67296352] w2: [0.42460584] bias: [16.63185334] loss: 69.24791328993315\nEpoch: 170 / 1000\nw1: [10.68854556] w2: [0.4022857] bias: [16.63591502] loss: 69.17196654665355\nEpoch: 171 / 1000\nw1: [10.70405765] w2: [0.37995496] bias: [16.63986738] loss: 69.09628282269593\nEpoch: 172 / 1000\nw1: [10.71950138] w2: [0.35761453] bias: [16.64371341] loss: 69.02085524332935\nEpoch: 173 / 1000\nw1: [10.73487828] w2: [0.33526525] bias: [16.64745599] loss: 68.94567729032708\nEpoch: 174 / 1000\nw1: [10.75018984] w2: [0.31290796] bias: [16.65109795] loss: 68.87074278278475\nEpoch: 175 / 1000\nw1: [10.76543752] w2: [0.29054348] bias: [16.65464203] loss: 68.7960458589718\nEpoch: 176 / 1000\nw1: [10.78062272] w2: [0.26817258] bias: [16.6580909] loss: 68.72158095916072\nEpoch: 177 / 1000\nw1: [10.79574683] w2: [0.24579605] bias: [16.66144716] loss: 68.64734280938151\nEpoch: 178 / 1000\nw1: [10.81081118] w2: [0.22341462] bias: [16.66471332] loss: 68.57332640605132\nEpoch: 179 / 1000\nw1: [10.82581706] w2: [0.20102902] bias: [16.66789185] loss: 68.49952700143216\nEpoch: 180 / 1000\nw1: [10.84076576] w2: [0.17863995] bias: [16.67098513] loss: 68.42594008987219\nEpoch: 181 / 1000\nw1: [10.85565849] w2: [0.15624809] bias: [16.67399549] loss: 68.35256139478831\nEpoch: 182 / 1000\nw1: [10.87049646] w2: [0.13385412] bias: [16.67692518] loss: 68.2793868563499\nEpoch: 183 / 1000\nw1: [10.88528084] w2: [0.11145868] bias: [16.6797764] loss: 68.20641261982634\nEpoch: 184 / 1000\nw1: [10.90001275] w2: [0.08906239] bias: [16.68255129] loss: 68.13363502456207\nEpoch: 185 / 1000\nw1: [10.9146933] w2: [0.06666586] bias: [16.68525193] loss: 68.06105059354589\nEpoch: 186 / 1000\nw1: [10.92932357] w2: [0.0442697] bias: [16.68788033] loss: 67.98865602354196\nEpoch: 187 / 1000\nw1: [10.9439046] w2: [0.02187447] bias: [16.69043848] loss: 67.91644817575282\nEpoch: 188 / 1000\nw1: [10.95843741] w2: [-0.00051926] bias: [16.69292828] loss: 67.84442406698494\nEpoch: 189 / 1000\nw1: [10.97292298] w2: [-0.02291096] bias: [16.69535159] loss: 67.77258086129083\nEpoch: 190 / 1000\nw1: [10.98736229] w2: [-0.04530008] bias: [16.69771023] loss: 67.70091586206088\nEpoch: 191 / 1000\nw1: [11.00175626] w2: [-0.06768612] bias: [16.70000595] loss: 67.62942650454166\nEpoch: 192 / 1000\nw1: [11.0161058] w2: [-0.09006859] bias: [16.70224047] loss: 67.55811034875691\nEpoch: 193 / 1000\nw1: [11.03041181] w2: [-0.11244698] bias: [16.70441545] loss: 67.4869650728103\nEpoch: 194 / 1000\nw1: [11.04467515] w2: [-0.13482084] bias: [16.70653251] loss: 67.41598846654864\nEpoch: 195 / 1000\nw1: [11.05889665] w2: [-0.1571897] bias: [16.70859324] loss: 67.34517842556664\nEpoch: 196 / 1000\nw1: [11.07307713] w2: [-0.17955313] bias: [16.71059916] loss: 67.27453294553469\nEpoch: 197 / 1000\nw1: [11.0872174] w2: [-0.20191068] bias: [16.71255176] loss: 67.20405011683204\nEpoch: 198 / 1000\nw1: [11.10131821] w2: [-0.22426194] bias: [16.71445251] loss: 67.13372811946925\nEpoch: 199 / 1000\nw1: [11.11538033] w2: [-0.2466065] bias: [16.71630279] loss: 67.0635652182841\nEpoch: 200 / 1000\nw1: [11.12940449] w2: [-0.26894397] bias: [16.718104] loss: 66.99355975839633\nEpoch: 201 / 1000\nw1: [11.14339139] w2: [-0.29127396] bias: [16.71985746] loss: 66.92371016090718\nEpoch: 202 / 1000\nw1: [11.15734174] w2: [-0.31359609] bias: [16.72156448] loss: 66.85401491883069\nEpoch: 203 / 1000\nw1: [11.17125621] w2: [-0.33591001] bias: [16.72322631] loss: 66.78447259324402\nEpoch: 204 / 1000\nw1: [11.18513545] w2: [-0.35821536] bias: [16.72484419] loss: 66.71508180964528\nEpoch: 205 / 1000\nw1: [11.19898011] w2: [-0.3805118] bias: [16.72641931] loss: 66.64584125450754\nEpoch: 206 / 1000\nw1: [11.2127908] w2: [-0.402799] bias: [16.72795283] loss: 66.57674967201841\nEpoch: 207 / 1000\nw1: [11.22656813] w2: [-0.42507663] bias: [16.72944588] loss: 66.50780586099518\nEpoch: 208 / 1000\nw1: [11.2403127] w2: [-0.44734438] bias: [16.73089957] loss: 66.43900867196629\nEpoch: 209 / 1000\nw1: [11.25402507] w2: [-0.46960195] bias: [16.73231495] loss: 66.37035700440994\nEpoch: 210 / 1000\nw1: [11.2677058] w2: [-0.49184905] bias: [16.73369309] loss: 66.30184980414137\nEpoch: 211 / 1000\nw1: [11.28135543] w2: [-0.51408538] bias: [16.73503498] loss: 66.23348606084087\nEpoch: 212 / 1000\nw1: [11.2949745] w2: [-0.53631067] bias: [16.73634161] loss: 66.16526480571513\nEpoch: 213 / 1000\nw1: [11.30856352] w2: [-0.55852464] bias: [16.73761393] loss: 66.09718510928428\nEpoch: 214 / 1000\nw1: [11.32212299] w2: [-0.58072704] bias: [16.73885289] loss: 66.02924607928819\nEpoch: 215 / 1000\nw1: [11.33565339] w2: [-0.60291761] bias: [16.74005938] loss: 65.96144685870571\nEpoch: 216 / 1000\nw1: [11.34915521] w2: [-0.6250961] bias: [16.74123429] loss: 65.89378662388037\nEpoch: 217 / 1000\nw1: [11.36262889] w2: [-0.64726228] bias: [16.74237848] loss: 65.82626458274714\nEpoch: 218 / 1000\nw1: [11.3760749] w2: [-0.6694159] bias: [16.74349277] loss: 65.75887997315465\nEpoch: 219 / 1000\nw1: [11.38949367] w2: [-0.69155676] bias: [16.74457798] loss: 65.69163206127773\nEpoch: 220 / 1000\nw1: [11.40288562] w2: [-0.71368461] bias: [16.7456349] loss: 65.62452014011548\nEpoch: 221 / 1000\nw1: [11.41625117] w2: [-0.73579927] bias: [16.74666429] loss: 65.55754352807024\nEpoch: 222 / 1000\nw1: [11.42959072] w2: [-0.75790051] bias: [16.74766691] loss: 65.49070156760305\nEpoch: 223 / 1000\nw1: [11.44290467] w2: [-0.77998813] bias: [16.74864348] loss: 65.42399362396145\nEpoch: 224 / 1000\nw1: [11.45619339] w2: [-0.80206196] bias: [16.74959469] loss: 65.35741908397587\nEpoch: 225 / 1000\nw1: [11.46945727] w2: [-0.82412178] bias: [16.75052125] loss: 65.29097735492081\nEpoch: 226 / 1000\nw1: [11.48269665] w2: [-0.84616743] bias: [16.75142382] loss: 65.2246678634373\nEpoch: 227 / 1000\nw1: [11.4959119] w2: [-0.86819873] bias: [16.75230305] loss: 65.1584900545133\nEpoch: 228 / 1000\nw1: [11.50910336] w2: [-0.8902155] bias: [16.75315957] loss: 65.09244339051924\nEpoch: 229 / 1000\nw1: [11.52227136] w2: [-0.91221757] bias: [16.753994] loss: 65.02652735029521\nEpoch: 230 / 1000\nw1: [11.53541623] w2: [-0.9342048] bias: [16.75480693] loss: 64.9607414282874\nEpoch: 231 / 1000\nw1: [11.54853828] w2: [-0.95617701] bias: [16.75559895] loss: 64.89508513373102\nEpoch: 232 / 1000\nw1: [11.56163782] w2: [-0.97813407] bias: [16.75637061] loss: 64.82955798987723\nEpoch: 233 / 1000\nw1: [11.57471516] w2: [-1.00007581] bias: [16.75712249] loss: 64.76415953326165\nEpoch: 234 / 1000\nw1: [11.58777058] w2: [-1.02200211] bias: [16.7578551] loss: 64.69888931301207\nEpoch: 235 / 1000\nw1: [11.60080437] w2: [-1.04391281] bias: [16.75856897] loss: 64.63374689019388\nEpoch: 236 / 1000\nw1: [11.61381681] w2: [-1.0658078] bias: [16.7592646] loss: 64.56873183719011\nEpoch: 237 / 1000\nw1: [11.62680817] w2: [-1.08768693] bias: [16.7599425] loss: 64.50384373711536\nEpoch: 238 / 1000\nw1: [11.63977871] w2: [-1.10955009] bias: [16.76060313] loss: 64.4390821832609\nEpoch: 239 / 1000\nw1: [11.65272868] w2: [-1.13139715] bias: [16.76124696] loss: 64.37444677856983\nEpoch: 240 / 1000\nw1: [11.66565834] w2: [-1.15322799] bias: [16.76187446] loss: 64.30993713514043\nEpoch: 241 / 1000\nw1: [11.67856793] w2: [-1.17504251] bias: [16.76248605] loss: 64.24555287375615\nEpoch: 242 / 1000\nw1: [11.69145768] w2: [-1.19684059] bias: [16.76308217] loss: 64.18129362344095\nEpoch: 243 / 1000\nw1: [11.70432783] w2: [-1.21862212] bias: [16.76366323] loss: 64.11715902103856\nEpoch: 244 / 1000\nw1: [11.7171786] w2: [-1.240387] bias: [16.76422965] loss: 64.0531487108143\nEpoch: 245 / 1000\nw1: [11.7300102] w2: [-1.26213513] bias: [16.76478181] loss: 63.98926234407838\nEpoch: 246 / 1000\nw1: [11.74282286] w2: [-1.28386641] bias: [16.7653201] loss: 63.92549957882945\nEpoch: 247 / 1000\nw1: [11.75561677] w2: [-1.30558076] bias: [16.76584489] loss: 63.86186007941725\nEpoch: 248 / 1000\nw1: [11.76839215] w2: [-1.32727808] bias: [16.76635656] loss: 63.798343516223404\nEpoch: 249 / 1000\nw1: [11.78114918] w2: [-1.34895828] bias: [16.76685544] loss: 63.734949565359436\nEpoch: 250 / 1000\nw1: [11.79388806] w2: [-1.37062128] bias: [16.76734189] loss: 63.671677908380985\nEpoch: 251 / 1000\nw1: [11.80660898] w2: [-1.392267] bias: [16.76781623] loss: 63.60852823201734\nEpoch: 252 / 1000\nw1: [11.81931212] w2: [-1.41389536] bias: [16.7682788] loss: 63.54550022791546\nEpoch: 253 / 1000\nw1: [11.83199764] w2: [-1.43550628] bias: [16.76872992] loss: 63.482593592397954\nEpoch: 254 / 1000\nw1: [11.84466574] w2: [-1.45709968] bias: [16.76916988] loss: 63.419808026233746\nEpoch: 255 / 1000\nw1: [11.85731657] w2: [-1.47867551] bias: [16.76959898] loss: 63.35714323442133\nEpoch: 256 / 1000\nw1: [11.8699503] w2: [-1.50023368] bias: [16.77001753] loss: 63.29459892598353\nEpoch: 257 / 1000\nw1: [11.88256709] w2: [-1.52177414] bias: [16.7704258] loss: 63.23217481377323\nEpoch: 258 / 1000\nw1: [11.8951671] w2: [-1.54329682] bias: [16.77082407] loss: 63.16987061428966\nEpoch: 259 / 1000\nw1: [11.90775047] w2: [-1.56480166] bias: [16.7712126] loss: 63.107686047504494\nEpoch: 260 / 1000\nw1: [11.92031736] w2: [-1.58628859] bias: [16.77159167] loss: 63.04562083669712\nEpoch: 261 / 1000\nw1: [11.9328679] w2: [-1.60775757] bias: [16.77196151] loss: 62.98367470829899\nEpoch: 262 / 1000\nw1: [11.94540224] w2: [-1.62920853] bias: [16.77232238] loss: 62.92184739174619\nEpoch: 263 / 1000\nw1: [11.95792052] w2: [-1.65064142] bias: [16.77267451] loss: 62.86013861933982\nEpoch: 264 / 1000\nw1: [11.97042287] w2: [-1.6720562] bias: [16.77301815] loss: 62.798548126114085\nEpoch: 265 / 1000\nw1: [11.98290942] w2: [-1.69345281] bias: [16.77335351] loss: 62.73707564971119\nEpoch: 266 / 1000\nw1: [11.9953803] w2: [-1.7148312] bias: [16.77368083] loss: 62.675720930263154\nEpoch: 267 / 1000\nw1: [12.00783562] w2: [-1.73619134] bias: [16.7740003] loss: 62.61448371027983\nEpoch: 268 / 1000\nw1: [12.02027552] w2: [-1.75753317] bias: [16.77431215] loss: 62.553363734542906\nEpoch: 269 / 1000\nw1: [12.03270011] w2: [-1.77885665] bias: [16.77461658] loss: 62.492360750005645\nEpoch: 270 / 1000\nw1: [12.04510949] w2: [-1.80016175] bias: [16.77491377] loss: 62.431474505697935\nEpoch: 271 / 1000\nw1: [12.0575038] w2: [-1.82144842] bias: [16.77520394] loss: 62.370704752636435\nEpoch: 272 / 1000\nw1: [12.06988313] w2: [-1.84271663] bias: [16.77548725] loss: 62.310051243739544\nEpoch: 273 / 1000\nw1: [12.08224758] w2: [-1.86396634] bias: [16.7757639] loss: 62.249513733746795\nEpoch: 274 / 1000\nw1: [12.09459728] w2: [-1.88519753] bias: [16.77603406] loss: 62.18909197914273\nEpoch: 275 / 1000\nw1: [12.1069323] w2: [-1.90641014] bias: [16.77629791] loss: 62.12878573808469\nEpoch: 276 / 1000\nw1: [12.11925276] w2: [-1.92760416] bias: [16.7765556] loss: 62.06859477033449\nEpoch: 277 / 1000\nw1: [12.13155876] w2: [-1.94877955] bias: [16.77680731] loss: 62.008518837193805\nEpoch: 278 / 1000\nw1: [12.14385037] w2: [-1.96993629] bias: [16.7770532] loss: 61.948557701442844\nEpoch: 279 / 1000\nw1: [12.15612771] w2: [-1.99107435] bias: [16.77729341] loss: 61.88871112728251\nEpoch: 280 / 1000\nw1: [12.16839085] w2: [-2.01219369] bias: [16.77752809] loss: 61.828978880279365\nEpoch: 281 / 1000\nw1: [12.18063988] w2: [-2.03329431] bias: [16.7777574] loss: 61.76936072731376\nEpoch: 282 / 1000\nw1: [12.19287489] w2: [-2.05437616] bias: [16.77798147] loss: 61.709856436530615\nEpoch: 283 / 1000\nw1: [12.20509596] w2: [-2.07543924] bias: [16.77820045] loss: 61.65046577729271\nEpoch: 284 / 1000\nw1: [12.21730317] w2: [-2.09648352] bias: [16.77841446] loss: 61.5911885201367\nEpoch: 285 / 1000\nw1: [12.22949661] w2: [-2.11750898] bias: [16.77862364] loss: 61.53202443673122\nEpoch: 286 / 1000\nw1: [12.24167634] w2: [-2.13851559] bias: [16.77882811] loss: 61.47297329983731\nEpoch: 287 / 1000\nw1: [12.25384245] w2: [-2.15950335] bias: [16.77902801] loss: 61.41403488327097\nEpoch: 288 / 1000\nw1: [12.26599501] w2: [-2.18047223] bias: [16.77922343] loss: 61.35520896186758\nEpoch: 289 / 1000\nw1: [12.27813409] w2: [-2.20142222] bias: [16.77941452] loss: 61.2964953114483\nEpoch: 290 / 1000\nw1: [12.29025976] w2: [-2.22235331] bias: [16.77960137] loss: 61.237893708788185\nEpoch: 291 / 1000\nw1: [12.30237209] w2: [-2.24326547] bias: [16.7797841] loss: 61.17940393158601\nEpoch: 292 / 1000\nw1: [12.31447114] w2: [-2.2641587] bias: [16.77996282] loss: 61.12102575843564\nEpoch: 293 / 1000\nw1: [12.32655699] w2: [-2.28503298] bias: [16.78013763] loss: 61.062758968798946\nEpoch: 294 / 1000\nw1: [12.3386297] w2: [-2.3058883] bias: [16.78030863] loss: 61.0046033429801\nEpoch: 295 / 1000\nw1: [12.35068932] w2: [-2.32672465] bias: [16.78047593] loss: 60.946558662101225\nEpoch: 296 / 1000\nw1: [12.36273593] w2: [-2.34754203] bias: [16.78063961] loss: 60.88862470807938\nEpoch: 297 / 1000\nw1: [12.37476957] w2: [-2.36834041] bias: [16.78079977] loss: 60.83080126360464\nEpoch: 298 / 1000\nw1: [12.38679031] w2: [-2.38911979] bias: [16.78095651] loss: 60.77308811211937\nEpoch: 299 / 1000\nw1: [12.39879821] w2: [-2.40988017] bias: [16.78110991] loss: 60.71548503779861\nEpoch: 300 / 1000\nw1: [12.41079332] w2: [-2.43062154] bias: [16.78126006] loss: 60.657991825531376\nEpoch: 301 / 1000\nw1: [12.4227757] w2: [-2.45134389] bias: [16.78140704] loss: 60.60060826090307\nEpoch: 302 / 1000\nw1: [12.43474539] w2: [-2.47204721] bias: [16.78155094] loss: 60.543334130178685\nEpoch: 303 / 1000\nw1: [12.44670246] w2: [-2.4927315] bias: [16.78169183] loss: 60.48616922028689\nEpoch: 304 / 1000\nw1: [12.45864695] w2: [-2.51339676] bias: [16.78182979] loss: 60.429113318805\nEpoch: 305 / 1000\nw1: [12.47057891] w2: [-2.53404298] bias: [16.78196489] loss: 60.37216621394464\nEpoch: 306 / 1000\nw1: [12.48249839] w2: [-2.55467016] bias: [16.78209721] loss: 60.31532769453818\nEpoch: 307 / 1000\nw1: [12.49440545] w2: [-2.5752783] bias: [16.78222683] loss: 60.25859755002588\nEpoch: 308 / 1000\nw1: [12.50630012] w2: [-2.59586739] bias: [16.7823538] loss: 60.201975570443594\nEpoch: 309 / 1000\nw1: [12.51818245] w2: [-2.61643743] bias: [16.78247819] loss: 60.145461546411255\nEpoch: 310 / 1000\nw1: [12.53005248] w2: [-2.63698842] bias: [16.78260008] loss: 60.08905526912183\nEpoch: 311 / 1000\nw1: [12.54191027] w2: [-2.65752037] bias: [16.78271953] loss: 60.03275653033076\nEpoch: 312 / 1000\nw1: [12.55375585] w2: [-2.67803326] bias: [16.78283659] loss: 59.9765651223462\nEpoch: 313 / 1000\nw1: [12.56558927] w2: [-2.69852711] bias: [16.78295133] loss: 59.92048083801941\nEpoch: 314 / 1000\nw1: [12.57741056] w2: [-2.71900191] bias: [16.7830638] loss: 59.86450347073584\nEpoch: 315 / 1000\nw1: [12.58921977] w2: [-2.73945767] bias: [16.78317407] loss: 59.808632814406664\nEpoch: 316 / 1000\nw1: [12.60101694] w2: [-2.75989438] bias: [16.78328219] loss: 59.752868663460575\nEpoch: 317 / 1000\nw1: [12.6128021] w2: [-2.78031205] bias: [16.78338821] loss: 59.69721081283612\nEpoch: 318 / 1000\nw1: [12.62457529] w2: [-2.80071068] bias: [16.78349218] loss: 59.64165905797439\nEpoch: 319 / 1000\nw1: [12.63633656] w2: [-2.82109028] bias: [16.78359416] loss: 59.586213194812004\nEpoch: 320 / 1000\nw1: [12.64808593] w2: [-2.84145084] bias: [16.78369419] loss: 59.5308730197745\nEpoch: 321 / 1000\nw1: [12.65982344] w2: [-2.86179238] bias: [16.78379233] loss: 59.475638329769986\nEpoch: 322 / 1000\nw1: [12.67154913] w2: [-2.8821149] bias: [16.78388862] loss: 59.42050892218317\nEpoch: 323 / 1000\nw1: [12.68326303] w2: [-2.9024184] bias: [16.78398311] loss: 59.36548459486956\nEpoch: 324 / 1000\nw1: [12.69496518] w2: [-2.92270288] bias: [16.78407583] loss: 59.310565146150104\nEpoch: 325 / 1000\nw1: [12.70665561] w2: [-2.94296836] bias: [16.78416685] loss: 59.2557503748059\nEpoch: 326 / 1000\nw1: [12.71833434] w2: [-2.96321483] bias: [16.78425618] loss: 59.201040080073255\nEpoch: 327 / 1000\nw1: [12.73000142] w2: [-2.98344231] bias: [16.78434389] loss: 59.14643406163898\nEpoch: 328 / 1000\nw1: [12.74165687] w2: [-3.0036508] bias: [16.78443] loss: 59.091932119635835\nEpoch: 329 / 1000\nw1: [12.75330073] w2: [-3.02384031] bias: [16.78451456] loss: 59.03753405463831\nEpoch: 330 / 1000\nw1: [12.76493302] w2: [-3.04401085] bias: [16.7845976] loss: 58.98323966765836\nEpoch: 331 / 1000\nw1: [12.77655377] w2: [-3.06416241] bias: [16.78467916] loss: 58.92904876014163\nEpoch: 332 / 1000\nw1: [12.78816302] w2: [-3.08429502] bias: [16.78475928] loss: 58.87496113396362\nEpoch: 333 / 1000\nw1: [12.79976079] w2: [-3.10440867] bias: [16.78483798] loss: 58.82097659142605\nEpoch: 334 / 1000\nw1: [12.8113471] w2: [-3.12450339] bias: [16.78491532] loss: 58.76709493525355\nEpoch: 335 / 1000\nw1: [12.822922] w2: [-3.14457916] bias: [16.78499131] loss: 58.7133159685903\nEpoch: 336 / 1000\nw1: [12.83448549] w2: [-3.16463601] bias: [16.78506598] loss: 58.659639494996824\nEpoch: 337 / 1000\nw1: [12.84603762] w2: [-3.18467395] bias: [16.78513938] loss: 58.6060653184471\nEpoch: 338 / 1000\nw1: [12.8575784] w2: [-3.20469297] bias: [16.78521153] loss: 58.5525932433256\nEpoch: 339 / 1000\nw1: [12.86910787] w2: [-3.2246931] bias: [16.78528246] loss: 58.49922307442452\nEpoch: 340 / 1000\nw1: [12.88062604] w2: [-3.24467433] bias: [16.7853522] loss: 58.44595461694115\nEpoch: 341 / 1000\nw1: [12.89213294] w2: [-3.26463669] bias: [16.78542077] loss: 58.39278767647528\nEpoch: 342 / 1000\nw1: [12.9036286] w2: [-3.28458018] bias: [16.78548821] loss: 58.33972205902679\nEpoch: 343 / 1000\nw1: [12.91511304] w2: [-3.30450481] bias: [16.78555453] loss: 58.286757570993295\nEpoch: 344 / 1000\nw1: [12.92658628] w2: [-3.32441059] bias: [16.78561977] loss: 58.23389401916784\nEpoch: 345 / 1000\nw1: [12.93804834] w2: [-3.34429754] bias: [16.78568395] loss: 58.18113121073679\nEpoch: 346 / 1000\nw1: [12.94949926] w2: [-3.36416566] bias: [16.7857471] loss: 58.1284689532776\nEpoch: 347 / 1000\nw1: [12.96093905] w2: [-3.38401496] bias: [16.78580923] loss: 58.07590705475696\nEpoch: 348 / 1000\nw1: [12.97236773] w2: [-3.40384546] bias: [16.78587038] loss: 58.023445323528755\nEpoch: 349 / 1000\nw1: [12.98378533] w2: [-3.42365717] bias: [16.78593056] loss: 57.97108356833219\nEpoch: 350 / 1000\nw1: [12.99519186] w2: [-3.4434501] bias: [16.78598979] loss: 57.918821598289945\nEpoch: 351 / 1000\nw1: [13.00658736] w2: [-3.46322425] bias: [16.7860481] loss: 57.866659222906506\nEpoch: 352 / 1000\nw1: [13.01797183] w2: [-3.48297966] bias: [16.78610551] loss: 57.814596252066416\nEpoch: 353 / 1000\nw1: [13.0293453] w2: [-3.50271631] bias: [16.78616203] loss: 57.7626324960326\nEpoch: 354 / 1000\nw1: [13.0407078] w2: [-3.52243423] bias: [16.78621769] loss: 57.71076776544485\nEpoch: 355 / 1000\nw1: [13.05205933] w2: [-3.54213344] bias: [16.78627251] loss: 57.659001871318274\nEpoch: 356 / 1000\nw1: [13.06339992] w2: [-3.56181393] bias: [16.7863265] loss: 57.607334625041766\nEpoch: 357 / 1000\nw1: [13.07472959] w2: [-3.58147573] bias: [16.78637968] loss: 57.555765838376544\nEpoch: 358 / 1000\nw1: [13.08604836] w2: [-3.60111884] bias: [16.78643207] loss: 57.504295323454855\nEpoch: 359 / 1000\nw1: [13.09735624] w2: [-3.62074329] bias: [16.78648369] loss: 57.45292289277849\nEpoch: 360 / 1000\nw1: [13.10865327] w2: [-3.64034908] bias: [16.78653455] loss: 57.40164835921752\nEpoch: 361 / 1000\nw1: [13.11993944] w2: [-3.65993623] bias: [16.78658467] loss: 57.35047153600902\nEpoch: 362 / 1000\nw1: [13.13121479] w2: [-3.67950474] bias: [16.78663406] loss: 57.299392236755736\nEpoch: 363 / 1000\nw1: [13.14247932] w2: [-3.69905464] bias: [16.78668274] loss: 57.248410275424966\nEpoch: 364 / 1000\nw1: [13.15373307] w2: [-3.71858594] bias: [16.78673073] loss: 57.197525466347294\nEpoch: 365 / 1000\nw1: [13.16497604] w2: [-3.73809865] bias: [16.78677803] loss: 57.14673762421545\nEpoch: 366 / 1000\nw1: [13.17620825] w2: [-3.75759278] bias: [16.78682467] loss: 57.09604656408315\nEpoch: 367 / 1000\nw1: [13.18742973] w2: [-3.77706834] bias: [16.78687065] loss: 57.04545210136407\nEpoch: 368 / 1000\nw1: [13.19864047] w2: [-3.79652536] bias: [16.786916] loss: 56.99495405183064\nEpoch: 369 / 1000\nw1: [13.20984052] w2: [-3.81596385] bias: [16.78696072] loss: 56.944552231613045\nEpoch: 370 / 1000\nw1: [13.22102987] w2: [-3.83538381] bias: [16.78700482] loss: 56.89424645719823\nEpoch: 371 / 1000\nw1: [13.23220854] w2: [-3.85478527] bias: [16.78704832] loss: 56.844036545428814\nEpoch: 372 / 1000\nw1: [13.24337656] w2: [-3.87416824] bias: [16.78709123] loss: 56.793922313502094\nEpoch: 373 / 1000\nw1: [13.25453394] w2: [-3.89353273] bias: [16.78713356] loss: 56.7439035789691\nEpoch: 374 / 1000\nw1: [13.26568069] w2: [-3.91287875] bias: [16.78717532] loss: 56.69398015973363\nEpoch: 375 / 1000\nw1: [13.27681683] w2: [-3.93220633] bias: [16.78721653] loss: 56.64415187405132\nEpoch: 376 / 1000\nw1: [13.28794237] w2: [-3.95151547] bias: [16.78725718] loss: 56.594418540528636\nEpoch: 377 / 1000\nw1: [13.29905733] w2: [-3.97080619] bias: [16.78729731] loss: 56.54477997812209\nEpoch: 378 / 1000\nw1: [13.31016173] w2: [-3.99007851] bias: [16.78733691] loss: 56.49523600613724\nEpoch: 379 / 1000\nw1: [13.32125558] w2: [-4.00933244] bias: [16.78737599] loss: 56.445786444227856\nEpoch: 380 / 1000\nw1: [13.33233889] w2: [-4.02856799] bias: [16.78741457] loss: 56.396431112395014\nEpoch: 381 / 1000\nw1: [13.34341168] w2: [-4.04778518] bias: [16.78745265] loss: 56.34716983098632\nEpoch: 382 / 1000\nw1: [13.35447396] w2: [-4.06698402] bias: [16.78749024] loss: 56.298002420694964\nEpoch: 383 / 1000\nw1: [13.36552576] w2: [-4.08616454] bias: [16.78752735] loss: 56.248928702558956\nEpoch: 384 / 1000\nw1: [13.37656707] w2: [-4.10532673] bias: [16.78756399] loss: 56.19994849796031\nEpoch: 385 / 1000\nw1: [13.38759792] w2: [-4.12447063] bias: [16.78760017] loss: 56.15106162862413\nEpoch: 386 / 1000\nw1: [13.39861832] w2: [-4.14359624] bias: [16.7876359] loss: 56.10226791661796\nEpoch: 387 / 1000\nw1: [13.40962829] w2: [-4.16270358] bias: [16.78767118] loss: 56.0535671843509\nEpoch: 388 / 1000\nw1: [13.42062783] w2: [-4.18179267] bias: [16.78770602] loss: 56.004959254572825\nEpoch: 389 / 1000\nw1: [13.43161697] w2: [-4.20086352] bias: [16.78774043] loss: 55.95644395037362\nEpoch: 390 / 1000\nw1: [13.44259571] w2: [-4.21991614] bias: [16.78777442] loss: 55.90802109518247\nEpoch: 391 / 1000\nw1: [13.45356407] w2: [-4.23895055] bias: [16.78780799] loss: 55.85969051276699\nEpoch: 392 / 1000\nw1: [13.46452206] w2: [-4.25796677] bias: [16.78784115] loss: 55.81145202723258\nEpoch: 393 / 1000\nw1: [13.47546969] w2: [-4.27696481] bias: [16.78787391] loss: 55.76330546302163\nEpoch: 394 / 1000\nw1: [13.48640698] w2: [-4.29594469] bias: [16.78790627] loss: 55.71525064491278\nEpoch: 395 / 1000\nw1: [13.49733395] w2: [-4.31490642] bias: [16.78793825] loss: 55.66728739802021\nEpoch: 396 / 1000\nw1: [13.5082506] w2: [-4.33385002] bias: [16.78796984] loss: 55.619415547792904\nEpoch: 397 / 1000\nw1: [13.51915694] w2: [-4.3527755] bias: [16.78800105] loss: 55.57163492001395\nEpoch: 398 / 1000\nw1: [13.530053] w2: [-4.37168288] bias: [16.78803189] loss: 55.52394534079979\nEpoch: 399 / 1000\nw1: [13.54093877] w2: [-4.39057218] bias: [16.78806236] loss: 55.47634663659953\nEpoch: 400 / 1000\nw1: [13.55181428] w2: [-4.4094434] bias: [16.78809247] loss: 55.4288386341943\nEpoch: 401 / 1000\nw1: [13.56267954] w2: [-4.42829658] bias: [16.78812223] loss: 55.381421160696405\nEpoch: 402 / 1000\nw1: [13.57353455] w2: [-4.44713171] bias: [16.78815164] loss: 55.334094043548845\nEpoch: 403 / 1000\nw1: [13.58437934] w2: [-4.46594883] bias: [16.78818071] loss: 55.28685711052442\nEpoch: 404 / 1000\nw1: [13.59521391] w2: [-4.48474793] bias: [16.78820943] loss: 55.239710189725166\nEpoch: 405 / 1000\nw1: [13.60603828] w2: [-4.50352905] bias: [16.78823782] loss: 55.19265310958164\nEpoch: 406 / 1000\nw1: [13.61685245] w2: [-4.52229219] bias: [16.78826588] loss: 55.145685698852326\nEpoch: 407 / 1000\nw1: [13.62765645] w2: [-4.54103738] bias: [16.78829362] loss: 55.09880778662277\nEpoch: 408 / 1000\nw1: [13.63845027] w2: [-4.55976462] bias: [16.78832103] loss: 55.05201920230514\nEpoch: 409 / 1000\nw1: [13.64923394] w2: [-4.57847393] bias: [16.78834813] loss: 55.00531977563738\nEpoch: 410 / 1000\nw1: [13.66000746] w2: [-4.59716533] bias: [16.78837492] loss: 54.95870933668269\nEpoch: 411 / 1000\nw1: [13.67077085] w2: [-4.61583884] bias: [16.7884014] loss: 54.91218771582875\nEpoch: 412 / 1000\nw1: [13.68152412] w2: [-4.63449447] bias: [16.78842757] loss: 54.865754743787186\nEpoch: 413 / 1000\nw1: [13.69226728] w2: [-4.65313223] bias: [16.78845345] loss: 54.819410251592814\nEpoch: 414 / 1000\nw1: [13.70300034] w2: [-4.67175215] bias: [16.78847902] loss: 54.77315407060303\nEpoch: 415 / 1000\nw1: [13.71372331] w2: [-4.69035423] bias: [16.78850431] loss: 54.726986032497194\nEpoch: 416 / 1000\nw1: [13.72443621] w2: [-4.7089385] bias: [16.78852931] loss: 54.68090596927598\nEpoch: 417 / 1000\nw1: [13.73513904] w2: [-4.72750497] bias: [16.78855402] loss: 54.634913713260666\nEpoch: 418 / 1000\nw1: [13.74583182] w2: [-4.74605366] bias: [16.78857845] loss: 54.58900909709261\nEpoch: 419 / 1000\nw1: [13.75651456] w2: [-4.76458458] bias: [16.7886026] loss: 54.543191953732546\nEpoch: 420 / 1000\nw1: [13.76718726] w2: [-4.78309775] bias: [16.78862648] loss: 54.49746211645995\nEpoch: 421 / 1000\nw1: [13.77784994] w2: [-4.80159318] bias: [16.78865008] loss: 54.451819418872454\nEpoch: 422 / 1000\nw1: [13.78850262] w2: [-4.8200709] bias: [16.78867342] loss: 54.40626369488518\nEpoch: 423 / 1000\nw1: [13.7991453] w2: [-4.83853091] bias: [16.78869649] loss: 54.36079477873014\nEpoch: 424 / 1000\nw1: [13.80977799] w2: [-4.85697323] bias: [16.7887193] loss: 54.315412504955596\nEpoch: 425 / 1000\nw1: [13.8204007] w2: [-4.87539789] bias: [16.78874185] loss: 54.270116708425476\nEpoch: 426 / 1000\nw1: [13.83101345] w2: [-4.89380489] bias: [16.78876414] loss: 54.22490722431866\nEpoch: 427 / 1000\nw1: [13.84161624] w2: [-4.91219425] bias: [16.78878617] loss: 54.179783888128526\nEpoch: 428 / 1000\nw1: [13.85220909] w2: [-4.93056598] bias: [16.78880796] loss: 54.13474653566218\nEpoch: 429 / 1000\nw1: [13.86279201] w2: [-4.94892012] bias: [16.78882949] loss: 54.08979500303993\nEpoch: 430 / 1000\nw1: [13.873365] w2: [-4.96725666] bias: [16.78885078] loss: 54.044929126694655\nEpoch: 431 / 1000\nw1: [13.88392808] w2: [-4.98557562] bias: [16.78887183] loss: 54.00014874337118\nEpoch: 432 / 1000\nw1: [13.89448126] w2: [-5.00387703] bias: [16.78889263] loss: 53.95545369012575\nEpoch: 433 / 1000\nw1: [13.90502455] w2: [-5.0221609] bias: [16.78891319] loss: 53.910843804325275\nEpoch: 434 / 1000\nw1: [13.91555796] w2: [-5.04042724] bias: [16.78893352] loss: 53.86631892364685\nEpoch: 435 / 1000\nw1: [13.9260815] w2: [-5.05867607] bias: [16.78895361] loss: 53.821878886077144\nEpoch: 436 / 1000\nw1: [13.93659517] w2: [-5.0769074] bias: [16.78897347] loss: 53.77752352991176\nEpoch: 437 / 1000\nw1: [13.947099] w2: [-5.09512126] bias: [16.7889931] loss: 53.733252693754636\nEpoch: 438 / 1000\nw1: [13.95759299] w2: [-5.11331766] bias: [16.7890125] loss: 53.689066216517496\nEpoch: 439 / 1000\nw1: [13.96807715] w2: [-5.13149661] bias: [16.78903167] loss: 53.644963937419185\nEpoch: 440 / 1000\nw1: [13.97855149] w2: [-5.14965814] bias: [16.78905062] loss: 53.60094569598516\nEpoch: 441 / 1000\nw1: [13.98901603] w2: [-5.16780225] bias: [16.78906934] loss: 53.55701133204684\nEpoch: 442 / 1000\nw1: [13.99947076] w2: [-5.18592896] bias: [16.78908785] loss: 53.51316068574103\nEpoch: 443 / 1000\nw1: [14.00991571] w2: [-5.2040383] bias: [16.78910613] loss: 53.469393597509324\nEpoch: 444 / 1000\nw1: [14.02035088] w2: [-5.22213026] bias: [16.7891242] loss: 53.42570990809755\nEpoch: 445 / 1000\nw1: [14.03077628] w2: [-5.24020488] bias: [16.78914205] loss: 53.38210945855515\nEpoch: 446 / 1000\nw1: [14.04119192] w2: [-5.25826217] bias: [16.78915969] loss: 53.33859209023461\nEpoch: 447 / 1000\nw1: [14.05159782] w2: [-5.27630214] bias: [16.78917712] loss: 53.29515764479089\nEpoch: 448 / 1000\nw1: [14.06199398] w2: [-5.29432481] bias: [16.78919434] loss: 53.25180596418082\nEpoch: 449 / 1000\nw1: [14.07238041] w2: [-5.3123302] bias: [16.78921134] loss: 53.208536890662515\nEpoch: 450 / 1000\nw1: [14.08275712] w2: [-5.33031832] bias: [16.78922814] loss: 53.165350266794846\nEpoch: 451 / 1000\nw1: [14.09312412] w2: [-5.34828918] bias: [16.78924474] loss: 53.122245935436794\nEpoch: 452 / 1000\nw1: [14.10348143] w2: [-5.36624282] bias: [16.78926112] loss: 53.079223739746915\nEpoch: 453 / 1000\nw1: [14.11382905] w2: [-5.38417923] bias: [16.78927731] loss: 53.036283523182775\nEpoch: 454 / 1000\nw1: [14.12416699] w2: [-5.40209843] bias: [16.78929329] loss: 52.99342512950037\nEpoch: 455 / 1000\nw1: [14.13449526] w2: [-5.42000045] bias: [16.78930908] loss: 52.9506484027535\nEpoch: 456 / 1000\nw1: [14.14481387] w2: [-5.4378853] bias: [16.78932466] loss: 52.90795318729327\nEpoch: 457 / 1000\nw1: [14.15512283] w2: [-5.455753] bias: [16.78934005] loss: 52.8653393277675\nEpoch: 458 / 1000\nw1: [14.16542215] w2: [-5.47360355] bias: [16.78935524] loss: 52.822806669120155\nEpoch: 459 / 1000\nw1: [14.17571185] w2: [-5.49143698] bias: [16.78937023] loss: 52.780355056590736\nEpoch: 460 / 1000\nw1: [14.18599192] w2: [-5.5092533] bias: [16.78938503] loss: 52.737984335713776\nEpoch: 461 / 1000\nw1: [14.19626238] w2: [-5.52705253] bias: [16.78939964] loss: 52.69569435231826\nEpoch: 462 / 1000\nw1: [14.20652324] w2: [-5.54483469] bias: [16.78941405] loss: 52.65348495252704\nEpoch: 463 / 1000\nw1: [14.21677452] w2: [-5.56259979] bias: [16.78942828] loss: 52.61135598275629\nEpoch: 464 / 1000\nw1: [14.2270162] w2: [-5.58034784] bias: [16.78944231] loss: 52.569307289714935\nEpoch: 465 / 1000\nw1: [14.23724832] w2: [-5.59807887] bias: [16.78945616] loss: 52.52733872040408\nEpoch: 466 / 1000\nw1: [14.24747088] w2: [-5.61579288] bias: [16.78946982] loss: 52.485450122116525\nEpoch: 467 / 1000\nw1: [14.25768388] w2: [-5.6334899] bias: [16.78948329] loss: 52.443641342436074\nEpoch: 468 / 1000\nw1: [14.26788734] w2: [-5.65116994] bias: [16.78949658] loss: 52.401912229237105\nEpoch: 469 / 1000\nw1: [14.27808126] w2: [-5.66883302] bias: [16.78950968] loss: 52.36026263068394\nEpoch: 470 / 1000\nw1: [14.28826566] w2: [-5.68647915] bias: [16.7895226] loss: 52.31869239523035\nEpoch: 471 / 1000\nw1: [14.29844054] w2: [-5.70410835] bias: [16.78953533] loss: 52.27720137161893\nEpoch: 472 / 1000\nw1: [14.30860592] w2: [-5.72172064] bias: [16.78954789] loss: 52.235789408880585\nEpoch: 473 / 1000\nw1: [14.3187618] w2: [-5.73931602] bias: [16.78956026] loss: 52.194456356334015\nEpoch: 474 / 1000\nw1: [14.3289082] w2: [-5.75689452] bias: [16.78957245] loss: 52.1532020635851\nEpoch: 475 / 1000\nw1: [14.33904512] w2: [-5.77445615] bias: [16.78958447] loss: 52.11202638052637\nEpoch: 476 / 1000\nw1: [14.34917257] w2: [-5.79200093] bias: [16.7895963] loss: 52.07092915733651\nEpoch: 477 / 1000\nw1: [14.35929056] w2: [-5.80952888] bias: [16.78960796] loss: 52.029910244479744\nEpoch: 478 / 1000\nw1: [14.3693991] w2: [-5.82704] bias: [16.78961944] loss: 51.98896949270531\nEpoch: 479 / 1000\nw1: [14.3794982] w2: [-5.84453433] bias: [16.78963074] loss: 51.94810675304694\nEpoch: 480 / 1000\nw1: [14.38958787] w2: [-5.86201186] bias: [16.78964187] loss: 51.907321876822316\nEpoch: 481 / 1000\nw1: [14.39966812] w2: [-5.87947262] bias: [16.78965282] loss: 51.866614715632466\nEpoch: 482 / 1000\nw1: [14.40973895] w2: [-5.89691662] bias: [16.78966361] loss: 51.825985121361306\nEpoch: 483 / 1000\nw1: [14.41980038] w2: [-5.91434389] bias: [16.78967421] loss: 51.785432946175085\nEpoch: 484 / 1000\nw1: [14.42985242] w2: [-5.93175442] bias: [16.78968465] loss: 51.74495804252176\nEpoch: 485 / 1000\nw1: [14.43989507] w2: [-5.94914825] bias: [16.78969491] loss: 51.70456026313058\nEpoch: 486 / 1000\nw1: [14.44992835] w2: [-5.96652539] bias: [16.789705] loss: 51.66423946101149\nEpoch: 487 / 1000\nw1: [14.45995226] w2: [-5.98388585] bias: [16.78971492] loss: 51.62399548945456\nEpoch: 488 / 1000\nw1: [14.46996682] w2: [-6.00122965] bias: [16.78972468] loss: 51.583828202029515\nEpoch: 489 / 1000\nw1: [14.47997202] w2: [-6.0185568] bias: [16.78973426] loss: 51.543737452585184\nEpoch: 490 / 1000\nw1: [14.48996789] w2: [-6.03586732] bias: [16.78974367] loss: 51.50372309524896\nEpoch: 491 / 1000\nw1: [14.49995442] w2: [-6.05316122] bias: [16.78975292] loss: 51.46378498442625\nEpoch: 492 / 1000\nw1: [14.50993164] w2: [-6.07043853] bias: [16.789762] loss: 51.42392297479994\nEpoch: 493 / 1000\nw1: [14.51989954] w2: [-6.08769926] bias: [16.78977091] loss: 51.384136921329976\nEpoch: 494 / 1000\nw1: [14.52985814] w2: [-6.10494341] bias: [16.78977965] loss: 51.34442667925266\nEpoch: 495 / 1000\nw1: [14.53980744] w2: [-6.12217102] bias: [16.78978823] loss: 51.30479210408025\nEpoch: 496 / 1000\nw1: [14.54974746] w2: [-6.13938209] bias: [16.78979665] loss: 51.26523305160039\nEpoch: 497 / 1000\nw1: [14.55967821] w2: [-6.15657664] bias: [16.7898049] loss: 51.22574937787557\nEpoch: 498 / 1000\nw1: [14.56959969] w2: [-6.17375468] bias: [16.78981299] loss: 51.18634093924267\nEpoch: 499 / 1000\nw1: [14.57951191] w2: [-6.19091624] bias: [16.78982091] loss: 51.14700759231232\nEpoch: 500 / 1000\nw1: [14.58941488] w2: [-6.20806132] bias: [16.78982867] loss: 51.10774919396852\nEpoch: 501 / 1000\nw1: [14.59930861] w2: [-6.22518995] bias: [16.78983626] loss: 51.06856560136799\nEpoch: 502 / 1000\nw1: [14.60919311] w2: [-6.24230213] bias: [16.7898437] loss: 51.02945667193974\nEpoch: 503 / 1000\nw1: [14.61906839] w2: [-6.25939789] bias: [16.78985097] loss: 50.99042226338448\nEpoch: 504 / 1000\nw1: [14.62893445] w2: [-6.27647723] bias: [16.78985808] loss: 50.95146223367418\nEpoch: 505 / 1000\nw1: [14.63879131] w2: [-6.29354018] bias: [16.78986503] loss: 50.91257644105145\nEpoch: 506 / 1000\nw1: [14.64863898] w2: [-6.31058675] bias: [16.78987182] loss: 50.87376474402915\nEpoch: 507 / 1000\nw1: [14.65847745] w2: [-6.32761696] bias: [16.78987846] loss: 50.835027001389776\nEpoch: 508 / 1000\nw1: [14.66830676] w2: [-6.34463082] bias: [16.78988493] loss: 50.79636307218499\nEpoch: 509 / 1000\nw1: [14.67812689] w2: [-6.36162834] bias: [16.78989124] loss: 50.75777281573508\nEpoch: 510 / 1000\nw1: [14.68793786] w2: [-6.37860955] bias: [16.78989739] loss: 50.71925609162848\nEpoch: 511 / 1000\nw1: [14.69773968] w2: [-6.39557446] bias: [16.78990339] loss: 50.68081275972124\nEpoch: 512 / 1000\nw1: [14.70753236] w2: [-6.41252308] bias: [16.78990923] loss: 50.6424426801365\nEpoch: 513 / 1000\nw1: [14.71731591] w2: [-6.42945542] bias: [16.78991491] loss: 50.604145713264025\nEpoch: 514 / 1000\nw1: [14.72709033] w2: [-6.44637152] bias: [16.78992043] loss: 50.56592171975967\nEpoch: 515 / 1000\nw1: [14.73685563] w2: [-6.46327137] bias: [16.7899258] loss: 50.52777056054483\nEpoch: 516 / 1000\nw1: [14.74661183] w2: [-6.48015499] bias: [16.78993101] loss: 50.48969209680606\nEpoch: 517 / 1000\nw1: [14.75635893] w2: [-6.49702241] bias: [16.78993606] loss: 50.4516861899944\nEpoch: 518 / 1000\nw1: [14.76609694] w2: [-6.51387364] bias: [16.78994096] loss: 50.41375270182499\nEpoch: 519 / 1000\nw1: [14.77582587] w2: [-6.53070868] bias: [16.78994571] loss: 50.375891494276594\nEpoch: 520 / 1000\nw1: [14.78554573] w2: [-6.54752756] bias: [16.7899503] loss: 50.338102429590926\nEpoch: 521 / 1000\nw1: [14.79525652] w2: [-6.5643303] bias: [16.78995473] loss: 50.30038537027233\nEpoch: 522 / 1000\nw1: [14.80495826] w2: [-6.5811169] bias: [16.78995902] loss: 50.26274017908722\nEpoch: 523 / 1000\nw1: [14.81465095] w2: [-6.59788738] bias: [16.78996314] loss: 50.22516671906352\nEpoch: 524 / 1000\nw1: [14.82433461] w2: [-6.61464177] bias: [16.78996712] loss: 50.187664853490276\nEpoch: 525 / 1000\nw1: [14.83400924] w2: [-6.63138006] bias: [16.78997094] loss: 50.15023444591703\nEpoch: 526 / 1000\nw1: [14.84367484] w2: [-6.64810229] bias: [16.78997461] loss: 50.11287536015345\nEpoch: 527 / 1000\nw1: [14.85333144] w2: [-6.66480846] bias: [16.78997812] loss: 50.07558746026876\nEpoch: 528 / 1000\nw1: [14.86297903] w2: [-6.68149859] bias: [16.78998149] loss: 50.03837061059124\nEpoch: 529 / 1000\nw1: [14.87261763] w2: [-6.69817269] bias: [16.7899847] loss: 50.00122467570775\nEpoch: 530 / 1000\nw1: [14.88224724] w2: [-6.71483078] bias: [16.78998776] loss: 49.964149520463295\nEpoch: 531 / 1000\nw1: [14.89186788] w2: [-6.73147288] bias: [16.78999067] loss: 49.92714500996042\nEpoch: 532 / 1000\nw1: [14.90147955] w2: [-6.748099] bias: [16.78999343] loss: 49.89021100955879\nEpoch: 533 / 1000\nw1: [14.91108226] w2: [-6.76470916] bias: [16.78999604] loss: 49.85334738487471\nEpoch: 534 / 1000\nw1: [14.92067602] w2: [-6.78130336] bias: [16.7899985] loss: 49.81655400178059\nEpoch: 535 / 1000\nw1: [14.93026083] w2: [-6.79788163] bias: [16.79000081] loss: 49.77983072640447\nEpoch: 536 / 1000\nw1: [14.93983671] w2: [-6.81444399] bias: [16.79000296] loss: 49.7431774251296\nEpoch: 537 / 1000\nw1: [14.94940367] w2: [-6.83099043] bias: [16.79000497] loss: 49.70659396459382\nEpoch: 538 / 1000\nw1: [14.95896171] w2: [-6.847521] bias: [16.79000683] loss: 49.670080211689196\nEpoch: 539 / 1000\nw1: [14.96851084] w2: [-6.86403568] bias: [16.79000854] loss: 49.6336360335615\nEpoch: 540 / 1000\nw1: [14.97805107] w2: [-6.88053451] bias: [16.79001011] loss: 49.59726129760968\nEpoch: 541 / 1000\nw1: [14.98758241] w2: [-6.8970175] bias: [16.79001152] loss: 49.56095587148541\nEpoch: 542 / 1000\nw1: [14.99710487] w2: [-6.91348465] bias: [16.79001279] loss: 49.524719623092665\nEpoch: 543 / 1000\nw1: [15.00661845] w2: [-6.929936] bias: [16.7900139] loss: 49.48855242058716\nEpoch: 544 / 1000\nw1: [15.01612317] w2: [-6.94637155] bias: [16.79001487] loss: 49.45245413237587\nEpoch: 545 / 1000\nw1: [15.02561903] w2: [-6.96279131] bias: [16.7900157] loss: 49.41642462711662\nEpoch: 546 / 1000\nw1: [15.03510604] w2: [-6.9791953] bias: [16.79001637] loss: 49.38046377371753\nEpoch: 547 / 1000\nw1: [15.04458421] w2: [-6.99558355] bias: [16.7900169] loss: 49.34457144133661\nEpoch: 548 / 1000\nw1: [15.05405355] w2: [-7.01195605] bias: [16.79001728] loss: 49.30874749938122\nEpoch: 549 / 1000\nw1: [15.06351407] w2: [-7.02831284] bias: [16.79001752] loss: 49.272991817507624\nEpoch: 550 / 1000\nw1: [15.07296577] w2: [-7.04465391] bias: [16.79001761] loss: 49.23730426562055\nEpoch: 551 / 1000\nw1: [15.08240866] w2: [-7.06097929] bias: [16.79001756] loss: 49.20168471387261\nEpoch: 552 / 1000\nw1: [15.09184276] w2: [-7.07728899] bias: [16.79001735] loss: 49.166133032663964\nEpoch: 553 / 1000\nw1: [15.10126806] w2: [-7.09358304] bias: [16.79001701] loss: 49.13064909264175\nEpoch: 554 / 1000\nw1: [15.11068459] w2: [-7.10986143] bias: [16.79001652] loss: 49.095232764699645\nEpoch: 555 / 1000\nw1: [15.12009234] w2: [-7.12612419] bias: [16.79001588] loss: 49.0598839199774\nEpoch: 556 / 1000\nw1: [15.12949133] w2: [-7.14237133] bias: [16.7900151] loss: 49.02460242986039\nEpoch: 557 / 1000\nw1: [15.13888156] w2: [-7.15860287] bias: [16.79001417] loss: 48.98938816597906\nEpoch: 558 / 1000\nw1: [15.14826304] w2: [-7.17481882] bias: [16.7900131] loss: 48.954241000208576\nEpoch: 559 / 1000\nw1: [15.15763578] w2: [-7.19101919] bias: [16.79001189] loss: 48.919160804668294\nEpoch: 560 / 1000\nw1: [15.16699979] w2: [-7.20720401] bias: [16.79001053] loss: 48.88414745172128\nEpoch: 561 / 1000\nw1: [15.17635508] w2: [-7.22337328] bias: [16.79000903] loss: 48.849200813973894\nEpoch: 562 / 1000\nw1: [15.18570166] w2: [-7.23952702] bias: [16.79000738] loss: 48.81432076427528\nEpoch: 563 / 1000\nw1: [15.19503952] w2: [-7.25566525] bias: [16.7900056] loss: 48.77950717571692\nEpoch: 564 / 1000\nw1: [15.20436869] w2: [-7.27178798] bias: [16.79000366] loss: 48.74475992163221\nEpoch: 565 / 1000\nw1: [15.21368917] w2: [-7.28789523] bias: [16.79000159] loss: 48.71007887559594\nEpoch: 566 / 1000\nw1: [15.22300097] w2: [-7.303987] bias: [16.78999937] loss: 48.67546391142385\nEpoch: 567 / 1000\nw1: [15.2323041] w2: [-7.32006332] bias: [16.78999702] loss: 48.640914903172195\nEpoch: 568 / 1000\nw1: [15.24159856] w2: [-7.3361242] bias: [16.78999452] loss: 48.60643172513728\nEpoch: 569 / 1000\nw1: [15.25088436] w2: [-7.35216965] bias: [16.78999187] loss: 48.57201425185497\nEpoch: 570 / 1000\nw1: [15.26016152] w2: [-7.3681997] bias: [16.78998909] loss: 48.53766235810026\nEpoch: 571 / 1000\nw1: [15.26943003] w2: [-7.38421434] bias: [16.78998616] loss: 48.50337591888685\nEpoch: 572 / 1000\nw1: [15.27868991] w2: [-7.40021361] bias: [16.7899831] loss: 48.469154809466616\nEpoch: 573 / 1000\nw1: [15.28794117] w2: [-7.4161975] bias: [16.78997989] loss: 48.434998905329216\nEpoch: 574 / 1000\nw1: [15.29718382] w2: [-7.43216605] bias: [16.78997654] loss: 48.4009080822016\nEpoch: 575 / 1000\nw1: [15.30641786] w2: [-7.44811926] bias: [16.78997305] loss: 48.36688221604762\nEpoch: 576 / 1000\nw1: [15.31564329] w2: [-7.46405714] bias: [16.78996942] loss: 48.332921183067455\nEpoch: 577 / 1000\nw1: [15.32486014] w2: [-7.47997971] bias: [16.78996565] loss: 48.29902485969732\nEpoch: 578 / 1000\nw1: [15.3340684] w2: [-7.495887] bias: [16.78996173] loss: 48.26519312260889\nEpoch: 579 / 1000\nw1: [15.34326809] w2: [-7.511779] bias: [16.78995768] loss: 48.2314258487089\nEpoch: 580 / 1000\nw1: [15.35245922] w2: [-7.52765573] bias: [16.78995349] loss: 48.19772291513872\nEpoch: 581 / 1000\nw1: [15.36164178] w2: [-7.54351722] bias: [16.78994916] loss: 48.16408419927385\nEpoch: 582 / 1000\nw1: [15.3708158] w2: [-7.55936347] bias: [16.78994469] loss: 48.130509578723505\nEpoch: 583 / 1000\nw1: [15.37998127] w2: [-7.5751945] bias: [16.78994008] loss: 48.096998931330184\nEpoch: 584 / 1000\nw1: [15.38913821] w2: [-7.59101033] bias: [16.78993534] loss: 48.06355213516922\nEpoch: 585 / 1000\nw1: [15.39828663] w2: [-7.60681096] bias: [16.78993045] loss: 48.030169068548304\nEpoch: 586 / 1000\nw1: [15.40742652] w2: [-7.62259641] bias: [16.78992542] loss: 47.99684961000707\nEpoch: 587 / 1000\nw1: [15.41655791] w2: [-7.6383667] bias: [16.78992026] loss: 47.96359363831667\nEpoch: 588 / 1000\nw1: [15.42568079] w2: [-7.65412184] bias: [16.78991496] loss: 47.930401032479274\nEpoch: 589 / 1000\nw1: [15.43479519] w2: [-7.66986185] bias: [16.78990952] loss: 47.8972716717277\nEpoch: 590 / 1000\nw1: [15.44390109] w2: [-7.68558674] bias: [16.78990394] loss: 47.86420543552492\nEpoch: 591 / 1000\nw1: [15.45299852] w2: [-7.70129652] bias: [16.78989822] loss: 47.831202203563656\nEpoch: 592 / 1000\nw1: [15.46208749] w2: [-7.71699121] bias: [16.78989237] loss: 47.79826185576592\nEpoch: 593 / 1000\nw1: [15.47116799] w2: [-7.73267083] bias: [16.78988638] loss: 47.76538427228258\nEpoch: 594 / 1000\nw1: [15.48024003] w2: [-7.74833538] bias: [16.78988025] loss: 47.73256933349295\nEpoch: 595 / 1000\nw1: [15.48930364] w2: [-7.76398489] bias: [16.78987398] loss: 47.6998169200043\nEpoch: 596 / 1000\nw1: [15.4983588] w2: [-7.77961936] bias: [16.78986758] loss: 47.66712691265149\nEpoch: 597 / 1000\nw1: [15.50740554] w2: [-7.79523881] bias: [16.78986104] loss: 47.634499192496484\nEpoch: 598 / 1000\nw1: [15.51644386] w2: [-7.81084326] bias: [16.78985436] loss: 47.60193364082794\nEpoch: 599 / 1000\nw1: [15.52547376] w2: [-7.82643272] bias: [16.78984755] loss: 47.569430139160765\nEpoch: 600 / 1000\nw1: [15.53449526] w2: [-7.8420072] bias: [16.7898406] loss: 47.53698856923569\nEpoch: 601 / 1000\nw1: [15.54350836] w2: [-7.85756672] bias: [16.78983352] loss: 47.504608813018855\nEpoch: 602 / 1000\nw1: [15.55251307] w2: [-7.87311129] bias: [16.7898263] loss: 47.47229075270136\nEpoch: 603 / 1000\nw1: [15.56150941] w2: [-7.88864092] bias: [16.78981894] loss: 47.44003427069883\nEpoch: 604 / 1000\nw1: [15.57049737] w2: [-7.90415564] bias: [16.78981145] loss: 47.40783924965102\nEpoch: 605 / 1000\nw1: [15.57947696] w2: [-7.91965546] bias: [16.78980382] loss: 47.37570557242133\nEpoch: 606 / 1000\nw1: [15.5884482] w2: [-7.93514038] bias: [16.78979606] loss: 47.34363312209646\nEpoch: 607 / 1000\nw1: [15.59741109] w2: [-7.95061043] bias: [16.78978816] loss: 47.31162178198593\nEpoch: 608 / 1000\nw1: [15.60636564] w2: [-7.96606561] bias: [16.78978013] loss: 47.2796714356216\nEpoch: 609 / 1000\nw1: [15.61531185] w2: [-7.98150595] bias: [16.78977196] loss: 47.24778196675744\nEpoch: 610 / 1000\nw1: [15.62424974] w2: [-7.99693145] bias: [16.78976366] loss: 47.21595325936883\nEpoch: 611 / 1000\nw1: [15.63317932] w2: [-8.01234214] bias: [16.78975523] loss: 47.184185197652404\nEpoch: 612 / 1000\nw1: [15.64210058] w2: [-8.02773801] bias: [16.78974665] loss: 47.152477666025455\nEpoch: 613 / 1000\nw1: [15.65101354] w2: [-8.0431191] bias: [16.78973795] loss: 47.120830549125564\nEpoch: 614 / 1000\nw1: [15.65991821] w2: [-8.05848541] bias: [16.78972911] loss: 47.08924373181021\nEpoch: 615 / 1000\nw1: [15.66881459] w2: [-8.07383695] bias: [16.78972014] loss: 47.057717099156314\nEpoch: 616 / 1000\nw1: [15.6777027] w2: [-8.08917375] bias: [16.78971103] loss: 47.026250536459834\nEpoch: 617 / 1000\nw1: [15.68658253] w2: [-8.10449581] bias: [16.7897018] loss: 46.994843929235365\nEpoch: 618 / 1000\nw1: [15.69545411] w2: [-8.11980315] bias: [16.78969242] loss: 46.96349716321567\nEpoch: 619 / 1000\nw1: [15.70431743] w2: [-8.13509578] bias: [16.78968292] loss: 46.93221012435132\nEpoch: 620 / 1000\nw1: [15.7131725] w2: [-8.15037372] bias: [16.78967328] loss: 46.90098269881025\nEpoch: 621 / 1000\nw1: [15.72201933] w2: [-8.16563699] bias: [16.78966351] loss: 46.86981477297738\nEpoch: 622 / 1000\nw1: [15.73085794] w2: [-8.18088558] bias: [16.7896536] loss: 46.83870623345412\nEpoch: 623 / 1000\nw1: [15.73968832] w2: [-8.19611953] bias: [16.78964357] loss: 46.80765696705802\nEpoch: 624 / 1000\nw1: [15.74851049] w2: [-8.21133884] bias: [16.7896334] loss: 46.776666860822424\nEpoch: 625 / 1000\nw1: [15.75732445] w2: [-8.22654352] bias: [16.7896231] loss: 46.7457358019959\nEpoch: 626 / 1000\nw1: [15.76613021] w2: [-8.2417336] bias: [16.78961266] loss: 46.71486367804193\nEpoch: 627 / 1000\nw1: [15.77492777] w2: [-8.25690908] bias: [16.7896021] loss: 46.68405037663851\nEpoch: 628 / 1000\nw1: [15.78371716] w2: [-8.27206998] bias: [16.7895914] loss: 46.65329578567767\nEpoch: 629 / 1000\nw1: [15.79249837] w2: [-8.28721631] bias: [16.78958057] loss: 46.62259979326519\nEpoch: 630 / 1000\nw1: [15.80127141] w2: [-8.30234809] bias: [16.78956961] loss: 46.59196228772003\nEpoch: 631 / 1000\nw1: [15.81003629] w2: [-8.31746533] bias: [16.78955852] loss: 46.561383157574035\nEpoch: 632 / 1000\nw1: [15.81879301] w2: [-8.33256804] bias: [16.7895473] loss: 46.53086229157153\nEpoch: 633 / 1000\nw1: [15.8275416] w2: [-8.34765625] bias: [16.78953595] loss: 46.50039957866884\nEpoch: 634 / 1000\nw1: [15.83628204] w2: [-8.36272995] bias: [16.78952446] loss: 46.46999490803397\nEpoch: 635 / 1000\nw1: [15.84501436] w2: [-8.37778917] bias: [16.78951285] loss: 46.439648169046144\nEpoch: 636 / 1000\nw1: [15.85373855] w2: [-8.39283392] bias: [16.78950111] loss: 46.40935925129544\nEpoch: 637 / 1000\nw1: [15.86245463] w2: [-8.40786422] bias: [16.78948923] loss: 46.37912804458234\nEpoch: 638 / 1000\nw1: [15.8711626] w2: [-8.42288007] bias: [16.78947722] loss: 46.348954438917396\nEpoch: 639 / 1000\nw1: [15.87986248] w2: [-8.43788149] bias: [16.78946509] loss: 46.31883832452074\nEpoch: 640 / 1000\nw1: [15.88855426] w2: [-8.4528685] bias: [16.78945282] loss: 46.28877959182181\nEpoch: 641 / 1000\nw1: [15.89723796] w2: [-8.4678411] bias: [16.78944043] loss: 46.258778131458826\nEpoch: 642 / 1000\nw1: [15.90591359] w2: [-8.48279932] bias: [16.7894279] loss: 46.228833834278454\nEpoch: 643 / 1000\nw1: [15.91458114] w2: [-8.49774317] bias: [16.78941525] loss: 46.19894659133542\nEpoch: 644 / 1000\nw1: [15.92324064] w2: [-8.51267265] bias: [16.78940247] loss: 46.16911629389206\nEpoch: 645 / 1000\nw1: [15.93189208] w2: [-8.52758779] bias: [16.78938955] loss: 46.139342833418\nEpoch: 646 / 1000\nw1: [15.94053548] w2: [-8.5424886] bias: [16.78937651] loss: 46.109626101589654\nEpoch: 647 / 1000\nw1: [15.94917085] w2: [-8.55737508] bias: [16.78936334] loss: 46.07996599028998\nEpoch: 648 / 1000\nw1: [15.95779818] w2: [-8.57224726] bias: [16.78935004] loss: 46.05036239160791\nEpoch: 649 / 1000\nw1: [15.96641749] w2: [-8.58710515] bias: [16.78933661] loss: 46.020815197838125\nEpoch: 650 / 1000\nw1: [15.97502879] w2: [-8.60194876] bias: [16.78932306] loss: 45.99132430148054\nEpoch: 651 / 1000\nw1: [15.98363208] w2: [-8.61677811] bias: [16.78930937] loss: 45.961889595239974\nEpoch: 652 / 1000\nw1: [15.99222737] w2: [-8.63159321] bias: [16.78929556] loss: 45.93251097202571\nEpoch: 653 / 1000\nw1: [16.00081466] w2: [-8.64639407] bias: [16.78928162] loss: 45.90318832495122\nEpoch: 654 / 1000\nw1: [16.00939398] w2: [-8.6611807] bias: [16.78926755] loss: 45.873921547333616\nEpoch: 655 / 1000\nw1: [16.01796532] w2: [-8.67595313] bias: [16.78925335] loss: 45.84471053269337\nEpoch: 656 / 1000\nw1: [16.02652869] w2: [-8.69071135] bias: [16.78923903] loss: 45.8155551747539\nEpoch: 657 / 1000\nw1: [16.0350841] w2: [-8.7054554] bias: [16.78922458] loss: 45.78645536744118\nEpoch: 658 / 1000\nw1: [16.04363155] w2: [-8.72018528] bias: [16.78921] loss: 45.75741100488334\nEpoch: 659 / 1000\nw1: [16.05217106] w2: [-8.734901] bias: [16.78919529] loss: 45.72842198141032\nEpoch: 660 / 1000\nw1: [16.06070263] w2: [-8.74960257] bias: [16.78918046] loss: 45.69948819155344\nEpoch: 661 / 1000\nw1: [16.06922627] w2: [-8.76429002] bias: [16.7891655] loss: 45.67060953004505\nEpoch: 662 / 1000\nw1: [16.07774199] w2: [-8.77896336] bias: [16.78915042] loss: 45.641785891818124\nEpoch: 663 / 1000\nw1: [16.08624979] w2: [-8.79362259] bias: [16.7891352] loss: 45.61301717200588\nEpoch: 664 / 1000\nw1: [16.09474968] w2: [-8.80826773] bias: [16.78911986] loss: 45.58430326594144\nEpoch: 665 / 1000\nw1: [16.10324167] w2: [-8.8228988] bias: [16.7891044] loss: 45.55564406915739\nEpoch: 666 / 1000\nw1: [16.11172577] w2: [-8.8375158] bias: [16.78908881] loss: 45.527039477385394\nEpoch: 667 / 1000\nw1: [16.12020198] w2: [-8.85211876] bias: [16.78907309] loss: 45.4984893865559\nEpoch: 668 / 1000\nw1: [16.12867031] w2: [-8.86670769] bias: [16.78905725] loss: 45.46999369279768\nEpoch: 669 / 1000\nw1: [16.13713078] w2: [-8.88128259] bias: [16.78904128] loss: 45.44155229243747\nEpoch: 670 / 1000\nw1: [16.14558338] w2: [-8.89584348] bias: [16.78902518] loss: 45.41316508199962\nEpoch: 671 / 1000\nw1: [16.15402812] w2: [-8.91039038] bias: [16.78900896] loss: 45.384831958205694\nEpoch: 672 / 1000\nw1: [16.16246501] w2: [-8.9249233] bias: [16.78899262] loss: 45.35655281797409\nEpoch: 673 / 1000\nw1: [16.17089407] w2: [-8.93944225] bias: [16.78897615] loss: 45.328327558419666\nEpoch: 674 / 1000\nw1: [16.17931529] w2: [-8.95394724] bias: [16.78895955] loss: 45.30015607685338\nEpoch: 675 / 1000\nw1: [16.18772868] w2: [-8.9684383] bias: [16.78894283] loss: 45.272038270781934\nEpoch: 676 / 1000\nw1: [16.19613426] w2: [-8.98291542] bias: [16.78892599] loss: 45.243974037907336\nEpoch: 677 / 1000\nw1: [16.20453202] w2: [-8.99737864] bias: [16.78890902] loss: 45.21596327612659\nEpoch: 678 / 1000\nw1: [16.21292198] w2: [-9.01182795] bias: [16.78889193] loss: 45.18800588353128\nEpoch: 679 / 1000\nw1: [16.22130415] w2: [-9.02626337] bias: [16.78887471] loss: 45.160101758407265\nEpoch: 680 / 1000\nw1: [16.22967852] w2: [-9.04068492] bias: [16.78885736] loss: 45.13225079923422\nEpoch: 681 / 1000\nw1: [16.23804512] w2: [-9.0550926] bias: [16.7888399] loss: 45.10445290468534\nEpoch: 682 / 1000\nw1: [16.24640393] w2: [-9.06948644] bias: [16.78882231] loss: 45.07670797362692\nEpoch: 683 / 1000\nw1: [16.25475499] w2: [-9.08386644] bias: [16.78880459] loss: 45.04901590511803\nEpoch: 684 / 1000\nw1: [16.26309828] w2: [-9.09823262] bias: [16.78878676] loss: 45.02137659841011\nEpoch: 685 / 1000\nw1: [16.27143382] w2: [-9.11258499] bias: [16.7887688] loss: 44.99378995294663\nEpoch: 686 / 1000\nw1: [16.27976162] w2: [-9.12692357] bias: [16.78875071] loss: 44.966255868362715\nEpoch: 687 / 1000\nw1: [16.28808168] w2: [-9.14124836] bias: [16.78873251] loss: 44.938774244484776\nEpoch: 688 / 1000\nw1: [16.296394] w2: [-9.15555938] bias: [16.78871417] loss: 44.91134498133016\nEpoch: 689 / 1000\nw1: [16.30469861] w2: [-9.16985665] bias: [16.78869572] loss: 44.883967979106735\nEpoch: 690 / 1000\nw1: [16.3129955] w2: [-9.18414018] bias: [16.78867715] loss: 44.856643138212625\nEpoch: 691 / 1000\nw1: [16.32128468] w2: [-9.19840997] bias: [16.78865845] loss: 44.829370359235746\nEpoch: 692 / 1000\nw1: [16.32956617] w2: [-9.21266605] bias: [16.78863963] loss: 44.802149542953494\nEpoch: 693 / 1000\nw1: [16.33783996] w2: [-9.22690842] bias: [16.78862068] loss: 44.7749805903324\nEpoch: 694 / 1000\nw1: [16.34610606] w2: [-9.24113711] bias: [16.78860162] loss: 44.74786340252771\nEpoch: 695 / 1000\nw1: [16.35436448] w2: [-9.25535211] bias: [16.78858243] loss: 44.72079788088309\nEpoch: 696 / 1000\nw1: [16.36261523] w2: [-9.26955346] bias: [16.78856312] loss: 44.693783926930244\nEpoch: 697 / 1000\nw1: [16.37085832] w2: [-9.28374115] bias: [16.78854369] loss: 44.66682144238852\nEpoch: 698 / 1000\nw1: [16.37909375] w2: [-9.2979152] bias: [16.78852414] loss: 44.63991032916459\nEpoch: 699 / 1000\nw1: [16.38732154] w2: [-9.31207562] bias: [16.78850446] loss: 44.613050489352126\nEpoch: 700 / 1000\nw1: [16.39554168] w2: [-9.32622244] bias: [16.78848466] loss: 44.586241825231355\nEpoch: 701 / 1000\nw1: [16.40375418] w2: [-9.34035565] bias: [16.78846475] loss: 44.55948423926876\nEpoch: 702 / 1000\nw1: [16.41195906] w2: [-9.35447528] bias: [16.78844471] loss: 44.53277763411674\nEpoch: 703 / 1000\nw1: [16.42015631] w2: [-9.36858134] bias: [16.78842455] loss: 44.50612191261321\nEpoch: 704 / 1000\nw1: [16.42834596] w2: [-9.38267383] bias: [16.78840427] loss: 44.47951697778129\nEpoch: 705 / 1000\nw1: [16.43652799] w2: [-9.39675278] bias: [16.78838387] loss: 44.452962732828894\nEpoch: 706 / 1000\nw1: [16.44470243] w2: [-9.41081819] bias: [16.78836335] loss: 44.426459081148465\nEpoch: 707 / 1000\nw1: [16.45286928] w2: [-9.42487008] bias: [16.78834271] loss: 44.400005926316545\nEpoch: 708 / 1000\nw1: [16.46102854] w2: [-9.43890846] bias: [16.78832194] loss: 44.37360317209344\nEpoch: 709 / 1000\nw1: [16.46918022] w2: [-9.45293335] bias: [16.78830106] loss: 44.34725072242291\nEpoch: 710 / 1000\nw1: [16.47732434] w2: [-9.46694475] bias: [16.78828006] loss: 44.320948481431806\nEpoch: 711 / 1000\nw1: [16.48546089] w2: [-9.48094268] bias: [16.78825894] loss: 44.29469635342965\nEpoch: 712 / 1000\nw1: [16.49358988] w2: [-9.49492716] bias: [16.78823769] loss: 44.2684942429084\nEpoch: 713 / 1000\nw1: [16.50171133] w2: [-9.50889819] bias: [16.78821633] loss: 44.24234205454201\nEpoch: 714 / 1000\nw1: [16.50982524] w2: [-9.52285579] bias: [16.78819485] loss: 44.21623969318615\nEpoch: 715 / 1000\nw1: [16.51793161] w2: [-9.53679997] bias: [16.78817325] loss: 44.19018706387779\nEpoch: 716 / 1000\nw1: [16.52603045] w2: [-9.55073074] bias: [16.78815153] loss: 44.16418407183493\nEpoch: 717 / 1000\nw1: [16.53412178] w2: [-9.56464812] bias: [16.78812969] loss: 44.1382306224562\nEpoch: 718 / 1000\nw1: [16.54220559] w2: [-9.57855212] bias: [16.78810773] loss: 44.11232662132054\nEpoch: 719 / 1000\nw1: [16.5502819] w2: [-9.59244276] bias: [16.78808566] loss: 44.08647197418686\nEpoch: 720 / 1000\nw1: [16.55835071] w2: [-9.60632003] bias: [16.78806346] loss: 44.060666586993634\nEpoch: 721 / 1000\nw1: [16.56641202] w2: [-9.62018397] bias: [16.78804115] loss: 44.034910365858714\nEpoch: 722 / 1000\nw1: [16.57446586] w2: [-9.63403458] bias: [16.78801871] loss: 44.009203217078785\nEpoch: 723 / 1000\nw1: [16.58251221] w2: [-9.64787187] bias: [16.78799616] loss: 43.98354504712917\nEpoch: 724 / 1000\nw1: [16.5905511] w2: [-9.66169585] bias: [16.78797349] loss: 43.95793576266344\nEpoch: 725 / 1000\nw1: [16.59858252] w2: [-9.67550655] bias: [16.78795071] loss: 43.93237527051309\nEpoch: 726 / 1000\nw1: [16.60660649] w2: [-9.68930396] bias: [16.7879278] loss: 43.90686347768717\nEpoch: 727 / 1000\nw1: [16.614623] w2: [-9.70308811] bias: [16.78790478] loss: 43.88140029137195\nEpoch: 728 / 1000\nw1: [16.62263208] w2: [-9.71685901] bias: [16.78788164] loss: 43.85598561893064\nEpoch: 729 / 1000\nw1: [16.63063372] w2: [-9.73061667] bias: [16.78785838] loss: 43.83061936790297\nEpoch: 730 / 1000\nw1: [16.63862794] w2: [-9.7443611] bias: [16.78783501] loss: 43.80530144600491\nEpoch: 731 / 1000\nw1: [16.64661473] w2: [-9.75809231] bias: [16.78781151] loss: 43.78003176112834\nEpoch: 732 / 1000\nw1: [16.65459411] w2: [-9.77181032] bias: [16.7877879] loss: 43.75481022134065\nEpoch: 733 / 1000\nw1: [16.66256609] w2: [-9.78551514] bias: [16.78776418] loss: 43.72963673488447\nEpoch: 734 / 1000\nw1: [16.67053066] w2: [-9.79920678] bias: [16.78774033] loss: 43.704511210177316\nEpoch: 735 / 1000\nw1: [16.67848784] w2: [-9.81288526] bias: [16.78771637] loss: 43.67943355581124\nEpoch: 736 / 1000\nw1: [16.68643764] w2: [-9.82655059] bias: [16.7876923] loss: 43.654403680552534\nEpoch: 737 / 1000\nw1: [16.69438006] w2: [-9.84020277] bias: [16.7876681] loss: 43.62942149334134\nEpoch: 738 / 1000\nw1: [16.70231511] w2: [-9.85384183] bias: [16.78764379] loss: 43.6044869032914\nEpoch: 739 / 1000\nw1: [16.71024279] w2: [-9.86746778] bias: [16.78761937] loss: 43.57959981968965\nEpoch: 740 / 1000\nw1: [16.71816311] w2: [-9.88108062] bias: [16.78759483] loss: 43.554760151995914\nEpoch: 741 / 1000\nw1: [16.72607609] w2: [-9.89468037] bias: [16.78757017] loss: 43.529967809842596\nEpoch: 742 / 1000\nw1: [16.73398172] w2: [-9.90826704] bias: [16.7875454] loss: 43.50522270303434\nEpoch: 743 / 1000\nw1: [16.74188002] w2: [-9.92184065] bias: [16.78752051] loss: 43.48052474154769\nEpoch: 744 / 1000\nw1: [16.74977099] w2: [-9.93540121] bias: [16.7874955] loss: 43.45587383553073\nEpoch: 745 / 1000\nw1: [16.75765463] w2: [-9.94894873] bias: [16.78747038] loss: 43.431269895302876\nEpoch: 746 / 1000\nw1: [16.76553096] w2: [-9.96248322] bias: [16.78744515] loss: 43.4067128313544\nEpoch: 747 / 1000\nw1: [16.77339998] w2: [-9.9760047] bias: [16.78741979] loss: 43.382202554346186\nEpoch: 748 / 1000\nw1: [16.7812617] w2: [-9.98951317] bias: [16.78739433] loss: 43.357738975109434\nEpoch: 749 / 1000\nw1: [16.78911612] w2: [-10.00300865] bias: [16.78736875] loss: 43.333322004645254\nEpoch: 750 / 1000\nw1: [16.79696326] w2: [-10.01649116] bias: [16.78734305] loss: 43.308951554124384\nEpoch: 751 / 1000\nw1: [16.80480311] w2: [-10.0299607] bias: [16.78731724] loss: 43.28462753488688\nEpoch: 752 / 1000\nw1: [16.81263569] w2: [-10.04341728] bias: [16.78729132] loss: 43.26034985844177\nEpoch: 753 / 1000\nw1: [16.82046101] w2: [-10.05686093] bias: [16.78726528] loss: 43.23611843646675\nEpoch: 754 / 1000\nw1: [16.82827906] w2: [-10.07029165] bias: [16.78723912] loss: 43.21193318080783\nEpoch: 755 / 1000\nw1: [16.83608987] w2: [-10.08370945] bias: [16.78721286] loss: 43.18779400347907\nEpoch: 756 / 1000\nw1: [16.84389342] w2: [-10.09711435] bias: [16.78718648] loss: 43.1637008166622\nEpoch: 757 / 1000\nw1: [16.85168974] w2: [-10.11050636] bias: [16.78715998] loss: 43.13965353270631\nEpoch: 758 / 1000\nw1: [16.85947882] w2: [-10.12388549] bias: [16.78713337] loss: 43.115652064127595\nEpoch: 759 / 1000\nw1: [16.86726068] w2: [-10.13725175] bias: [16.78710665] loss: 43.09169632360895\nEpoch: 760 / 1000\nw1: [16.87503532] w2: [-10.15060516] bias: [16.78707981] loss: 43.067786223999704\nEpoch: 761 / 1000\nw1: [16.88280274] w2: [-10.16394572] bias: [16.78705286] loss: 43.04392167831529\nEpoch: 762 / 1000\nw1: [16.89056297] w2: [-10.17727346] bias: [16.7870258] loss: 43.02010259973694\nEpoch: 763 / 1000\nw1: [16.89831599] w2: [-10.19058838] bias: [16.78699862] loss: 42.996328901611314\nEpoch: 764 / 1000\nw1: [16.90606182] w2: [-10.20389049] bias: [16.78697133] loss: 42.97260049745028\nEpoch: 765 / 1000\nw1: [16.91380047] w2: [-10.21717981] bias: [16.78694393] loss: 42.94891730093052\nEpoch: 766 / 1000\nw1: [16.92153194] w2: [-10.23045635] bias: [16.78691642] loss: 42.925279225893235\nEpoch: 767 / 1000\nw1: [16.92925624] w2: [-10.24372012] bias: [16.78688879] loss: 42.901686186343866\nEpoch: 768 / 1000\nw1: [16.93697337] w2: [-10.25697114] bias: [16.78686105] loss: 42.87813809645173\nEpoch: 769 / 1000\nw1: [16.94468335] w2: [-10.2702094] bias: [16.7868332] loss: 42.85463487054973\nEpoch: 770 / 1000\nw1: [16.95238618] w2: [-10.28343494] bias: [16.78680523] loss: 42.83117642313408\nEpoch: 771 / 1000\nw1: [16.96008186] w2: [-10.29664776] bias: [16.78677716] loss: 42.807762668863916\nEpoch: 772 / 1000\nw1: [16.96777041] w2: [-10.30984787] bias: [16.78674897] loss: 42.78439352256105\nEpoch: 773 / 1000\nw1: [16.97545182] w2: [-10.32303528] bias: [16.78672067] loss: 42.76106889920961\nEpoch: 774 / 1000\nw1: [16.98312612] w2: [-10.33621001] bias: [16.78669226] loss: 42.737788713955844\nEpoch: 775 / 1000\nw1: [16.99079329] w2: [-10.34937207] bias: [16.78666373] loss: 42.714552882107604\nEpoch: 776 / 1000\nw1: [16.99845336] w2: [-10.36252147] bias: [16.7866351] loss: 42.69136131913426\nEpoch: 777 / 1000\nw1: [17.00610632] w2: [-10.37565822] bias: [16.78660635] loss: 42.66821394066621\nEpoch: 778 / 1000\nw1: [17.01375219] w2: [-10.38878234] bias: [16.78657749] loss: 42.64511066249474\nEpoch: 779 / 1000\nw1: [17.02139097] w2: [-10.40189383] bias: [16.78654852] loss: 42.62205140057156\nEpoch: 780 / 1000\nw1: [17.02902266] w2: [-10.41499272] bias: [16.78651944] loss: 42.59903607100862\nEpoch: 781 / 1000\nw1: [17.03664728] w2: [-10.428079] bias: [16.78649025] loss: 42.57606459007772\nEpoch: 782 / 1000\nw1: [17.04426483] w2: [-10.4411527] bias: [16.78646095] loss: 42.55313687421027\nEpoch: 783 / 1000\nw1: [17.05187531] w2: [-10.45421382] bias: [16.78643154] loss: 42.53025283999691\nEpoch: 784 / 1000\nw1: [17.05947874] w2: [-10.46726238] bias: [16.78640202] loss: 42.50741240418732\nEpoch: 785 / 1000\nw1: [17.06707512] w2: [-10.48029838] bias: [16.78637238] loss: 42.4846154836898\nEpoch: 786 / 1000\nw1: [17.07466446] w2: [-10.49332185] bias: [16.78634264] loss: 42.46186199557103\nEpoch: 787 / 1000\nw1: [17.08224676] w2: [-10.50633279] bias: [16.78631279] loss: 42.439151857055755\nEpoch: 788 / 1000\nw1: [17.08982204] w2: [-10.51933122] bias: [16.78628282] loss: 42.41648498552649\nEpoch: 789 / 1000\nw1: [17.09739029] w2: [-10.53231714] bias: [16.78625275] loss: 42.393861298523206\nEpoch: 790 / 1000\nw1: [17.10495152] w2: [-10.54529057] bias: [16.78622257] loss: 42.37128071374305\nEpoch: 791 / 1000\nw1: [17.11250574] w2: [-10.55825152] bias: [16.78619227] loss: 42.34874314904001\nEpoch: 792 / 1000\nw1: [17.12005297] w2: [-10.57120001] bias: [16.78616187] loss: 42.32624852242467\nEpoch: 793 / 1000\nw1: [17.12759319] w2: [-10.58413604] bias: [16.78613136] loss: 42.303796752063846\nEpoch: 794 / 1000\nw1: [17.13512643] w2: [-10.59705962] bias: [16.78610074] loss: 42.28138775628035\nEpoch: 795 / 1000\nw1: [17.14265268] w2: [-10.60997078] bias: [16.78607001] loss: 42.259021453552656\nEpoch: 796 / 1000\nw1: [17.15017196] w2: [-10.62286951] bias: [16.78603917] loss: 42.236697762514595\nEpoch: 797 / 1000\nw1: [17.15768427] w2: [-10.63575584] bias: [16.78600822] loss: 42.214416601955115\nEpoch: 798 / 1000\nw1: [17.16518962] w2: [-10.64862977] bias: [16.78597716] loss: 42.192177890817916\nEpoch: 799 / 1000\nw1: [17.17268801] w2: [-10.66149131] bias: [16.785946] loss: 42.1699815482012\nEpoch: 800 / 1000\nw1: [17.18017945] w2: [-10.67434049] bias: [16.78591473] loss: 42.14782749335735\nEpoch: 801 / 1000\nw1: [17.18766394] w2: [-10.6871773] bias: [16.78588334] loss: 42.12571564569266\nEpoch: 802 / 1000\nw1: [17.1951415] w2: [-10.70000176] bias: [16.78585185] loss: 42.103645924767015\nEpoch: 803 / 1000\nw1: [17.20261214] w2: [-10.71281389] bias: [16.78582025] loss: 42.08161825029365\nEpoch: 804 / 1000\nw1: [17.21007584] w2: [-10.72561369] bias: [16.78578855] loss: 42.05963254213878\nEpoch: 805 / 1000\nw1: [17.21753263] w2: [-10.73840117] bias: [16.78575673] loss: 42.03768872032138\nEpoch: 806 / 1000\nw1: [17.22498252] w2: [-10.75117636] bias: [16.78572481] loss: 42.01578670501283\nEpoch: 807 / 1000\nw1: [17.23242549] w2: [-10.76393925] bias: [16.78569278] loss: 41.99392641653668\nEpoch: 808 / 1000\nw1: [17.23986157] w2: [-10.77668987] bias: [16.78566064] loss: 41.97210777536835\nEpoch: 809 / 1000\nw1: [17.24729076] w2: [-10.78942822] bias: [16.7856284] loss: 41.950330702134806\nEpoch: 810 / 1000\nw1: [17.25471307] w2: [-10.80215432] bias: [16.78559605] loss: 41.9285951176143\nEpoch: 811 / 1000\nw1: [17.2621285] w2: [-10.81486817] bias: [16.78556359] loss: 41.90690094273609\nEpoch: 812 / 1000\nw1: [17.26953706] w2: [-10.82756979] bias: [16.78553102] loss: 41.885248098580114\nEpoch: 813 / 1000\nw1: [17.27693875] w2: [-10.84025919] bias: [16.78549835] loss: 41.863636506376736\nEpoch: 814 / 1000\nw1: [17.28433359] w2: [-10.85293638] bias: [16.78546557] loss: 41.84206608750647\nEpoch: 815 / 1000\nw1: [17.29172157] w2: [-10.86560138] bias: [16.78543268] loss: 41.82053676349962\nEpoch: 816 / 1000\nw1: [17.29910271] w2: [-10.87825419] bias: [16.78539968] loss: 41.79904845603611\nEpoch: 817 / 1000\nw1: [17.30647701] w2: [-10.89089483] bias: [16.78536658] loss: 41.77760108694512\nEpoch: 818 / 1000\nw1: [17.31384448] w2: [-10.9035233] bias: [16.78533338] loss: 41.75619457820477\nEpoch: 819 / 1000\nw1: [17.32120513] w2: [-10.91613963] bias: [16.78530007] loss: 41.734828851941955\nEpoch: 820 / 1000\nw1: [17.32855895] w2: [-10.92874381] bias: [16.78526665] loss: 41.713503830431954\nEpoch: 821 / 1000\nw1: [17.33590597] w2: [-10.94133588] bias: [16.78523312] loss: 41.692219436098185\nEpoch: 822 / 1000\nw1: [17.34324618] w2: [-10.95391582] bias: [16.78519949] loss: 41.670975591511905\nEpoch: 823 / 1000\nw1: [17.35057958] w2: [-10.96648366] bias: [16.78516575] loss: 41.64977221939199\nEpoch: 824 / 1000\nw1: [17.3579062] w2: [-10.97903941] bias: [16.78513191] loss: 41.62860924260458\nEpoch: 825 / 1000\nw1: [17.36522603] w2: [-10.99158308] bias: [16.78509797] loss: 41.607486584162835\nEpoch: 826 / 1000\nw1: [17.37253908] w2: [-11.00411468] bias: [16.78506391] loss: 41.58640416722664\nEpoch: 827 / 1000\nw1: [17.37984535] w2: [-11.01663422] bias: [16.78502975] loss: 41.56536191510232\nEpoch: 828 / 1000\nw1: [17.38714486] w2: [-11.02914172] bias: [16.78499549] loss: 41.54435975124242\nEpoch: 829 / 1000\nw1: [17.39443761] w2: [-11.04163718] bias: [16.78496112] loss: 41.52339759924532\nEpoch: 830 / 1000\nw1: [17.4017236] w2: [-11.05412062] bias: [16.78492665] loss: 41.50247538285505\nEpoch: 831 / 1000\nw1: [17.40900285] w2: [-11.06659205] bias: [16.78489207] loss: 41.48159302596099\nEpoch: 832 / 1000\nw1: [17.41627535] w2: [-11.07905147] bias: [16.78485739] loss: 41.46075045259755\nEpoch: 833 / 1000\nw1: [17.42354112] w2: [-11.09149891] bias: [16.7848226] loss: 41.43994758694394\nEpoch: 834 / 1000\nw1: [17.43080016] w2: [-11.10393437] bias: [16.78478771] loss: 41.41918435332387\nEpoch: 835 / 1000\nw1: [17.43805248] w2: [-11.11635787] bias: [16.78475271] loss: 41.398460676205325\nEpoch: 836 / 1000\nw1: [17.44529808] w2: [-11.12876941] bias: [16.78471761] loss: 41.37777648020019\nEpoch: 837 / 1000\nw1: [17.45253697] w2: [-11.14116901] bias: [16.78468241] loss: 41.35713169006405\nEpoch: 838 / 1000\nw1: [17.45976916] w2: [-11.15355667] bias: [16.7846471] loss: 41.33652623069595\nEpoch: 839 / 1000\nw1: [17.46699465] w2: [-11.16593242] bias: [16.78461169] loss: 41.31596002713801\nEpoch: 840 / 1000\nw1: [17.47421345] w2: [-11.17829626] bias: [16.78457617] loss: 41.29543300457525\nEpoch: 841 / 1000\nw1: [17.48142557] w2: [-11.1906482] bias: [16.78454055] loss: 41.274945088335265\nEpoch: 842 / 1000\nw1: [17.48863101] w2: [-11.20298825] bias: [16.78450483] loss: 41.254496203887996\nEpoch: 843 / 1000\nw1: [17.49582978] w2: [-11.21531643] bias: [16.784469] loss: 41.23408627684538\nEpoch: 844 / 1000\nw1: [17.50302188] w2: [-11.22763275] bias: [16.78443307] loss: 41.21371523296121\nEpoch: 845 / 1000\nw1: [17.51020732] w2: [-11.23993721] bias: [16.78439704] loss: 41.19338299813071\nEpoch: 846 / 1000\nw1: [17.51738612] w2: [-11.25222983] bias: [16.78436091] loss: 41.1730894983904\nEpoch: 847 / 1000\nw1: [17.52455826] w2: [-11.26451063] bias: [16.78432467] loss: 41.15283465991774\nEpoch: 848 / 1000\nw1: [17.53172377] w2: [-11.2767796] bias: [16.78428833] loss: 41.13261840903089\nEpoch: 849 / 1000\nw1: [17.53888264] w2: [-11.28903677] bias: [16.78425188] loss: 41.11244067218846\nEpoch: 850 / 1000\nw1: [17.54603489] w2: [-11.30128215] bias: [16.78421533] loss: 41.09230137598923\nEpoch: 851 / 1000\nw1: [17.55318051] w2: [-11.31351573] bias: [16.78417869] loss: 41.07220044717183\nEpoch: 852 / 1000\nw1: [17.56031952] w2: [-11.32573755] bias: [16.78414193] loss: 41.05213781261458\nEpoch: 853 / 1000\nw1: [17.56745192] w2: [-11.3379476] bias: [16.78410508] loss: 41.03211339933514\nEpoch: 854 / 1000\nw1: [17.57457772] w2: [-11.35014591] bias: [16.78406812] loss: 41.01212713449024\nEpoch: 855 / 1000\nw1: [17.58169692] w2: [-11.36233247] bias: [16.78403107] loss: 40.992178945375514\nEpoch: 856 / 1000\nw1: [17.58880954] w2: [-11.37450731] bias: [16.78399391] loss: 40.9722687594251\nEpoch: 857 / 1000\nw1: [17.59591557] w2: [-11.38667043] bias: [16.78395665] loss: 40.95239650421147\nEpoch: 858 / 1000\nw1: [17.60301502] w2: [-11.39882184] bias: [16.78391928] loss: 40.93256210744514\nEpoch: 859 / 1000\nw1: [17.6101079] w2: [-11.41096156] bias: [16.78388182] loss: 40.9127654969744\nEpoch: 860 / 1000\nw1: [17.61719422] w2: [-11.42308959] bias: [16.78384425] loss: 40.89300660078506\nEpoch: 861 / 1000\nw1: [17.62427398] w2: [-11.43520596] bias: [16.78380658] loss: 40.87328534700017\nEpoch: 862 / 1000\nw1: [17.63134719] w2: [-11.44731066] bias: [16.78376882] loss: 40.85360166387977\nEpoch: 863 / 1000\nw1: [17.63841385] w2: [-11.45940371] bias: [16.78373095] loss: 40.83395547982069\nEpoch: 864 / 1000\nw1: [17.64547397] w2: [-11.47148512] bias: [16.78369298] loss: 40.81434672335613\nEpoch: 865 / 1000\nw1: [17.65252756] w2: [-11.48355491] bias: [16.7836549] loss: 40.7947753231556\nEpoch: 866 / 1000\nw1: [17.65957462] w2: [-11.49561308] bias: [16.78361673] loss: 40.77524120802448\nEpoch: 867 / 1000\nw1: [17.66661516] w2: [-11.50765964] bias: [16.78357846] loss: 40.75574430690392\nEpoch: 868 / 1000\nw1: [17.67364919] w2: [-11.51969461] bias: [16.78354009] loss: 40.73628454887042\nEpoch: 869 / 1000\nw1: [17.68067671] w2: [-11.53171799] bias: [16.78350161] loss: 40.716861863135726\nEpoch: 870 / 1000\nw1: [17.68769772] w2: [-11.54372981] bias: [16.78346304] loss: 40.697476179046475\nEpoch: 871 / 1000\nw1: [17.69471224] w2: [-11.55573006] bias: [16.78342437] loss: 40.67812742608395\nEpoch: 872 / 1000\nw1: [17.70172027] w2: [-11.56771876] bias: [16.78338559] loss: 40.658815533863844\nEpoch: 873 / 1000\nw1: [17.70872181] w2: [-11.57969592] bias: [16.78334672] loss: 40.63954043213603\nEpoch: 874 / 1000\nw1: [17.71571688] w2: [-11.59166155] bias: [16.78330774] loss: 40.620302050784225\nEpoch: 875 / 1000\nw1: [17.72270547] w2: [-11.60361567] bias: [16.78326867] loss: 40.601100319825825\nEpoch: 876 / 1000\nw1: [17.7296876] w2: [-11.61555828] bias: [16.7832295] loss: 40.58193516941158\nEpoch: 877 / 1000\nw1: [17.73666327] w2: [-11.62748939] bias: [16.78319023] loss: 40.56280652982536\nEpoch: 878 / 1000\nw1: [17.74363248] w2: [-11.63940902] bias: [16.78315085] loss: 40.54371433148395\nEpoch: 879 / 1000\nw1: [17.75059525] w2: [-11.65131718] bias: [16.78311138] loss: 40.52465850493671\nEpoch: 880 / 1000\nw1: [17.75755158] w2: [-11.66321388] bias: [16.78307181] loss: 40.505638980865406\nEpoch: 881 / 1000\nw1: [17.76450147] w2: [-11.67509912] bias: [16.78303214] loss: 40.486655690083914\nEpoch: 882 / 1000\nw1: [17.77144494] w2: [-11.68697292] bias: [16.78299238] loss: 40.46770856353795\nEpoch: 883 / 1000\nw1: [17.77838198] w2: [-11.69883529] bias: [16.78295251] loss: 40.44879753230484\nEpoch: 884 / 1000\nw1: [17.7853126] w2: [-11.71068625] bias: [16.78291254] loss: 40.42992252759331\nEpoch: 885 / 1000\nw1: [17.79223682] w2: [-11.7225258] bias: [16.78287248] loss: 40.41108348074317\nEpoch: 886 / 1000\nw1: [17.79915463] w2: [-11.73435395] bias: [16.78283232] loss: 40.3922803232251\nEpoch: 887 / 1000\nw1: [17.80606604] w2: [-11.74617071] bias: [16.78279206] loss: 40.373512986640385\nEpoch: 888 / 1000\nw1: [17.81297106] w2: [-11.7579761] bias: [16.7827517] loss: 40.35478140272068\nEpoch: 889 / 1000\nw1: [17.81986969] w2: [-11.76977012] bias: [16.78271124] loss: 40.33608550332776\nEpoch: 890 / 1000\nw1: [17.82676194] w2: [-11.78155279] bias: [16.78267068] loss: 40.31742522045325\nEpoch: 891 / 1000\nw1: [17.83364782] w2: [-11.79332412] bias: [16.78263003] loss: 40.2988004862184\nEpoch: 892 / 1000\nw1: [17.84052733] w2: [-11.80508412] bias: [16.78258928] loss: 40.28021123287384\nEpoch: 893 / 1000\nw1: [17.84740048] w2: [-11.8168328] bias: [16.78254843] loss: 40.26165739279933\nEpoch: 894 / 1000\nw1: [17.85426727] w2: [-11.82857016] bias: [16.78250749] loss: 40.2431388985035\nEpoch: 895 / 1000\nw1: [17.86112771] w2: [-11.84029623] bias: [16.78246644] loss: 40.224655682623606\nEpoch: 896 / 1000\nw1: [17.86798181] w2: [-11.85201101] bias: [16.7824253] loss: 40.206207677925306\nEpoch: 897 / 1000\nw1: [17.87482957] w2: [-11.86371451] bias: [16.78238406] loss: 40.18779481730241\nEpoch: 898 / 1000\nw1: [17.881671] w2: [-11.87540674] bias: [16.78234273] loss: 40.1694170337766\nEpoch: 899 / 1000\nw1: [17.88850611] w2: [-11.88708772] bias: [16.7823013] loss: 40.15107426049724\nEpoch: 900 / 1000\nw1: [17.89533489] w2: [-11.89875745] bias: [16.78225977] loss: 40.13276643074108\nEpoch: 901 / 1000\nw1: [17.90215736] w2: [-11.91041594] bias: [16.78221814] loss: 40.11449347791208\nEpoch: 902 / 1000\nw1: [17.90897352] w2: [-11.92206322] bias: [16.78217642] loss: 40.09625533554109\nEpoch: 903 / 1000\nw1: [17.91578338] w2: [-11.93369928] bias: [16.7821346] loss: 40.078051937285665\nEpoch: 904 / 1000\nw1: [17.92258694] w2: [-11.94532413] bias: [16.78209268] loss: 40.05988321692979\nEpoch: 905 / 1000\nw1: [17.92938422] w2: [-11.9569378] bias: [16.78205067] loss: 40.041749108383684\nEpoch: 906 / 1000\nw1: [17.93617521] w2: [-11.96854028] bias: [16.78200857] loss: 40.02364954568348\nEpoch: 907 / 1000\nw1: [17.94295992] w2: [-11.98013159] bias: [16.78196636] loss: 40.005584462991074\nEpoch: 908 / 1000\nw1: [17.94973836] w2: [-11.99171175] bias: [16.78192406] loss: 39.98755379459383\nEpoch: 909 / 1000\nw1: [17.95651053] w2: [-12.00328075] bias: [16.78188166] loss: 39.96955747490436\nEpoch: 910 / 1000\nw1: [17.96327645] w2: [-12.01483862] bias: [16.78183917] loss: 39.95159543846028\nEpoch: 911 / 1000\nw1: [17.97003611] w2: [-12.02638536] bias: [16.78179659] loss: 39.93366761992395\nEpoch: 912 / 1000\nw1: [17.97678952] w2: [-12.03792098] bias: [16.7817539] loss: 39.915773954082304\nEpoch: 913 / 1000\nw1: [17.98353669] w2: [-12.04944549] bias: [16.78171112] loss: 39.897914375846526\nEpoch: 914 / 1000\nw1: [17.99027762] w2: [-12.06095891] bias: [16.78166825] loss: 39.88008882025189\nEpoch: 915 / 1000\nw1: [17.99701232] w2: [-12.07246124] bias: [16.78162528] loss: 39.86229722245745\nEpoch: 916 / 1000\nw1: [18.0037408] w2: [-12.0839525] bias: [16.78158222] loss: 39.844539517745886\nEpoch: 917 / 1000\nw1: [18.01046306] w2: [-12.09543269] bias: [16.78153906] loss: 39.82681564152318\nEpoch: 918 / 1000\nw1: [18.0171791] w2: [-12.10690183] bias: [16.7814958] loss: 39.809125529318464\nEpoch: 919 / 1000\nw1: [18.02388894] w2: [-12.11835992] bias: [16.78145245] loss: 39.791469116783716\nEpoch: 920 / 1000\nw1: [18.03059258] w2: [-12.12980699] bias: [16.78140901] loss: 39.77384633969358\nEpoch: 921 / 1000\nw1: [18.03729002] w2: [-12.14124303] bias: [16.78136547] loss: 39.7562571339451\nEpoch: 922 / 1000\nw1: [18.04398128] w2: [-12.15266806] bias: [16.78132184] loss: 39.73870143555749\nEpoch: 923 / 1000\nw1: [18.05066635] w2: [-12.16408208] bias: [16.78127811] loss: 39.721179180671925\nEpoch: 924 / 1000\nw1: [18.05734524] w2: [-12.17548512] bias: [16.78123429] loss: 39.70369030555126\nEpoch: 925 / 1000\nw1: [18.06401796] w2: [-12.18687718] bias: [16.78119038] loss: 39.68623474657988\nEpoch: 926 / 1000\nw1: [18.07068452] w2: [-12.19825827] bias: [16.78114637] loss: 39.66881244026334\nEpoch: 927 / 1000\nw1: [18.07734492] w2: [-12.20962839] bias: [16.78110226] loss: 39.65142332322828\nEpoch: 928 / 1000\nw1: [18.08399916] w2: [-12.22098757] bias: [16.78105806] loss: 39.63406733222211\nEpoch: 929 / 1000\nw1: [18.09064725] w2: [-12.23233582] bias: [16.78101377] loss: 39.61674440411275\nEpoch: 930 / 1000\nw1: [18.0972892] w2: [-12.24367313] bias: [16.78096939] loss: 39.59945447588852\nEpoch: 931 / 1000\nw1: [18.10392502] w2: [-12.25499953] bias: [16.78092491] loss: 39.582197484657776\nEpoch: 932 / 1000\nw1: [18.1105547] w2: [-12.26631502] bias: [16.78088034] loss: 39.56497336764877\nEpoch: 933 / 1000\nw1: [18.11717826] w2: [-12.27761961] bias: [16.78083567] loss: 39.54778206220937\nEpoch: 934 / 1000\nw1: [18.1237957] w2: [-12.28891332] bias: [16.78079092] loss: 39.53062350580688\nEpoch: 935 / 1000\nw1: [18.13040703] w2: [-12.30019615] bias: [16.78074606] loss: 39.513497636027786\nEpoch: 936 / 1000\nw1: [18.13701225] w2: [-12.31146811] bias: [16.78070112] loss: 39.4964043905775\nEpoch: 937 / 1000\nw1: [18.14361137] w2: [-12.32272922] bias: [16.78065608] loss: 39.479343707280194\nEpoch: 938 / 1000\nw1: [18.15020439] w2: [-12.33397949] bias: [16.78061095] loss: 39.46231552407853\nEpoch: 939 / 1000\nw1: [18.15679132] w2: [-12.34521892] bias: [16.78056573] loss: 39.445319779033426\nEpoch: 940 / 1000\nw1: [18.16337217] w2: [-12.35644753] bias: [16.78052042] loss: 39.42835641032391\nEpoch: 941 / 1000\nw1: [18.16994694] w2: [-12.36766532] bias: [16.78047501] loss: 39.41142535624678\nEpoch: 942 / 1000\nw1: [18.17651564] w2: [-12.37887231] bias: [16.78042951] loss: 39.39452655521646\nEpoch: 943 / 1000\nw1: [18.18307827] w2: [-12.39006851] bias: [16.78038392] loss: 39.37765994576475\nEpoch: 944 / 1000\nw1: [18.18963484] w2: [-12.40125393] bias: [16.78033823] loss: 39.36082546654061\nEpoch: 945 / 1000\nw1: [18.19618535] w2: [-12.41242857] bias: [16.78029246] loss: 39.34402305630991\nEpoch: 946 / 1000\nw1: [18.20272981] w2: [-12.42359245] bias: [16.78024659] loss: 39.32725265395524\nEpoch: 947 / 1000\nw1: [18.20926823] w2: [-12.43474558] bias: [16.78020063] loss: 39.31051419847569\nEpoch: 948 / 1000\nw1: [18.21580061] w2: [-12.44588797] bias: [16.78015458] loss: 39.293807628986585\nEpoch: 949 / 1000\nw1: [18.22232696] w2: [-12.45701963] bias: [16.78010844] loss: 39.27713288471932\nEpoch: 950 / 1000\nw1: [18.22884728] w2: [-12.46814056] bias: [16.7800622] loss: 39.26048990502108\nEpoch: 951 / 1000\nw1: [18.23536158] w2: [-12.47925079] bias: [16.78001587] loss: 39.243878629354704\nEpoch: 952 / 1000\nw1: [18.24186987] w2: [-12.49035031] bias: [16.77996946] loss: 39.22729899729834\nEpoch: 953 / 1000\nw1: [18.24837215] w2: [-12.50143915] bias: [16.77992295] loss: 39.21075094854534\nEpoch: 954 / 1000\nw1: [18.25486842] w2: [-12.5125173] bias: [16.77987635] loss: 39.19423442290398\nEpoch: 955 / 1000\nw1: [18.26135869] w2: [-12.52358478] bias: [16.77982966] loss: 39.17774936029728\nEpoch: 956 / 1000\nw1: [18.26784297] w2: [-12.53464161] bias: [16.77978288] loss: 39.16129570076272\nEpoch: 957 / 1000\nw1: [18.27432127] w2: [-12.54568778] bias: [16.77973601] loss: 39.144873384452126\nEpoch: 958 / 1000\nw1: [18.28079358] w2: [-12.55672332] bias: [16.77968904] loss: 39.12848235163132\nEpoch: 959 / 1000\nw1: [18.28725992] w2: [-12.56774822] bias: [16.77964199] loss: 39.112122542680034\nEpoch: 960 / 1000\nw1: [18.29372028] w2: [-12.57876251] bias: [16.77959485] loss: 39.095793898091586\nEpoch: 961 / 1000\nw1: [18.30017469] w2: [-12.58976619] bias: [16.77954761] loss: 39.07949635847274\nEpoch: 962 / 1000\nw1: [18.30662313] w2: [-12.60075927] bias: [16.77950029] loss: 39.063229864543445\nEpoch: 963 / 1000\nw1: [18.31306562] w2: [-12.61174176] bias: [16.77945287] loss: 39.04699435713664\nEpoch: 964 / 1000\nw1: [18.31950217] w2: [-12.62271367] bias: [16.77940537] loss: 39.03078977719803\nEpoch: 965 / 1000\nw1: [18.32593277] w2: [-12.63367501] bias: [16.77935778] loss: 39.01461606578587\nEpoch: 966 / 1000\nw1: [18.33235744] w2: [-12.6446258] bias: [16.77931009] loss: 38.99847316407075\nEpoch: 967 / 1000\nw1: [18.33877618] w2: [-12.65556603] bias: [16.77926232] loss: 38.982361013335414\nEpoch: 968 / 1000\nw1: [18.34518899] w2: [-12.66649573] bias: [16.77921445] loss: 38.96627955497447\nEpoch: 969 / 1000\nw1: [18.35159588] w2: [-12.6774149] bias: [16.7791665] loss: 38.95022873049427\nEpoch: 970 / 1000\nw1: [18.35799686] w2: [-12.68832355] bias: [16.77911846] loss: 38.9342084815126\nEpoch: 971 / 1000\nw1: [18.36439193] w2: [-12.6992217] bias: [16.77907033] loss: 38.91821874975857\nEpoch: 972 / 1000\nw1: [18.3707811] w2: [-12.71010934] bias: [16.77902211] loss: 38.90225947707233\nEpoch: 973 / 1000\nw1: [18.37716438] w2: [-12.7209865] bias: [16.7789738] loss: 38.88633060540485\nEpoch: 974 / 1000\nw1: [18.38354176] w2: [-12.73185318] bias: [16.7789254] loss: 38.87043207681779\nEpoch: 975 / 1000\nw1: [18.38991326] w2: [-12.74270939] bias: [16.77887691] loss: 38.8545638334832\nEpoch: 976 / 1000\nw1: [18.39627887] w2: [-12.75355515] bias: [16.77882833] loss: 38.83872581768335\nEpoch: 977 / 1000\nw1: [18.40263862] w2: [-12.76439045] bias: [16.77877967] loss: 38.82291797181052\nEpoch: 978 / 1000\nw1: [18.40899249] w2: [-12.77521532] bias: [16.77873092] loss: 38.807140238366806\nEpoch: 979 / 1000\nw1: [18.4153405] w2: [-12.78602976] bias: [16.77868207] loss: 38.79139255996387\nEpoch: 980 / 1000\nw1: [18.42168265] w2: [-12.79683378] bias: [16.77863314] loss: 38.775674879322736\nEpoch: 981 / 1000\nw1: [18.42801895] w2: [-12.80762739] bias: [16.77858412] loss: 38.75998713927361\nEpoch: 982 / 1000\nw1: [18.43434941] w2: [-12.81841061] bias: [16.77853502] loss: 38.7443292827557\nEpoch: 983 / 1000\nw1: [18.44067402] w2: [-12.82918343] bias: [16.77848582] loss: 38.72870125281692\nEpoch: 984 / 1000\nw1: [18.4469928] w2: [-12.83994588] bias: [16.77843654] loss: 38.71310299261373\nEpoch: 985 / 1000\nw1: [18.45330575] w2: [-12.85069796] bias: [16.77838717] loss: 38.69753444541095\nEpoch: 986 / 1000\nw1: [18.45961288] w2: [-12.86143968] bias: [16.77833771] loss: 38.68199555458154\nEpoch: 987 / 1000\nw1: [18.46591418] w2: [-12.87217106] bias: [16.77828816] loss: 38.66648626360636\nEpoch: 988 / 1000\nw1: [18.47220967] w2: [-12.88289209] bias: [16.77823853] loss: 38.651006516074006\nEpoch: 989 / 1000\nw1: [18.47849936] w2: [-12.89360279] bias: [16.77818881] loss: 38.6355562556806\nEpoch: 990 / 1000\nw1: [18.48478324] w2: [-12.90430318] bias: [16.778139] loss: 38.62013542622956\nEpoch: 991 / 1000\nw1: [18.49106133] w2: [-12.91499326] bias: [16.7780891] loss: 38.60474397163141\nEpoch: 992 / 1000\nw1: [18.49733362] w2: [-12.92567303] bias: [16.77803912] loss: 38.589381835903595\nEpoch: 993 / 1000\nw1: [18.50360013] w2: [-12.93634252] bias: [16.77798905] loss: 38.57404896317024\nEpoch: 994 / 1000\nw1: [18.50986086] w2: [-12.94700173] bias: [16.77793889] loss: 38.558745297661964\nEpoch: 995 / 1000\nw1: [18.51611582] w2: [-12.95765066] bias: [16.77788865] loss: 38.54347078371569\nEpoch: 996 / 1000\nw1: [18.522365] w2: [-12.96828934] bias: [16.77783832] loss: 38.528225365774425\nEpoch: 997 / 1000\nw1: [18.52860842] w2: [-12.97891777] bias: [16.7777879] loss: 38.51300898838708\nEpoch: 998 / 1000\nw1: [18.53484608] w2: [-12.98953596] bias: [16.7777374] loss: 38.49782159620822\nEpoch: 999 / 1000\nw1: [18.54107799] w2: [-13.00014391] bias: [16.77768681] loss: 38.48266313399792\nEpoch: 1000 / 1000\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\n##### 최종 w1, w2, bias #######\n[18.54730416] [-13.01074165] [16.77763613]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n최초 w1, w2, bias: [0.] [0.] [1.]\\nEpoch: 1 / 1000\\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\\nEpoch: 2 / 1000\\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\\nEpoch: 3 / 1000\\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\\n~~~\\nEpoch: 1000 / 1000\\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\\n##### 최종 w1, w2, bias #######\\n[18.54730416] [-13.01074165] [16.77763613]\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### 계산된 Weight와 Bias를 이용하여 Price 예측\n* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. ","metadata":{}},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nprint(predicted)\n# 506개의 피쳐 * 18.547 + 506개의 피쳐 * (-13.01) + bias\n# 즉 506개의 예측값이 나옴. ","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:17:38.783301Z","iopub.execute_input":"2022-01-08T00:17:38.783984Z","iopub.status.idle":"2022-01-08T00:17:38.796313Z","shell.execute_reply.started":"2022-01-08T00:17:38.783918Z","shell.execute_reply":"2022-01-08T00:17:38.795581Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[26.32199985 24.28120724 28.83088566 28.55765177 28.22912024 25.72412455\n 21.64653851 19.80255933 14.00975824 19.94150312 20.06436833 21.33430362\n 20.03184795 22.91974577 22.72412289 22.43566492 23.47313948 20.76415887\n 19.93629525 21.04657277 16.99180918 20.97688724 19.8503221  18.2646619\n 19.94441163 18.71403189 20.08487328 20.02969382 23.23019    24.16071412\n 16.93275783 21.64076697 15.94043741 18.41593039 19.10524111 22.35308212\n 21.40503436 22.38482097 22.30880043 26.63002653 28.99470609 27.06525392\n 24.58117202 24.14523564 22.88307157 21.27079875 20.22587115 19.42357475\n 12.86934154 18.83598828 21.10620573 23.08962849 25.98685157 23.03284945\n 20.35499875 28.77829662 25.35604676 27.5482532  24.11891387 22.49690667\n 20.42496602 20.76144126 25.27081906 25.36380869 27.099783   25.42046063\n 21.63317771 22.72486824 19.92409776 22.5020238  25.13581086 22.38078542\n 24.31569104 24.23016338 24.60252209 23.87324688 22.76055162 22.87690012\n 22.46068688 22.35163698 26.75088312 25.67417128 24.72712338 23.96373713\n 23.9951578  25.96098421 21.50282302 23.46637721 27.67055492 27.85462562\n 24.38546732 24.56182146 24.70767285 24.58682538 23.14937964 25.90013326\n 22.57449044 31.9078257  31.25269872 28.87633977 25.26814683 26.08833034\n 23.68941245 21.72815783 22.23327977 19.62395119 18.78439898 21.45925935\n 23.34583862 21.29758395 22.09224553 24.9598293  19.93763123 20.25782806\n 23.21741091 20.15249551 22.36937866 22.44322694 20.09349874 20.22428059\n 20.44540718 20.95751855 19.49070622 16.43211673 19.32494989 20.6996814\n 14.90436261 18.80395857 22.07289497 18.19207708 23.17050246 22.82346576\n 23.39621905 20.03787398 18.98832011 21.16807456 19.79297047 22.4418453\n 17.90404618 19.97565052 18.01099653 10.22643816 14.31603079 14.69063448\n 11.65235904 16.54781072 18.76682997 11.64793269 13.0063312  16.933385\n 21.43790017 19.18066191 18.20402928 19.36344191 21.09658043 21.21421953\n 17.68477385 27.76981606 23.99254024 25.2257739  24.98032514 30.73697906\n 31.78110673 33.31481407 21.36866013 22.90344914 31.59339129 21.2708896\n 23.21504775 23.43102997 20.44163524 21.32104278 19.27148839 24.29933982\n 22.10446125 26.09328391 22.50788618 24.92769973 26.63834291 27.73974959\n 29.62476545 23.1855085  28.44064886 26.02804297 19.64011152 21.88913335\n 30.97585644 26.44731004 26.40526431 28.34262384 27.61514831 27.0089384\n 29.22246765 27.103655   26.6404698  31.66356758 29.17542289 26.90939543\n 28.21732267 27.89430828 28.50240871 23.97468939 30.67154722 31.28381041\n 32.26093389 21.77659324 23.28659646 18.81145699 21.03073054 15.44548621\n 19.72410301 15.33919018 19.62555087 24.03156839 13.36790121 23.31337273\n 20.81812946 24.86913914 19.45875834 23.62236281 25.96008432 18.94480182\n 25.62522688 25.53419138 32.63305625 34.08833451 32.19250271 27.91619458\n 30.65083334 26.67826194 21.8164031  29.19959964 33.48493315 32.63374702\n 25.75644501 22.4659949  24.8838997  29.19438256 25.49250011 25.57411942\n 25.16862209 21.95227474 23.31056419 25.59981412 20.13236342 18.03527342\n 23.1616862  23.22560921 24.18742679 26.26294849 25.67897032 26.33416098\n 28.20012633 32.82358925 24.09153777 22.30489218 30.11715376 33.83778676\n 28.00694834 26.58155476 26.8987249  28.86178823 32.46669999 26.74342998\n 27.44432023 20.75107079 24.36017245 31.55847121 30.15606383 20.88158894\n 20.88750591 24.5534596  25.20321496 29.71361967 27.49291929 28.03154332\n 28.39708519 27.50839787 25.19807059 27.21092353 31.18448567 27.85866116\n 30.83183197 31.76593713 27.11472535 24.72163358 22.2417598  24.24583284\n 24.45738871 24.6601191  27.93076434 28.86813245 26.61798358 23.43743779\n 22.36467968 26.22491098 25.36437226 19.63334925 25.50821504 28.06423646\n 26.9825984  24.75255437 24.71292629 27.81148009 27.97098313 25.04959243\n 28.79003049 26.38025128 26.69319517 22.38756583 17.87874257 24.3531105\n 21.94053175 24.17554749 24.7498004  20.88940555 19.180071   19.56901721\n 23.70460929 21.89776792 24.99546739 24.93625248 23.44774475 20.81390311\n 25.34411288 25.77435044 24.96730057 21.76961283 22.01797019 24.61470136\n 23.31475427 20.55907442 23.36553462 25.15025333 24.74478333 23.32223456\n 22.08255672 21.90210343 23.22091022 22.51633897 22.61748152 28.50573527\n 24.8800278  25.96217485 27.51742325 22.33577661 21.15524098 25.61688325\n 26.17262189 27.28887998 25.66091044 26.15311688 22.85748604 27.0380596\n 21.97875114 23.83929949 20.50118637 22.70607204 22.39629124 21.91575508\n 24.68414141 21.86402947 20.14076173 20.11036796 34.04684315 14.84253962\n 17.3549427  13.68629883 21.2356517  27.15458077 28.61443993 23.41266111\n 22.43417431  9.6956125   5.81743951 25.90743157 20.02934829 21.262682\n 18.91181774 19.03959113 23.32787861 20.43522732 15.94471843 15.54320216\n  9.26999229 12.43577742 11.1229604  11.02773472 11.0931302  16.41315715\n 18.90375594 19.51256528 13.42106294 21.3060731  19.79497007 21.59394937\n 20.55161218 18.01575018 13.14023049 14.78078832 16.40940325 19.99021107\n 20.21058329 16.65518852 14.56986874 16.6897176  11.06984432 20.31850643\n 15.22733163 21.99296608 21.57323567 20.78299115  8.85126135 15.85445552\n  7.52686481 17.17939714 19.58651332 14.02882706 18.51073783 20.83077205\n 22.13465482 20.4512332  19.75339698 18.07102039 18.35984211 16.94046527\n 19.85423045 21.57141777 19.02632118 18.77180148 20.9701431  21.95775524\n 23.25786607 21.79268067 21.35938016 19.94750173 21.2245264  17.11066608\n 13.62173907 16.52660617 17.48182513 20.50131358 20.88866921 21.02260486\n 17.00661512 19.08845362 20.89141407 21.05624336 20.21495515 20.61579879\n 22.4670218  22.02884955 20.92173503 25.00698295 21.93644153 21.42324859\n 19.15627595 19.75728701 21.30932694 21.07678457 22.66263557 22.14310761\n 22.17038376 24.19528877 22.06301526 20.13709876 19.7387545  18.42290159\n 19.29451987 19.74744366 20.81157621 22.25974696 22.46400432 25.36306334\n 17.51758141 17.98989184 21.08008379 14.64992483 20.25085676 22.17358311\n 23.0706506  25.95303129 27.32034611 21.47970972 20.81874751 23.37685947\n 21.09353561 21.61811719 17.6422554  15.37831838 12.18756558 19.51862767\n 21.21318342 20.71341497 20.92445273 18.57502449 16.3090425  20.27225239\n 21.49549728 19.11363039 21.01771508 24.70218305 23.23305313 27.51012475\n 26.5617589  23.34403001]\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n# scaled_features : rm, lstat. 학습을 0~1로 scaled된 값으로 했기 때문에, 예측도 0~1 scaled된 값으로 해야 함. \nbostonDF['PREDICTED_PRICE'] = predicted\nbostonDF.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:17:14.867264Z","iopub.execute_input":"2022-01-08T00:17:14.867545Z","iopub.status.idle":"2022-01-08T00:17:14.899855Z","shell.execute_reply.started":"2022-01-08T00:17:14.867517Z","shell.execute_reply":"2022-01-08T00:17:14.899080Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n\n   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  \n0     15.3  396.90   4.98   24.0        26.322000  \n1     17.8  396.90   9.14   21.6        24.281207  \n2     17.8  392.83   4.03   34.7        28.830886  \n3     18.7  394.63   2.94   33.4        28.557652  \n4     18.7  396.90   5.33   36.2        28.229120  \n5     18.7  394.12   5.21   28.7        25.724125  \n6     15.2  395.60  12.43   22.9        21.646539  \n7     15.2  396.90  19.15   27.1        19.802559  \n8     15.2  386.63  29.93   16.5        14.009758  \n9     15.2  386.71  17.10   18.9        19.941503  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n      <th>PREDICTED_PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n      <td>26.322000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n      <td>24.281207</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n      <td>28.830886</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n      <td>28.557652</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n      <td>28.229120</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.02985</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.430</td>\n      <td>58.7</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.12</td>\n      <td>5.21</td>\n      <td>28.7</td>\n      <td>25.724125</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.08829</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.012</td>\n      <td>66.6</td>\n      <td>5.5605</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>395.60</td>\n      <td>12.43</td>\n      <td>22.9</td>\n      <td>21.646539</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.14455</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.172</td>\n      <td>96.1</td>\n      <td>5.9505</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>396.90</td>\n      <td>19.15</td>\n      <td>27.1</td>\n      <td>19.802559</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.21124</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>5.631</td>\n      <td>100.0</td>\n      <td>6.0821</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.63</td>\n      <td>29.93</td>\n      <td>16.5</td>\n      <td>14.009758</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.17004</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.004</td>\n      <td>85.9</td>\n      <td>6.5921</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.71</td>\n      <td>17.10</td>\n      <td>18.9</td>\n      <td>19.941503</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Keras를 이용하여 보스턴 주택가격 모델 학습 및 예측\n* Dense Layer를 이용하여 퍼셉트론 구현. units는 1로 설정. ","metadata":{}},{"cell_type":"markdown","source":"**Keras 기초 설명**\n\n* 우리가 앞에서 만들었던 코드는 모두 keras의 Dense layer 하나로 해결할 수 있음. (keras의 대표적인 기본 layer)\n* 우리가 keras에서 해야 할 것\n    1. 입력이 어떻게 생겨 있는가\n    2. 커널 초기화 / bias 초기화","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:38:30.244453Z","iopub.execute_input":"2022-01-08T00:38:30.244706Z","iopub.status.idle":"2022-01-08T00:38:30.249310Z","shell.execute_reply.started":"2022-01-08T00:38:30.244678Z","shell.execute_reply":"2022-01-08T00:38:30.248483Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # deep learning의 심층 신경망은 Sequential을 활용해 이렇게 편하게 만들 수 있음. \n\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n'''\n    unit의 개수 => 1개라서 single layer. 근데 node 자체도 1개 뿐임. \n        *참고. unit개수 3개면 세로로 3개 가진 layer 만들어짐\n    입력값 => 2개의 컬럼(rm, lstat) 가진 1차원 데이터. \n        *참고. Dense에서 입력값 정해주는 건 '첫 번째 layer'에서만 가능!\n    kernel_initializer => weight값 초기화. 아까 weight는 0으로 초기화해줬기 때문에 'zeros'\n    bias_initializer => bias값 초기화. 아까 bias는 1로 초기화해줬기 때문에 'ones'\n    \n    \n    ★하나짜리 Dense layer 하나 만들어짐. 그 안에는 rm, lstat, 1개의 node 들어있음(?더알아보기?)\n    ★위에서 다 코드로 해줬지만, 이것처럼 Dense를 사용하면 한 번에 해줄 수 있음. \n'''\n\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\nmodel.fit(scaled_features, bostonDF['PRICE'].values, epochs=1000)\n\n'''\nEpoch 1/1000\n16/16 [==============================] - 1s 2ms/step - loss: 542.3250 - mse: 542.3250\nEpoch 2/1000\n16/16 [==============================] - 0s 2ms/step - loss: 530.1854 - mse: 530.1854\nEpoch 3/1000\n16/16 [==============================] - 0s 2ms/step - loss: 518.1102 - mse: 518.1102\n~\nEpoch 998/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5184 - mse: 30.5184\nEpoch 999/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5257 - mse: 30.5257\nEpoch 1000/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\n\n아까 위에서 keras 없이 numpy를 써서 직접 계산한 최소 loss는 38.46이었다. \n지금 keras를 쓴 결과를 보면 똑같이 mse로 계산했지만 loss가 30.52로 더 줄어든 모습을 볼 수 있다. \n얘가 조금 더 정밀하게 계산됐음!\n'''","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:38:54.217962Z","iopub.execute_input":"2022-01-08T00:38:54.218257Z","iopub.status.idle":"2022-01-08T00:39:33.097531Z","shell.execute_reply.started":"2022-01-08T00:38:54.218228Z","shell.execute_reply":"2022-01-08T00:39:33.096767Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2022-01-08 00:38:54.293016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:54.385647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:54.386433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:54.387642: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-01-08 00:38:54.388856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:54.389527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:54.390185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:56.191714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:56.192534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:56.193220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-08 00:38:56.193835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2022-01-08 00:38:56.584169: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n16/16 [==============================] - 1s 2ms/step - loss: 542.3250 - mse: 542.3250\nEpoch 2/1000\n16/16 [==============================] - 0s 2ms/step - loss: 530.1854 - mse: 530.1854\nEpoch 3/1000\n16/16 [==============================] - 0s 2ms/step - loss: 518.1102 - mse: 518.1102\nEpoch 4/1000\n16/16 [==============================] - 0s 2ms/step - loss: 506.5363 - mse: 506.5363\nEpoch 5/1000\n16/16 [==============================] - 0s 2ms/step - loss: 494.9872 - mse: 494.9872\nEpoch 6/1000\n16/16 [==============================] - 0s 2ms/step - loss: 483.7516 - mse: 483.7516\nEpoch 7/1000\n16/16 [==============================] - 0s 2ms/step - loss: 472.8481 - mse: 472.8481\nEpoch 8/1000\n16/16 [==============================] - 0s 2ms/step - loss: 462.0984 - mse: 462.0984\nEpoch 9/1000\n16/16 [==============================] - 0s 2ms/step - loss: 451.5417 - mse: 451.5417\nEpoch 10/1000\n16/16 [==============================] - 0s 2ms/step - loss: 441.3442 - mse: 441.3442\nEpoch 11/1000\n16/16 [==============================] - 0s 2ms/step - loss: 431.2235 - mse: 431.2235\nEpoch 12/1000\n16/16 [==============================] - 0s 2ms/step - loss: 421.3737 - mse: 421.3737\nEpoch 13/1000\n16/16 [==============================] - 0s 2ms/step - loss: 411.8806 - mse: 411.8806\nEpoch 14/1000\n16/16 [==============================] - 0s 2ms/step - loss: 402.4603 - mse: 402.4603\nEpoch 15/1000\n16/16 [==============================] - 0s 5ms/step - loss: 393.2828 - mse: 393.2828\nEpoch 16/1000\n16/16 [==============================] - 0s 4ms/step - loss: 384.2758 - mse: 384.2758\nEpoch 17/1000\n16/16 [==============================] - 0s 3ms/step - loss: 375.5626 - mse: 375.5626\nEpoch 18/1000\n16/16 [==============================] - 0s 3ms/step - loss: 367.0663 - mse: 367.0663\nEpoch 19/1000\n16/16 [==============================] - 0s 2ms/step - loss: 358.6653 - mse: 358.6653\nEpoch 20/1000\n16/16 [==============================] - 0s 2ms/step - loss: 350.5188 - mse: 350.5188\nEpoch 21/1000\n16/16 [==============================] - 0s 3ms/step - loss: 342.6408 - mse: 342.6408\nEpoch 22/1000\n16/16 [==============================] - 0s 3ms/step - loss: 334.8469 - mse: 334.8469\nEpoch 23/1000\n16/16 [==============================] - 0s 3ms/step - loss: 327.2103 - mse: 327.2103\nEpoch 24/1000\n16/16 [==============================] - 0s 3ms/step - loss: 319.9573 - mse: 319.9573\nEpoch 25/1000\n16/16 [==============================] - 0s 3ms/step - loss: 312.6029 - mse: 312.6029\nEpoch 26/1000\n16/16 [==============================] - 0s 2ms/step - loss: 305.6520 - mse: 305.6520\nEpoch 27/1000\n16/16 [==============================] - 0s 2ms/step - loss: 298.8467 - mse: 298.8467\nEpoch 28/1000\n16/16 [==============================] - 0s 2ms/step - loss: 292.0953 - mse: 292.0953\nEpoch 29/1000\n16/16 [==============================] - 0s 3ms/step - loss: 285.6758 - mse: 285.6758\nEpoch 30/1000\n16/16 [==============================] - 0s 2ms/step - loss: 279.2625 - mse: 279.2625\nEpoch 31/1000\n16/16 [==============================] - 0s 2ms/step - loss: 273.1921 - mse: 273.1921\nEpoch 32/1000\n16/16 [==============================] - 0s 2ms/step - loss: 267.0537 - mse: 267.0537\nEpoch 33/1000\n16/16 [==============================] - 0s 2ms/step - loss: 261.3249 - mse: 261.3249\nEpoch 34/1000\n16/16 [==============================] - 0s 2ms/step - loss: 255.5922 - mse: 255.5922\nEpoch 35/1000\n16/16 [==============================] - 0s 2ms/step - loss: 249.9732 - mse: 249.9732\nEpoch 36/1000\n16/16 [==============================] - 0s 2ms/step - loss: 244.6116 - mse: 244.6116\nEpoch 37/1000\n16/16 [==============================] - 0s 2ms/step - loss: 239.4049 - mse: 239.4049\nEpoch 38/1000\n16/16 [==============================] - 0s 2ms/step - loss: 234.3025 - mse: 234.3025\nEpoch 39/1000\n16/16 [==============================] - 0s 2ms/step - loss: 229.2744 - mse: 229.2744\nEpoch 40/1000\n16/16 [==============================] - 0s 2ms/step - loss: 224.5275 - mse: 224.5275\nEpoch 41/1000\n16/16 [==============================] - 0s 2ms/step - loss: 219.7603 - mse: 219.7603\nEpoch 42/1000\n16/16 [==============================] - 0s 2ms/step - loss: 215.2268 - mse: 215.2268\nEpoch 43/1000\n16/16 [==============================] - 0s 2ms/step - loss: 210.8376 - mse: 210.8376\nEpoch 44/1000\n16/16 [==============================] - 0s 2ms/step - loss: 206.4800 - mse: 206.4800\nEpoch 45/1000\n16/16 [==============================] - 0s 2ms/step - loss: 202.3307 - mse: 202.3307\nEpoch 46/1000\n16/16 [==============================] - 0s 2ms/step - loss: 198.2697 - mse: 198.2697\nEpoch 47/1000\n16/16 [==============================] - 0s 2ms/step - loss: 194.3608 - mse: 194.3608\nEpoch 48/1000\n16/16 [==============================] - 0s 2ms/step - loss: 190.5640 - mse: 190.5640\nEpoch 49/1000\n16/16 [==============================] - 0s 2ms/step - loss: 186.8026 - mse: 186.8026\nEpoch 50/1000\n16/16 [==============================] - 0s 2ms/step - loss: 183.1598 - mse: 183.1598\nEpoch 51/1000\n16/16 [==============================] - 0s 2ms/step - loss: 179.7117 - mse: 179.7117\nEpoch 52/1000\n16/16 [==============================] - 0s 2ms/step - loss: 176.3457 - mse: 176.3457\nEpoch 53/1000\n16/16 [==============================] - 0s 2ms/step - loss: 173.0725 - mse: 173.0725\nEpoch 54/1000\n16/16 [==============================] - 0s 2ms/step - loss: 169.8350 - mse: 169.8350\nEpoch 55/1000\n16/16 [==============================] - 0s 2ms/step - loss: 166.7834 - mse: 166.7834\nEpoch 56/1000\n16/16 [==============================] - 0s 2ms/step - loss: 163.8087 - mse: 163.8087\nEpoch 57/1000\n16/16 [==============================] - 0s 2ms/step - loss: 160.9110 - mse: 160.9110\nEpoch 58/1000\n16/16 [==============================] - 0s 2ms/step - loss: 158.1160 - mse: 158.1160\nEpoch 59/1000\n16/16 [==============================] - 0s 2ms/step - loss: 155.4265 - mse: 155.4265\nEpoch 60/1000\n16/16 [==============================] - 0s 2ms/step - loss: 152.7597 - mse: 152.7597\nEpoch 61/1000\n16/16 [==============================] - 0s 2ms/step - loss: 150.2577 - mse: 150.2577\nEpoch 62/1000\n16/16 [==============================] - 0s 2ms/step - loss: 147.7994 - mse: 147.7994\nEpoch 63/1000\n16/16 [==============================] - 0s 2ms/step - loss: 145.4083 - mse: 145.4083\nEpoch 64/1000\n16/16 [==============================] - 0s 2ms/step - loss: 143.0844 - mse: 143.0844\nEpoch 65/1000\n16/16 [==============================] - 0s 2ms/step - loss: 140.9186 - mse: 140.9186\nEpoch 66/1000\n16/16 [==============================] - 0s 2ms/step - loss: 138.7397 - mse: 138.7397\nEpoch 67/1000\n16/16 [==============================] - 0s 2ms/step - loss: 136.6219 - mse: 136.6219\nEpoch 68/1000\n16/16 [==============================] - 0s 2ms/step - loss: 134.6322 - mse: 134.6322\nEpoch 69/1000\n16/16 [==============================] - 0s 2ms/step - loss: 132.7247 - mse: 132.7247\nEpoch 70/1000\n16/16 [==============================] - 0s 2ms/step - loss: 130.7898 - mse: 130.7898\nEpoch 71/1000\n16/16 [==============================] - 0s 2ms/step - loss: 128.9874 - mse: 128.9874\nEpoch 72/1000\n16/16 [==============================] - 0s 2ms/step - loss: 127.2421 - mse: 127.2421\nEpoch 73/1000\n16/16 [==============================] - 0s 2ms/step - loss: 125.5396 - mse: 125.5396\nEpoch 74/1000\n16/16 [==============================] - 0s 2ms/step - loss: 123.8921 - mse: 123.8921\nEpoch 75/1000\n16/16 [==============================] - 0s 2ms/step - loss: 122.3302 - mse: 122.3302\nEpoch 76/1000\n16/16 [==============================] - 0s 2ms/step - loss: 120.7919 - mse: 120.7919\nEpoch 77/1000\n16/16 [==============================] - 0s 2ms/step - loss: 119.2947 - mse: 119.2947\nEpoch 78/1000\n16/16 [==============================] - 0s 2ms/step - loss: 117.8842 - mse: 117.8842\nEpoch 79/1000\n16/16 [==============================] - 0s 2ms/step - loss: 116.5190 - mse: 116.5190\nEpoch 80/1000\n16/16 [==============================] - 0s 2ms/step - loss: 115.1525 - mse: 115.1525\nEpoch 81/1000\n16/16 [==============================] - 0s 2ms/step - loss: 113.8572 - mse: 113.8572\nEpoch 82/1000\n16/16 [==============================] - 0s 2ms/step - loss: 112.6524 - mse: 112.6524\nEpoch 83/1000\n16/16 [==============================] - 0s 2ms/step - loss: 111.4379 - mse: 111.4379\nEpoch 84/1000\n16/16 [==============================] - 0s 2ms/step - loss: 110.2291 - mse: 110.2291\nEpoch 85/1000\n16/16 [==============================] - 0s 2ms/step - loss: 109.1099 - mse: 109.1099\nEpoch 86/1000\n16/16 [==============================] - 0s 2ms/step - loss: 108.0679 - mse: 108.0679\nEpoch 87/1000\n16/16 [==============================] - 0s 2ms/step - loss: 106.9688 - mse: 106.9688\nEpoch 88/1000\n16/16 [==============================] - 0s 2ms/step - loss: 105.9680 - mse: 105.9680\nEpoch 89/1000\n16/16 [==============================] - 0s 2ms/step - loss: 104.9461 - mse: 104.9461\nEpoch 90/1000\n16/16 [==============================] - 0s 2ms/step - loss: 104.0311 - mse: 104.0311\nEpoch 91/1000\n16/16 [==============================] - 0s 2ms/step - loss: 103.0649 - mse: 103.0649\nEpoch 92/1000\n16/16 [==============================] - 0s 2ms/step - loss: 102.1713 - mse: 102.1713\nEpoch 93/1000\n16/16 [==============================] - 0s 2ms/step - loss: 101.2888 - mse: 101.2888\nEpoch 94/1000\n16/16 [==============================] - 0s 2ms/step - loss: 100.4589 - mse: 100.4589\nEpoch 95/1000\n16/16 [==============================] - 0s 2ms/step - loss: 99.5871 - mse: 99.5871\nEpoch 96/1000\n16/16 [==============================] - 0s 2ms/step - loss: 98.8159 - mse: 98.8159\nEpoch 97/1000\n16/16 [==============================] - 0s 2ms/step - loss: 98.0267 - mse: 98.0267\nEpoch 98/1000\n16/16 [==============================] - 0s 2ms/step - loss: 97.2667 - mse: 97.2667\nEpoch 99/1000\n16/16 [==============================] - 0s 2ms/step - loss: 96.5281 - mse: 96.5281\nEpoch 100/1000\n16/16 [==============================] - 0s 2ms/step - loss: 95.8144 - mse: 95.8144\nEpoch 101/1000\n16/16 [==============================] - 0s 2ms/step - loss: 95.1240 - mse: 95.1240\nEpoch 102/1000\n16/16 [==============================] - 0s 2ms/step - loss: 94.4141 - mse: 94.4141\nEpoch 103/1000\n16/16 [==============================] - 0s 2ms/step - loss: 93.7695 - mse: 93.7695\nEpoch 104/1000\n16/16 [==============================] - 0s 2ms/step - loss: 93.1391 - mse: 93.1391\nEpoch 105/1000\n16/16 [==============================] - 0s 2ms/step - loss: 92.4749 - mse: 92.4749\nEpoch 106/1000\n16/16 [==============================] - 0s 2ms/step - loss: 91.8841 - mse: 91.8841\nEpoch 107/1000\n16/16 [==============================] - 0s 2ms/step - loss: 91.2628 - mse: 91.2628\nEpoch 108/1000\n16/16 [==============================] - 0s 2ms/step - loss: 90.6627 - mse: 90.6627\nEpoch 109/1000\n16/16 [==============================] - 0s 2ms/step - loss: 90.0917 - mse: 90.0917\nEpoch 110/1000\n16/16 [==============================] - 0s 2ms/step - loss: 89.5601 - mse: 89.5601\nEpoch 111/1000\n16/16 [==============================] - 0s 2ms/step - loss: 88.9625 - mse: 88.9625\nEpoch 112/1000\n16/16 [==============================] - 0s 2ms/step - loss: 88.4423 - mse: 88.4423\nEpoch 113/1000\n16/16 [==============================] - 0s 2ms/step - loss: 87.9138 - mse: 87.9138\nEpoch 114/1000\n16/16 [==============================] - 0s 2ms/step - loss: 87.3548 - mse: 87.3548\nEpoch 115/1000\n16/16 [==============================] - 0s 2ms/step - loss: 86.8696 - mse: 86.8696\nEpoch 116/1000\n16/16 [==============================] - 0s 2ms/step - loss: 86.3483 - mse: 86.3483\nEpoch 117/1000\n16/16 [==============================] - 0s 2ms/step - loss: 85.8300 - mse: 85.8300\nEpoch 118/1000\n16/16 [==============================] - 0s 2ms/step - loss: 85.3709 - mse: 85.3709\nEpoch 119/1000\n16/16 [==============================] - 0s 2ms/step - loss: 84.8703 - mse: 84.8703\nEpoch 120/1000\n16/16 [==============================] - 0s 2ms/step - loss: 84.4024 - mse: 84.4024\nEpoch 121/1000\n16/16 [==============================] - 0s 2ms/step - loss: 83.9310 - mse: 83.9310\nEpoch 122/1000\n16/16 [==============================] - 0s 2ms/step - loss: 83.4687 - mse: 83.4687\nEpoch 123/1000\n16/16 [==============================] - 0s 2ms/step - loss: 82.9921 - mse: 82.9921\nEpoch 124/1000\n16/16 [==============================] - 0s 2ms/step - loss: 82.5617 - mse: 82.5617\nEpoch 125/1000\n16/16 [==============================] - 0s 2ms/step - loss: 82.1120 - mse: 82.1120\nEpoch 126/1000\n16/16 [==============================] - 0s 2ms/step - loss: 81.6534 - mse: 81.6534\nEpoch 127/1000\n16/16 [==============================] - 0s 2ms/step - loss: 81.2299 - mse: 81.2299\nEpoch 128/1000\n16/16 [==============================] - 0s 2ms/step - loss: 80.7834 - mse: 80.7834\nEpoch 129/1000\n16/16 [==============================] - 0s 2ms/step - loss: 80.3557 - mse: 80.3557\nEpoch 130/1000\n16/16 [==============================] - 0s 2ms/step - loss: 79.9234 - mse: 79.9234\nEpoch 131/1000\n16/16 [==============================] - 0s 2ms/step - loss: 79.5087 - mse: 79.5087\nEpoch 132/1000\n16/16 [==============================] - 0s 2ms/step - loss: 79.0875 - mse: 79.0875\nEpoch 133/1000\n16/16 [==============================] - 0s 2ms/step - loss: 78.6752 - mse: 78.6752\nEpoch 134/1000\n16/16 [==============================] - 0s 2ms/step - loss: 78.2681 - mse: 78.2681\nEpoch 135/1000\n16/16 [==============================] - 0s 2ms/step - loss: 77.8609 - mse: 77.8609\nEpoch 136/1000\n16/16 [==============================] - 0s 2ms/step - loss: 77.4612 - mse: 77.4612\nEpoch 137/1000\n16/16 [==============================] - 0s 2ms/step - loss: 77.0659 - mse: 77.0659\nEpoch 138/1000\n16/16 [==============================] - 0s 2ms/step - loss: 76.6530 - mse: 76.6530\nEpoch 139/1000\n16/16 [==============================] - 0s 2ms/step - loss: 76.2604 - mse: 76.2604\nEpoch 140/1000\n16/16 [==============================] - 0s 2ms/step - loss: 75.8750 - mse: 75.8750\nEpoch 141/1000\n16/16 [==============================] - 0s 2ms/step - loss: 75.4862 - mse: 75.4862\nEpoch 142/1000\n16/16 [==============================] - 0s 2ms/step - loss: 75.0938 - mse: 75.0938\nEpoch 143/1000\n16/16 [==============================] - 0s 2ms/step - loss: 74.7222 - mse: 74.7222\nEpoch 144/1000\n16/16 [==============================] - 0s 2ms/step - loss: 74.3303 - mse: 74.3303\nEpoch 145/1000\n16/16 [==============================] - 0s 2ms/step - loss: 73.9583 - mse: 73.9583\nEpoch 146/1000\n16/16 [==============================] - 0s 2ms/step - loss: 73.5794 - mse: 73.5794\nEpoch 147/1000\n16/16 [==============================] - 0s 2ms/step - loss: 73.2002 - mse: 73.2002\nEpoch 148/1000\n16/16 [==============================] - 0s 2ms/step - loss: 72.8542 - mse: 72.8542\nEpoch 149/1000\n16/16 [==============================] - 0s 2ms/step - loss: 72.4584 - mse: 72.4584\nEpoch 150/1000\n16/16 [==============================] - 0s 2ms/step - loss: 72.0991 - mse: 72.0991\nEpoch 151/1000\n16/16 [==============================] - 0s 2ms/step - loss: 71.7336 - mse: 71.7336\nEpoch 152/1000\n16/16 [==============================] - 0s 2ms/step - loss: 71.3728 - mse: 71.3728\nEpoch 153/1000\n16/16 [==============================] - 0s 2ms/step - loss: 71.0078 - mse: 71.0078\nEpoch 154/1000\n16/16 [==============================] - 0s 2ms/step - loss: 70.6484 - mse: 70.6484\nEpoch 155/1000\n16/16 [==============================] - 0s 2ms/step - loss: 70.2920 - mse: 70.2920\nEpoch 156/1000\n16/16 [==============================] - 0s 2ms/step - loss: 69.9350 - mse: 69.9350\nEpoch 157/1000\n16/16 [==============================] - 0s 2ms/step - loss: 69.5792 - mse: 69.5792\nEpoch 158/1000\n16/16 [==============================] - 0s 2ms/step - loss: 69.2287 - mse: 69.2287\nEpoch 159/1000\n16/16 [==============================] - 0s 2ms/step - loss: 68.8894 - mse: 68.8894\nEpoch 160/1000\n16/16 [==============================] - 0s 2ms/step - loss: 68.5369 - mse: 68.5369\nEpoch 161/1000\n16/16 [==============================] - 0s 2ms/step - loss: 68.1837 - mse: 68.1837\nEpoch 162/1000\n16/16 [==============================] - 0s 2ms/step - loss: 67.8468 - mse: 67.8468\nEpoch 163/1000\n16/16 [==============================] - 0s 2ms/step - loss: 67.5103 - mse: 67.5103\nEpoch 164/1000\n16/16 [==============================] - 0s 2ms/step - loss: 67.1661 - mse: 67.1661\nEpoch 165/1000\n16/16 [==============================] - 0s 2ms/step - loss: 66.8212 - mse: 66.8212\nEpoch 166/1000\n16/16 [==============================] - 0s 2ms/step - loss: 66.4906 - mse: 66.4906\nEpoch 167/1000\n16/16 [==============================] - 0s 2ms/step - loss: 66.1543 - mse: 66.1543\nEpoch 168/1000\n16/16 [==============================] - 0s 2ms/step - loss: 65.8246 - mse: 65.8246\nEpoch 169/1000\n16/16 [==============================] - 0s 2ms/step - loss: 65.4901 - mse: 65.4901\nEpoch 170/1000\n16/16 [==============================] - 0s 2ms/step - loss: 65.1766 - mse: 65.1766\nEpoch 171/1000\n16/16 [==============================] - 0s 2ms/step - loss: 64.8293 - mse: 64.8293\nEpoch 172/1000\n16/16 [==============================] - 0s 2ms/step - loss: 64.5178 - mse: 64.5178\nEpoch 173/1000\n16/16 [==============================] - 0s 2ms/step - loss: 64.1871 - mse: 64.1871\nEpoch 174/1000\n16/16 [==============================] - 0s 2ms/step - loss: 63.8723 - mse: 63.8723\nEpoch 175/1000\n16/16 [==============================] - 0s 2ms/step - loss: 63.5539 - mse: 63.5539\nEpoch 176/1000\n16/16 [==============================] - 0s 2ms/step - loss: 63.2265 - mse: 63.2265\nEpoch 177/1000\n16/16 [==============================] - 0s 2ms/step - loss: 62.9104 - mse: 62.9104\nEpoch 178/1000\n16/16 [==============================] - 0s 2ms/step - loss: 62.5989 - mse: 62.5989\nEpoch 179/1000\n16/16 [==============================] - 0s 2ms/step - loss: 62.2862 - mse: 62.2862\nEpoch 180/1000\n16/16 [==============================] - 0s 2ms/step - loss: 61.9849 - mse: 61.9849\nEpoch 181/1000\n16/16 [==============================] - 0s 2ms/step - loss: 61.6671 - mse: 61.6671\nEpoch 182/1000\n16/16 [==============================] - 0s 2ms/step - loss: 61.3545 - mse: 61.3545\nEpoch 183/1000\n16/16 [==============================] - 0s 2ms/step - loss: 61.0631 - mse: 61.0631\nEpoch 184/1000\n16/16 [==============================] - 0s 2ms/step - loss: 60.7555 - mse: 60.7555\nEpoch 185/1000\n16/16 [==============================] - 0s 2ms/step - loss: 60.4540 - mse: 60.4540\nEpoch 186/1000\n16/16 [==============================] - 0s 2ms/step - loss: 60.1574 - mse: 60.1574\nEpoch 187/1000\n16/16 [==============================] - 0s 2ms/step - loss: 59.8552 - mse: 59.8552\nEpoch 188/1000\n16/16 [==============================] - 0s 2ms/step - loss: 59.5521 - mse: 59.5521\nEpoch 189/1000\n16/16 [==============================] - 0s 2ms/step - loss: 59.2595 - mse: 59.2595\nEpoch 190/1000\n16/16 [==============================] - 0s 2ms/step - loss: 58.9760 - mse: 58.9760\nEpoch 191/1000\n16/16 [==============================] - 0s 2ms/step - loss: 58.6797 - mse: 58.6797\nEpoch 192/1000\n16/16 [==============================] - 0s 2ms/step - loss: 58.3873 - mse: 58.3873\nEpoch 193/1000\n16/16 [==============================] - 0s 2ms/step - loss: 58.1076 - mse: 58.1076\nEpoch 194/1000\n16/16 [==============================] - 0s 2ms/step - loss: 57.8258 - mse: 57.8258\nEpoch 195/1000\n16/16 [==============================] - 0s 2ms/step - loss: 57.5254 - mse: 57.5254\nEpoch 196/1000\n16/16 [==============================] - 0s 2ms/step - loss: 57.2518 - mse: 57.2518\nEpoch 197/1000\n16/16 [==============================] - 0s 2ms/step - loss: 56.9744 - mse: 56.9744\nEpoch 198/1000\n16/16 [==============================] - 0s 2ms/step - loss: 56.6884 - mse: 56.6884\nEpoch 199/1000\n16/16 [==============================] - 0s 2ms/step - loss: 56.4270 - mse: 56.4270\nEpoch 200/1000\n16/16 [==============================] - 0s 2ms/step - loss: 56.1339 - mse: 56.1339\nEpoch 201/1000\n16/16 [==============================] - 0s 2ms/step - loss: 55.8704 - mse: 55.8704\nEpoch 202/1000\n16/16 [==============================] - 0s 2ms/step - loss: 55.5916 - mse: 55.5916\nEpoch 203/1000\n16/16 [==============================] - 0s 2ms/step - loss: 55.3310 - mse: 55.3310\nEpoch 204/1000\n16/16 [==============================] - 0s 2ms/step - loss: 55.0671 - mse: 55.0671\nEpoch 205/1000\n16/16 [==============================] - 0s 2ms/step - loss: 54.7933 - mse: 54.7933\nEpoch 206/1000\n16/16 [==============================] - 0s 2ms/step - loss: 54.5305 - mse: 54.5305\nEpoch 207/1000\n16/16 [==============================] - 0s 2ms/step - loss: 54.2683 - mse: 54.2683\nEpoch 208/1000\n16/16 [==============================] - 0s 2ms/step - loss: 54.0099 - mse: 54.0099\nEpoch 209/1000\n16/16 [==============================] - 0s 2ms/step - loss: 53.7520 - mse: 53.7520\nEpoch 210/1000\n16/16 [==============================] - 0s 2ms/step - loss: 53.4994 - mse: 53.4994\nEpoch 211/1000\n16/16 [==============================] - 0s 2ms/step - loss: 53.2385 - mse: 53.2385\nEpoch 212/1000\n16/16 [==============================] - 0s 2ms/step - loss: 52.9865 - mse: 52.9865\nEpoch 213/1000\n16/16 [==============================] - 0s 2ms/step - loss: 52.7331 - mse: 52.7331\nEpoch 214/1000\n16/16 [==============================] - 0s 2ms/step - loss: 52.4920 - mse: 52.4920\nEpoch 215/1000\n16/16 [==============================] - 0s 2ms/step - loss: 52.2366 - mse: 52.2366\nEpoch 216/1000\n16/16 [==============================] - 0s 2ms/step - loss: 51.9872 - mse: 51.9872\nEpoch 217/1000\n16/16 [==============================] - 0s 3ms/step - loss: 51.7507 - mse: 51.7507\nEpoch 218/1000\n16/16 [==============================] - 0s 2ms/step - loss: 51.4985 - mse: 51.4985\nEpoch 219/1000\n16/16 [==============================] - 0s 2ms/step - loss: 51.2615 - mse: 51.2615\nEpoch 220/1000\n16/16 [==============================] - 0s 2ms/step - loss: 51.0330 - mse: 51.0330\nEpoch 221/1000\n16/16 [==============================] - 0s 2ms/step - loss: 50.7849 - mse: 50.7849\nEpoch 222/1000\n16/16 [==============================] - 0s 2ms/step - loss: 50.5583 - mse: 50.5583\nEpoch 223/1000\n16/16 [==============================] - 0s 2ms/step - loss: 50.3165 - mse: 50.3165\nEpoch 224/1000\n16/16 [==============================] - 0s 2ms/step - loss: 50.0879 - mse: 50.0879\nEpoch 225/1000\n16/16 [==============================] - 0s 2ms/step - loss: 49.8685 - mse: 49.8685\nEpoch 226/1000\n16/16 [==============================] - 0s 2ms/step - loss: 49.6252 - mse: 49.6252\nEpoch 227/1000\n16/16 [==============================] - 0s 2ms/step - loss: 49.4036 - mse: 49.4036\nEpoch 228/1000\n16/16 [==============================] - 0s 2ms/step - loss: 49.1864 - mse: 49.1864\nEpoch 229/1000\n16/16 [==============================] - 0s 2ms/step - loss: 48.9482 - mse: 48.9482\nEpoch 230/1000\n16/16 [==============================] - 0s 2ms/step - loss: 48.7402 - mse: 48.7402\nEpoch 231/1000\n16/16 [==============================] - 0s 2ms/step - loss: 48.5054 - mse: 48.5054\nEpoch 232/1000\n16/16 [==============================] - 0s 2ms/step - loss: 48.2956 - mse: 48.2956\nEpoch 233/1000\n16/16 [==============================] - 0s 2ms/step - loss: 48.0813 - mse: 48.0813\nEpoch 234/1000\n16/16 [==============================] - 0s 2ms/step - loss: 47.8695 - mse: 47.8695\nEpoch 235/1000\n16/16 [==============================] - 0s 2ms/step - loss: 47.6485 - mse: 47.6485\nEpoch 236/1000\n16/16 [==============================] - 0s 2ms/step - loss: 47.4511 - mse: 47.4511\nEpoch 237/1000\n16/16 [==============================] - 0s 2ms/step - loss: 47.2307 - mse: 47.2307\nEpoch 238/1000\n16/16 [==============================] - 0s 2ms/step - loss: 47.0303 - mse: 47.0303\nEpoch 239/1000\n16/16 [==============================] - 0s 2ms/step - loss: 46.8310 - mse: 46.8310\nEpoch 240/1000\n16/16 [==============================] - 0s 2ms/step - loss: 46.6256 - mse: 46.6256\nEpoch 241/1000\n16/16 [==============================] - 0s 2ms/step - loss: 46.4211 - mse: 46.4211\nEpoch 242/1000\n16/16 [==============================] - 0s 2ms/step - loss: 46.2208 - mse: 46.2208\nEpoch 243/1000\n16/16 [==============================] - 0s 2ms/step - loss: 46.0338 - mse: 46.0338\nEpoch 244/1000\n16/16 [==============================] - 0s 2ms/step - loss: 45.8174 - mse: 45.8174\nEpoch 245/1000\n16/16 [==============================] - 0s 2ms/step - loss: 45.6303 - mse: 45.6303\nEpoch 246/1000\n16/16 [==============================] - 0s 2ms/step - loss: 45.4306 - mse: 45.4306\nEpoch 247/1000\n16/16 [==============================] - 0s 2ms/step - loss: 45.2493 - mse: 45.2493\nEpoch 248/1000\n16/16 [==============================] - 0s 2ms/step - loss: 45.0505 - mse: 45.0505\nEpoch 249/1000\n16/16 [==============================] - 0s 2ms/step - loss: 44.8684 - mse: 44.8684\nEpoch 250/1000\n16/16 [==============================] - 0s 2ms/step - loss: 44.6756 - mse: 44.6756\nEpoch 251/1000\n16/16 [==============================] - 0s 2ms/step - loss: 44.4871 - mse: 44.4871\nEpoch 252/1000\n16/16 [==============================] - 0s 2ms/step - loss: 44.3105 - mse: 44.3105\nEpoch 253/1000\n16/16 [==============================] - 0s 2ms/step - loss: 44.1281 - mse: 44.1281\nEpoch 254/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.9450 - mse: 43.9450\nEpoch 255/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.7715 - mse: 43.7715\nEpoch 256/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.5921 - mse: 43.5921\nEpoch 257/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.4122 - mse: 43.4122\nEpoch 258/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.2424 - mse: 43.2424\nEpoch 259/1000\n16/16 [==============================] - 0s 2ms/step - loss: 43.0655 - mse: 43.0655\nEpoch 260/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.8911 - mse: 42.8911\nEpoch 261/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.7308 - mse: 42.7308\nEpoch 262/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.5544 - mse: 42.5544\nEpoch 263/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.3948 - mse: 42.3948\nEpoch 264/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.2362 - mse: 42.2362\nEpoch 265/1000\n16/16 [==============================] - 0s 2ms/step - loss: 42.0576 - mse: 42.0576\nEpoch 266/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.9027 - mse: 41.9027\nEpoch 267/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.7411 - mse: 41.7411\nEpoch 268/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.5822 - mse: 41.5822\nEpoch 269/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.4233 - mse: 41.4233\nEpoch 270/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.2702 - mse: 41.2702\nEpoch 271/1000\n16/16 [==============================] - 0s 2ms/step - loss: 41.1184 - mse: 41.1184\nEpoch 272/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.9714 - mse: 40.9714\nEpoch 273/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.8136 - mse: 40.8136\nEpoch 274/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.6687 - mse: 40.6687\nEpoch 275/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.5164 - mse: 40.5164\nEpoch 276/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.3777 - mse: 40.3777\nEpoch 277/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.2283 - mse: 40.2283\nEpoch 278/1000\n16/16 [==============================] - 0s 2ms/step - loss: 40.0825 - mse: 40.0825\nEpoch 279/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.9514 - mse: 39.9514\nEpoch 280/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.8033 - mse: 39.8033\nEpoch 281/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.6631 - mse: 39.6631\nEpoch 282/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.5235 - mse: 39.5235\nEpoch 283/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.3894 - mse: 39.3894\nEpoch 284/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.2528 - mse: 39.2528\nEpoch 285/1000\n16/16 [==============================] - 0s 2ms/step - loss: 39.1212 - mse: 39.1212\nEpoch 286/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.9932 - mse: 38.9932\nEpoch 287/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.8634 - mse: 38.8634\nEpoch 288/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.7263 - mse: 38.7263\nEpoch 289/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.5989 - mse: 38.5989\nEpoch 290/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.4802 - mse: 38.4802\nEpoch 291/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.3464 - mse: 38.3464\nEpoch 292/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.2270 - mse: 38.2270\nEpoch 293/1000\n16/16 [==============================] - 0s 2ms/step - loss: 38.1049 - mse: 38.1049\nEpoch 294/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.9845 - mse: 37.9845\nEpoch 295/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.8676 - mse: 37.8676\nEpoch 296/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.7444 - mse: 37.7444\nEpoch 297/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.6359 - mse: 37.6359\nEpoch 298/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.5162 - mse: 37.5162\nEpoch 299/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.4170 - mse: 37.4170\nEpoch 300/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.2921 - mse: 37.2921\nEpoch 301/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.1822 - mse: 37.1822\nEpoch 302/1000\n16/16 [==============================] - 0s 2ms/step - loss: 37.0740 - mse: 37.0740\nEpoch 303/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.9686 - mse: 36.9686\nEpoch 304/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.8581 - mse: 36.8581\nEpoch 305/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.7547 - mse: 36.7547\nEpoch 306/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.6473 - mse: 36.6473\nEpoch 307/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.5472 - mse: 36.5472\nEpoch 308/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.4448 - mse: 36.4448\nEpoch 309/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.3419 - mse: 36.3419\nEpoch 310/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.2461 - mse: 36.2461\nEpoch 311/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.1564 - mse: 36.1564\nEpoch 312/1000\n16/16 [==============================] - 0s 2ms/step - loss: 36.0487 - mse: 36.0487\nEpoch 313/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.9522 - mse: 35.9522\nEpoch 314/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.8659 - mse: 35.8659\nEpoch 315/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.7630 - mse: 35.7630\nEpoch 316/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.6715 - mse: 35.6715\nEpoch 317/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.5807 - mse: 35.5807\nEpoch 318/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.4923 - mse: 35.4923\nEpoch 319/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.4012 - mse: 35.4012\nEpoch 320/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.3198 - mse: 35.3198\nEpoch 321/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.2286 - mse: 35.2286\nEpoch 322/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.1408 - mse: 35.1408\nEpoch 323/1000\n16/16 [==============================] - 0s 2ms/step - loss: 35.0642 - mse: 35.0642\nEpoch 324/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.9778 - mse: 34.9778\nEpoch 325/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.8972 - mse: 34.8972\nEpoch 326/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.8158 - mse: 34.8158\nEpoch 327/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.7384 - mse: 34.7384\nEpoch 328/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.6614 - mse: 34.6614\nEpoch 329/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.5804 - mse: 34.5804\nEpoch 330/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.5058 - mse: 34.5058\nEpoch 331/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.4310 - mse: 34.4310\nEpoch 332/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.3560 - mse: 34.3560\nEpoch 333/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.2810 - mse: 34.2810\nEpoch 334/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.2176 - mse: 34.2176\nEpoch 335/1000\n16/16 [==============================] - 0s 2ms/step - loss: 34.1417 - mse: 34.1417\nEpoch 336/1000\n16/16 [==============================] - ETA: 0s - loss: 34.8127 - mse: 34.812 - 0s 4ms/step - loss: 34.0744 - mse: 34.0744\nEpoch 337/1000\n16/16 [==============================] - 0s 4ms/step - loss: 34.0068 - mse: 34.0068\nEpoch 338/1000\n16/16 [==============================] - 0s 3ms/step - loss: 33.9358 - mse: 33.9358\nEpoch 339/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.8733 - mse: 33.8733\nEpoch 340/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.8024 - mse: 33.8024\nEpoch 341/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.7413 - mse: 33.7413\nEpoch 342/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.6740 - mse: 33.6740\nEpoch 343/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.6202 - mse: 33.6202\nEpoch 344/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.5544 - mse: 33.5544\nEpoch 345/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.4973 - mse: 33.4973\nEpoch 346/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.4378 - mse: 33.4378\nEpoch 347/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.3731 - mse: 33.3731\nEpoch 348/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.3123 - mse: 33.3123\nEpoch 349/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.2592 - mse: 33.2592\nEpoch 350/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.2031 - mse: 33.2031\nEpoch 351/1000\n16/16 [==============================] - 0s 3ms/step - loss: 33.1426 - mse: 33.1426\nEpoch 352/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.0954 - mse: 33.0954\nEpoch 353/1000\n16/16 [==============================] - 0s 2ms/step - loss: 33.0374 - mse: 33.0374\nEpoch 354/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.9881 - mse: 32.9881\nEpoch 355/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.9377 - mse: 32.9377\nEpoch 356/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.8816 - mse: 32.8816\nEpoch 357/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.8329 - mse: 32.8329\nEpoch 358/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.7878 - mse: 32.7878\nEpoch 359/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.7414 - mse: 32.7414\nEpoch 360/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.6934 - mse: 32.6934\nEpoch 361/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.6510 - mse: 32.6510\nEpoch 362/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.5974 - mse: 32.5974\nEpoch 363/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.5558 - mse: 32.5558\nEpoch 364/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.5141 - mse: 32.5141\nEpoch 365/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.4664 - mse: 32.4664\nEpoch 366/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.4216 - mse: 32.4216\nEpoch 367/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.3892 - mse: 32.3892\nEpoch 368/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.3410 - mse: 32.3410\nEpoch 369/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.2989 - mse: 32.2989\nEpoch 370/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.2584 - mse: 32.2584\nEpoch 371/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.2201 - mse: 32.2201\nEpoch 372/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.1824 - mse: 32.1824\nEpoch 373/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.1513 - mse: 32.1513\nEpoch 374/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.1127 - mse: 32.1127\nEpoch 375/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.0729 - mse: 32.0729\nEpoch 376/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.0381 - mse: 32.0381\nEpoch 377/1000\n16/16 [==============================] - 0s 2ms/step - loss: 32.0361 - mse: 32.0361\nEpoch 378/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.9779 - mse: 31.9779\nEpoch 379/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.9391 - mse: 31.9391\nEpoch 380/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.9037 - mse: 31.9037\nEpoch 381/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.8730 - mse: 31.8730\nEpoch 382/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.8392 - mse: 31.8392\nEpoch 383/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.8088 - mse: 31.8088\nEpoch 384/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.7802 - mse: 31.7802\nEpoch 385/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.7462 - mse: 31.7462\nEpoch 386/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.7232 - mse: 31.7232\nEpoch 387/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.6944 - mse: 31.6944\nEpoch 388/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.6738 - mse: 31.6738\nEpoch 389/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.6357 - mse: 31.6357\nEpoch 390/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.6122 - mse: 31.6122\nEpoch 391/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.5828 - mse: 31.5828\nEpoch 392/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.5734 - mse: 31.5734\nEpoch 393/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.5350 - mse: 31.5350\nEpoch 394/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.5073 - mse: 31.5073\nEpoch 395/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.4823 - mse: 31.4823\nEpoch 396/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.4586 - mse: 31.4586\nEpoch 397/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.4401 - mse: 31.4401\nEpoch 398/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.4223 - mse: 31.4223\nEpoch 399/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.3907 - mse: 31.3907\nEpoch 400/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.3718 - mse: 31.3718\nEpoch 401/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.3507 - mse: 31.3507\nEpoch 402/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.3523 - mse: 31.3523\nEpoch 403/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.3014 - mse: 31.3014\nEpoch 404/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.2896 - mse: 31.2896\nEpoch 405/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.2665 - mse: 31.2665\nEpoch 406/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.2465 - mse: 31.2465\nEpoch 407/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.2281 - mse: 31.2281\nEpoch 408/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.2101 - mse: 31.2101\nEpoch 409/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1935 - mse: 31.1935\nEpoch 410/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1809 - mse: 31.1809\nEpoch 411/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1561 - mse: 31.1561\nEpoch 412/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1412 - mse: 31.1412\nEpoch 413/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1212 - mse: 31.1212\nEpoch 414/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.1105 - mse: 31.1105\nEpoch 415/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0954 - mse: 31.0954\nEpoch 416/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0767 - mse: 31.0767\nEpoch 417/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0647 - mse: 31.0647\nEpoch 418/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0480 - mse: 31.0480\nEpoch 419/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0341 - mse: 31.0341\nEpoch 420/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0256 - mse: 31.0256\nEpoch 421/1000\n16/16 [==============================] - 0s 2ms/step - loss: 31.0064 - mse: 31.0064\nEpoch 422/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9931 - mse: 30.9931\nEpoch 423/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9795 - mse: 30.9795\nEpoch 424/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9712 - mse: 30.9712\nEpoch 425/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9576 - mse: 30.9576\nEpoch 426/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9497 - mse: 30.9497\nEpoch 427/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9308 - mse: 30.9308\nEpoch 428/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9231 - mse: 30.9231\nEpoch 429/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.9096 - mse: 30.9096\nEpoch 430/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8997 - mse: 30.8997\nEpoch 431/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8935 - mse: 30.8935\nEpoch 432/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8789 - mse: 30.8789\nEpoch 433/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8852 - mse: 30.8852\nEpoch 434/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8564 - mse: 30.8564\nEpoch 435/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8507 - mse: 30.8507\nEpoch 436/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8478 - mse: 30.8478\nEpoch 437/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8310 - mse: 30.8310\nEpoch 438/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8283 - mse: 30.8283\nEpoch 439/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8175 - mse: 30.8175\nEpoch 440/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8085 - mse: 30.8085\nEpoch 441/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8083 - mse: 30.8083\nEpoch 442/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.8012 - mse: 30.8012\nEpoch 443/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7842 - mse: 30.7842\nEpoch 444/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7797 - mse: 30.7797\nEpoch 445/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7718 - mse: 30.7718\nEpoch 446/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7767 - mse: 30.7767\nEpoch 447/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7578 - mse: 30.7578\nEpoch 448/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7482 - mse: 30.7482\nEpoch 449/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7438 - mse: 30.7438\nEpoch 450/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7391 - mse: 30.7391\nEpoch 451/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7326 - mse: 30.7326\nEpoch 452/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7231 - mse: 30.7231\nEpoch 453/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7235 - mse: 30.7235\nEpoch 454/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7224 - mse: 30.7224\nEpoch 455/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7104 - mse: 30.7104\nEpoch 456/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7068 - mse: 30.7068\nEpoch 457/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.7067 - mse: 30.7067\nEpoch 458/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6908 - mse: 30.6908\nEpoch 459/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6954 - mse: 30.6954\nEpoch 460/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6885 - mse: 30.6885\nEpoch 461/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6855 - mse: 30.6855\nEpoch 462/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6754 - mse: 30.6754\nEpoch 463/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6752 - mse: 30.6752\nEpoch 464/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6673 - mse: 30.6673\nEpoch 465/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6656 - mse: 30.6656\nEpoch 466/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6579 - mse: 30.6579\nEpoch 467/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6588 - mse: 30.6588\nEpoch 468/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6492 - mse: 30.6492\nEpoch 469/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6538 - mse: 30.6538\nEpoch 470/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6429 - mse: 30.6429\nEpoch 471/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6399 - mse: 30.6399\nEpoch 472/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6443 - mse: 30.6443\nEpoch 473/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6390 - mse: 30.6390\nEpoch 474/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6341 - mse: 30.6341\nEpoch 475/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6306 - mse: 30.6306\nEpoch 476/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6435 - mse: 30.6435\nEpoch 477/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6222 - mse: 30.6222\nEpoch 478/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6213 - mse: 30.6213\nEpoch 479/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6220 - mse: 30.6220\nEpoch 480/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6165 - mse: 30.6165\nEpoch 481/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6133 - mse: 30.6133\nEpoch 482/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6122 - mse: 30.6122\nEpoch 483/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6073 - mse: 30.6073\nEpoch 484/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6068 - mse: 30.6068\nEpoch 485/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6050 - mse: 30.6050\nEpoch 486/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6063 - mse: 30.6063\nEpoch 487/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5990 - mse: 30.5990\nEpoch 488/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6036 - mse: 30.6036\nEpoch 489/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5966 - mse: 30.5966\nEpoch 490/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5970 - mse: 30.5970\nEpoch 491/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6037 - mse: 30.6037\nEpoch 492/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5926 - mse: 30.5926\nEpoch 493/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5909 - mse: 30.5909\nEpoch 494/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5854 - mse: 30.5854\nEpoch 495/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5858 - mse: 30.5858\nEpoch 496/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5821 - mse: 30.5821\nEpoch 497/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5805 - mse: 30.5805\nEpoch 498/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5778 - mse: 30.5778\nEpoch 499/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.6050 - mse: 30.6050\nEpoch 500/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5861 - mse: 30.5861\nEpoch 501/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5739 - mse: 30.5739\nEpoch 502/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5747 - mse: 30.5747\nEpoch 503/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5768 - mse: 30.5768\nEpoch 504/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5711 - mse: 30.5711\nEpoch 505/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5686 - mse: 30.5686\nEpoch 506/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5682 - mse: 30.5682\nEpoch 507/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5766 - mse: 30.5766\nEpoch 508/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5712 - mse: 30.5712\nEpoch 509/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5702 - mse: 30.5702\nEpoch 510/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5686 - mse: 30.5686\nEpoch 511/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5636 - mse: 30.5636\nEpoch 512/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5804 - mse: 30.5804\nEpoch 513/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5596 - mse: 30.5596\nEpoch 514/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5626 - mse: 30.5626\nEpoch 515/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5760 - mse: 30.5760\nEpoch 516/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5675 - mse: 30.5675\nEpoch 517/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5590 - mse: 30.5590\nEpoch 518/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5578 - mse: 30.5578\nEpoch 519/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5568 - mse: 30.5568\nEpoch 520/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5570 - mse: 30.5570\nEpoch 521/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5574 - mse: 30.5574\nEpoch 522/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5533 - mse: 30.5533\nEpoch 523/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5529 - mse: 30.5529\nEpoch 524/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5545 - mse: 30.5545\nEpoch 525/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5547 - mse: 30.5547\nEpoch 526/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5536 - mse: 30.5536\nEpoch 527/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5532 - mse: 30.5532\nEpoch 528/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5543 - mse: 30.5543\nEpoch 529/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5494 - mse: 30.5494\nEpoch 530/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5554 - mse: 30.5554\nEpoch 531/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5478 - mse: 30.5478\nEpoch 532/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5502 - mse: 30.5502\nEpoch 533/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5565 - mse: 30.5565\nEpoch 534/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5505 - mse: 30.5505\nEpoch 535/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5466 - mse: 30.5466\nEpoch 536/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5473 - mse: 30.5473\nEpoch 537/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5544 - mse: 30.5544\nEpoch 538/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5478 - mse: 30.5478\nEpoch 539/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5466 - mse: 30.5466\nEpoch 540/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5473 - mse: 30.5473\nEpoch 541/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5532 - mse: 30.5532\nEpoch 542/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5400 - mse: 30.5400\nEpoch 543/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5439 - mse: 30.5439\nEpoch 544/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5441 - mse: 30.5441\nEpoch 545/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5452 - mse: 30.5452\nEpoch 546/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5445 - mse: 30.5445\nEpoch 547/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5446 - mse: 30.5446\nEpoch 548/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5468 - mse: 30.5468\nEpoch 549/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5459 - mse: 30.5459\nEpoch 550/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5434 - mse: 30.5434\nEpoch 551/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\nEpoch 552/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5461 - mse: 30.5461\nEpoch 553/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5434 - mse: 30.5434\nEpoch 554/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5476 - mse: 30.5476\nEpoch 555/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5412 - mse: 30.5412\nEpoch 556/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5410 - mse: 30.5410\nEpoch 557/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5469 - mse: 30.5469\nEpoch 558/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5380 - mse: 30.5380\nEpoch 559/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5551 - mse: 30.5551\nEpoch 560/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5424 - mse: 30.5424\nEpoch 561/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5373 - mse: 30.5373\nEpoch 562/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\nEpoch 563/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5386 - mse: 30.5386\nEpoch 564/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5398 - mse: 30.5398\nEpoch 565/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5399 - mse: 30.5399\nEpoch 566/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5393 - mse: 30.5393\nEpoch 567/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5431 - mse: 30.5431\nEpoch 568/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5382 - mse: 30.5382\nEpoch 569/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5523 - mse: 30.5523\nEpoch 570/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5479 - mse: 30.5479\nEpoch 571/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5354 - mse: 30.5354\nEpoch 572/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5416 - mse: 30.5416\nEpoch 573/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5367 - mse: 30.5367\nEpoch 574/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5338 - mse: 30.5338\nEpoch 575/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5434 - mse: 30.5434\nEpoch 576/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5339 - mse: 30.5339\nEpoch 577/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5349 - mse: 30.5349\nEpoch 578/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5503 - mse: 30.5503\nEpoch 579/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5366 - mse: 30.5366\nEpoch 580/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5426 - mse: 30.5426\nEpoch 581/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5505 - mse: 30.5505\nEpoch 582/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5374 - mse: 30.5374\nEpoch 583/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5347 - mse: 30.5347\nEpoch 584/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5333 - mse: 30.5333\nEpoch 585/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5353 - mse: 30.5353\nEpoch 586/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5521 - mse: 30.5521\nEpoch 587/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\nEpoch 588/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5375 - mse: 30.5375\nEpoch 589/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5410 - mse: 30.5410\nEpoch 590/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5333 - mse: 30.5333\nEpoch 591/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5332 - mse: 30.5332\nEpoch 592/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5478 - mse: 30.5478\nEpoch 593/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\nEpoch 594/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5359 - mse: 30.5359\nEpoch 595/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\nEpoch 596/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5362 - mse: 30.5362\nEpoch 597/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5333 - mse: 30.5333\nEpoch 598/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\nEpoch 599/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5336 - mse: 30.5336\nEpoch 600/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5353 - mse: 30.5353\nEpoch 601/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\nEpoch 602/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5309 - mse: 30.5309\nEpoch 603/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5327 - mse: 30.5327\nEpoch 604/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5331 - mse: 30.5331\nEpoch 605/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5497 - mse: 30.5497\nEpoch 606/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5456 - mse: 30.5456\nEpoch 607/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5382 - mse: 30.5382\nEpoch 608/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5325 - mse: 30.5325\nEpoch 609/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5324 - mse: 30.5324\nEpoch 610/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\nEpoch 611/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5351 - mse: 30.5351\nEpoch 612/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5399 - mse: 30.5399\nEpoch 613/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5329 - mse: 30.5329\nEpoch 614/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5302 - mse: 30.5302\nEpoch 615/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5414 - mse: 30.5414\nEpoch 616/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5357 - mse: 30.5357\nEpoch 617/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5373 - mse: 30.5373\nEpoch 618/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5312 - mse: 30.5312\nEpoch 619/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5322 - mse: 30.5322\nEpoch 620/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5308 - mse: 30.5308\nEpoch 621/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\nEpoch 622/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5341 - mse: 30.5341\nEpoch 623/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5395 - mse: 30.5395\nEpoch 624/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5325 - mse: 30.5325\nEpoch 625/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\nEpoch 626/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5316 - mse: 30.5316\nEpoch 627/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5313 - mse: 30.5313\nEpoch 628/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 629/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5367 - mse: 30.5367\nEpoch 630/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5340 - mse: 30.5340\nEpoch 631/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5325 - mse: 30.5325\nEpoch 632/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5341 - mse: 30.5341\nEpoch 633/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5421 - mse: 30.5421\nEpoch 634/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\nEpoch 635/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5310 - mse: 30.5310\nEpoch 636/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5279 - mse: 30.5279\nEpoch 637/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\nEpoch 638/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\nEpoch 639/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\nEpoch 640/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\nEpoch 641/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5403 - mse: 30.5403\nEpoch 642/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 643/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5358 - mse: 30.5358\nEpoch 644/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5286 - mse: 30.5286\nEpoch 645/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5397 - mse: 30.5397\nEpoch 646/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5395 - mse: 30.5395\nEpoch 647/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5412 - mse: 30.5412\nEpoch 648/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 649/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5295 - mse: 30.5295\nEpoch 650/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\nEpoch 651/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5282 - mse: 30.5282\nEpoch 652/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5290 - mse: 30.5290\nEpoch 653/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5294 - mse: 30.5294\nEpoch 654/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5302 - mse: 30.5302\nEpoch 655/1000\n16/16 [==============================] - 0s 7ms/step - loss: 30.5284 - mse: 30.5284\nEpoch 656/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5351 - mse: 30.5351\nEpoch 657/1000\n16/16 [==============================] - 0s 4ms/step - loss: 30.5385 - mse: 30.5385\nEpoch 658/1000\n16/16 [==============================] - 0s 6ms/step - loss: 30.5491 - mse: 30.5491\nEpoch 659/1000\n16/16 [==============================] - 0s 4ms/step - loss: 30.5462 - mse: 30.5462\nEpoch 660/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5393 - mse: 30.5393\nEpoch 661/1000\n16/16 [==============================] - 0s 4ms/step - loss: 30.5289 - mse: 30.5289\nEpoch 662/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5341 - mse: 30.5341\nEpoch 663/1000\n16/16 [==============================] - 0s 9ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 664/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5389 - mse: 30.5389\nEpoch 665/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5353 - mse: 30.5353\nEpoch 666/1000\n16/16 [==============================] - 0s 6ms/step - loss: 30.5304 - mse: 30.5304\nEpoch 667/1000\n16/16 [==============================] - 0s 6ms/step - loss: 30.5278 - mse: 30.5278\nEpoch 668/1000\n16/16 [==============================] - 0s 4ms/step - loss: 30.5290 - mse: 30.5290\nEpoch 669/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5310 - mse: 30.5310\nEpoch 670/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5330 - mse: 30.5330\nEpoch 671/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5302 - mse: 30.5302\nEpoch 672/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5347 - mse: 30.5347\nEpoch 673/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5311 - mse: 30.5311\nEpoch 674/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\nEpoch 675/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5305 - mse: 30.5305\nEpoch 676/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5303 - mse: 30.5303\nEpoch 677/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\nEpoch 678/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5322 - mse: 30.5322\nEpoch 679/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5386 - mse: 30.5386\nEpoch 680/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5287 - mse: 30.5287\nEpoch 681/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5279 - mse: 30.5279\nEpoch 682/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5316 - mse: 30.5316\nEpoch 683/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\nEpoch 684/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5402 - mse: 30.5402\nEpoch 685/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5327 - mse: 30.5327\nEpoch 686/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5282 - mse: 30.5282\nEpoch 687/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5363 - mse: 30.5363\nEpoch 688/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5312 - mse: 30.5312\nEpoch 689/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5370 - mse: 30.5370\nEpoch 690/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5323 - mse: 30.5323\nEpoch 691/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 692/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5276 - mse: 30.5276\nEpoch 693/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\nEpoch 694/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5290 - mse: 30.5290\nEpoch 695/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5492 - mse: 30.5492\nEpoch 696/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\nEpoch 697/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 698/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5316 - mse: 30.5316\nEpoch 699/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\nEpoch 700/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5391 - mse: 30.5391\nEpoch 701/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\nEpoch 702/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5302 - mse: 30.5302\nEpoch 703/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5306 - mse: 30.5306\nEpoch 704/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5305 - mse: 30.5305\nEpoch 705/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5247 - mse: 30.5247\nEpoch 706/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\nEpoch 707/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5366 - mse: 30.5366\nEpoch 708/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5491 - mse: 30.5491\nEpoch 709/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5261 - mse: 30.5261\nEpoch 710/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5348 - mse: 30.5348\nEpoch 711/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\nEpoch 712/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5265 - mse: 30.5265\nEpoch 713/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5373 - mse: 30.5373\nEpoch 714/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5312 - mse: 30.5312\nEpoch 715/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5376 - mse: 30.5376\nEpoch 716/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5377 - mse: 30.5377\nEpoch 717/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5284 - mse: 30.5284\nEpoch 718/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\nEpoch 719/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5282 - mse: 30.5282\nEpoch 720/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\nEpoch 721/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5347 - mse: 30.5347\nEpoch 722/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5295 - mse: 30.5295\nEpoch 723/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5274 - mse: 30.5274\nEpoch 724/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 725/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5233 - mse: 30.5233\nEpoch 726/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5359 - mse: 30.5359\nEpoch 727/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 728/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5272 - mse: 30.5272\nEpoch 729/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5323 - mse: 30.5323\nEpoch 730/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5258 - mse: 30.5258\nEpoch 731/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\nEpoch 732/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5297 - mse: 30.5297\nEpoch 733/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\nEpoch 734/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5247 - mse: 30.5247\nEpoch 735/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\nEpoch 736/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5384 - mse: 30.5384\nEpoch 737/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5317 - mse: 30.5317\nEpoch 738/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\nEpoch 739/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5260 - mse: 30.5260\nEpoch 740/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\nEpoch 741/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\nEpoch 742/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5278 - mse: 30.5278\nEpoch 743/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5280 - mse: 30.5280\nEpoch 744/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\nEpoch 745/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 746/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5485 - mse: 30.5485\nEpoch 747/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5331 - mse: 30.5331\nEpoch 748/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5246 - mse: 30.5246\nEpoch 749/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\nEpoch 750/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5289 - mse: 30.5289\nEpoch 751/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5257 - mse: 30.5257\nEpoch 752/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\nEpoch 753/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5475 - mse: 30.5475\nEpoch 754/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5294 - mse: 30.5294\nEpoch 755/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5413 - mse: 30.5413\nEpoch 756/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 757/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\nEpoch 758/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 759/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5352 - mse: 30.5352\nEpoch 760/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 761/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5248 - mse: 30.5248\nEpoch 762/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5286 - mse: 30.5286\nEpoch 763/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5289 - mse: 30.5289\nEpoch 764/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\nEpoch 765/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5278 - mse: 30.5278\nEpoch 766/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5271 - mse: 30.5271\nEpoch 767/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5269 - mse: 30.5269\nEpoch 768/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5274 - mse: 30.5274\nEpoch 769/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5361 - mse: 30.5361\nEpoch 770/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\nEpoch 771/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5239 - mse: 30.5239\nEpoch 772/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\nEpoch 773/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\nEpoch 774/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 775/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5246 - mse: 30.5246\nEpoch 776/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\nEpoch 777/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5265 - mse: 30.5265\nEpoch 778/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\nEpoch 779/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5250 - mse: 30.5250\nEpoch 780/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5260 - mse: 30.5260\nEpoch 781/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5307 - mse: 30.5307\nEpoch 782/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\nEpoch 783/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 784/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 785/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 786/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\nEpoch 787/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5237 - mse: 30.5237\nEpoch 788/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5297 - mse: 30.5297\nEpoch 789/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5287 - mse: 30.5287\nEpoch 790/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\nEpoch 791/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5256 - mse: 30.5256\nEpoch 792/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\nEpoch 793/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5278 - mse: 30.5278\nEpoch 794/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5281 - mse: 30.5281\nEpoch 795/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 796/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\nEpoch 797/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5302 - mse: 30.5302\nEpoch 798/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\nEpoch 799/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 800/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5281 - mse: 30.5281\nEpoch 801/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5271 - mse: 30.5271\nEpoch 802/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\nEpoch 803/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5212 - mse: 30.5212\nEpoch 804/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5233 - mse: 30.5233\nEpoch 805/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\nEpoch 806/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\nEpoch 807/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\nEpoch 808/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\nEpoch 809/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5232 - mse: 30.5232\nEpoch 810/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5352 - mse: 30.5352\nEpoch 811/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5299 - mse: 30.5299\nEpoch 812/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\nEpoch 813/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5347 - mse: 30.5347\nEpoch 814/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5446 - mse: 30.5446\nEpoch 815/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5256 - mse: 30.5256\nEpoch 816/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5223 - mse: 30.5223\nEpoch 817/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5487 - mse: 30.5487\nEpoch 818/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5237 - mse: 30.5237\nEpoch 819/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5286 - mse: 30.5286\nEpoch 820/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5227 - mse: 30.5227\nEpoch 821/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\nEpoch 822/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5463 - mse: 30.5463\nEpoch 823/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\nEpoch 824/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5241 - mse: 30.5241\nEpoch 825/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 826/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5276 - mse: 30.5276\nEpoch 827/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5334 - mse: 30.5334\nEpoch 828/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\nEpoch 829/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\nEpoch 830/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5222 - mse: 30.5222\nEpoch 831/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5368 - mse: 30.5368\nEpoch 832/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\nEpoch 833/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5374 - mse: 30.5374\nEpoch 834/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5385 - mse: 30.5385\nEpoch 835/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5186 - mse: 30.5186\nEpoch 836/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5222 - mse: 30.5222\nEpoch 837/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5349 - mse: 30.5349\nEpoch 838/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5209 - mse: 30.5209\nEpoch 839/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5227 - mse: 30.5227\nEpoch 840/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5365 - mse: 30.5365\nEpoch 841/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5276 - mse: 30.5276\nEpoch 842/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\nEpoch 843/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\nEpoch 844/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5280 - mse: 30.5280\nEpoch 845/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5238 - mse: 30.5238\nEpoch 846/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\nEpoch 847/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5381 - mse: 30.5381\nEpoch 848/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5232 - mse: 30.5232\nEpoch 849/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5299 - mse: 30.5299\nEpoch 850/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\nEpoch 851/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5226 - mse: 30.5226\nEpoch 852/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5230 - mse: 30.5230\nEpoch 853/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5226 - mse: 30.5226\nEpoch 854/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\nEpoch 855/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5283 - mse: 30.5283\nEpoch 856/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\nEpoch 857/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5281 - mse: 30.5281\nEpoch 858/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5250 - mse: 30.5250\nEpoch 859/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5272 - mse: 30.5272\nEpoch 860/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5301 - mse: 30.5301\nEpoch 861/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5230 - mse: 30.5230\nEpoch 862/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\nEpoch 863/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5284 - mse: 30.5284\nEpoch 864/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5267 - mse: 30.5267\nEpoch 865/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 866/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5306 - mse: 30.5306\nEpoch 867/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5235 - mse: 30.5235\nEpoch 868/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5466 - mse: 30.5466\nEpoch 869/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5174 - mse: 30.5174\nEpoch 870/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5208 - mse: 30.5208\nEpoch 871/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5352 - mse: 30.5352\nEpoch 872/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 873/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5235 - mse: 30.5235\nEpoch 874/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5226 - mse: 30.5226\nEpoch 875/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5340 - mse: 30.5340\nEpoch 876/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5208 - mse: 30.5208\nEpoch 877/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5321 - mse: 30.5321\nEpoch 878/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5311 - mse: 30.5311\nEpoch 879/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5230 - mse: 30.5230\nEpoch 880/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\nEpoch 881/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\nEpoch 882/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5223 - mse: 30.5223\nEpoch 883/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 884/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\nEpoch 885/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5300 - mse: 30.5300\nEpoch 886/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\nEpoch 887/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5241 - mse: 30.5241\nEpoch 888/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\nEpoch 889/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5214 - mse: 30.5214\nEpoch 890/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5219 - mse: 30.5219\nEpoch 891/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5255 - mse: 30.5255\nEpoch 892/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5327 - mse: 30.5327\nEpoch 893/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5500 - mse: 30.5500\nEpoch 894/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\nEpoch 895/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5250 - mse: 30.5250\nEpoch 896/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5222 - mse: 30.5222\nEpoch 897/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5228 - mse: 30.5228\nEpoch 898/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 899/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 900/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\nEpoch 901/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5272 - mse: 30.5272\nEpoch 902/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5230 - mse: 30.5230\nEpoch 903/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\nEpoch 904/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5231 - mse: 30.5231\nEpoch 905/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\nEpoch 906/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5275 - mse: 30.5275\nEpoch 907/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5301 - mse: 30.5301\nEpoch 908/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5221 - mse: 30.5221\nEpoch 909/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5225 - mse: 30.5225\nEpoch 910/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5207 - mse: 30.5207\nEpoch 911/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\nEpoch 912/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5258 - mse: 30.5258\nEpoch 913/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 914/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5218 - mse: 30.5218\nEpoch 915/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\nEpoch 916/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5206 - mse: 30.5206\nEpoch 917/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5200 - mse: 30.5200\nEpoch 918/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\nEpoch 919/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5317 - mse: 30.5317\nEpoch 920/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5267 - mse: 30.5267\nEpoch 921/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5357 - mse: 30.5357\nEpoch 922/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 923/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\nEpoch 924/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\nEpoch 925/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5280 - mse: 30.5280\nEpoch 926/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\nEpoch 927/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5205 - mse: 30.5205\nEpoch 928/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\nEpoch 929/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5239 - mse: 30.5239\nEpoch 930/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5224 - mse: 30.5224\nEpoch 931/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5219 - mse: 30.5219\nEpoch 932/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5247 - mse: 30.5247\nEpoch 933/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5248 - mse: 30.5248\nEpoch 934/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\nEpoch 935/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5441 - mse: 30.5441\nEpoch 936/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5194 - mse: 30.5194\nEpoch 937/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5204 - mse: 30.5204\nEpoch 938/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5209 - mse: 30.5209\nEpoch 939/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5361 - mse: 30.5361\nEpoch 940/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5259 - mse: 30.5259\nEpoch 941/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5204 - mse: 30.5204\nEpoch 942/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\nEpoch 943/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5216 - mse: 30.5216\nEpoch 944/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 945/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\nEpoch 946/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\nEpoch 947/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\nEpoch 948/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5261 - mse: 30.5261\nEpoch 949/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5188 - mse: 30.5188\nEpoch 950/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5255 - mse: 30.5255\nEpoch 951/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5213 - mse: 30.5213\nEpoch 952/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5212 - mse: 30.5212\nEpoch 953/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5218 - mse: 30.5218\nEpoch 954/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 955/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\nEpoch 956/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5179 - mse: 30.5179\nEpoch 957/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5218 - mse: 30.5218\nEpoch 958/1000\n16/16 [==============================] - 0s 5ms/step - loss: 30.5221 - mse: 30.5221\nEpoch 959/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5235 - mse: 30.5235\nEpoch 960/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5200 - mse: 30.5200\nEpoch 961/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5257 - mse: 30.5257\nEpoch 962/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5263 - mse: 30.5263\nEpoch 963/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5214 - mse: 30.5214\nEpoch 964/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5244 - mse: 30.5244\nEpoch 965/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5239 - mse: 30.5239\nEpoch 966/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5275 - mse: 30.5275\nEpoch 967/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5229 - mse: 30.5229\nEpoch 968/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5245 - mse: 30.5245\nEpoch 969/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5273 - mse: 30.5273\nEpoch 970/1000\n16/16 [==============================] - 0s 3ms/step - loss: 30.5209 - mse: 30.5209\nEpoch 971/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\nEpoch 972/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5216 - mse: 30.5216\nEpoch 973/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5216 - mse: 30.5216\nEpoch 974/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5668 - mse: 30.5668\nEpoch 975/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5168 - mse: 30.5168\nEpoch 976/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5234 - mse: 30.5234\nEpoch 977/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5202 - mse: 30.5202\nEpoch 978/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5187 - mse: 30.5187\nEpoch 979/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\nEpoch 980/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5209 - mse: 30.5209\nEpoch 981/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\nEpoch 982/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5216 - mse: 30.5216\nEpoch 983/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5269 - mse: 30.5269\nEpoch 984/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5217 - mse: 30.5217\nEpoch 985/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5347 - mse: 30.5347\nEpoch 986/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5223 - mse: 30.5223\nEpoch 987/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\nEpoch 988/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5297 - mse: 30.5297\nEpoch 989/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5203 - mse: 30.5203\nEpoch 990/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5241 - mse: 30.5241\nEpoch 991/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5184 - mse: 30.5184\nEpoch 992/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\nEpoch 993/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5237 - mse: 30.5237\nEpoch 994/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5204 - mse: 30.5204\nEpoch 995/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5218 - mse: 30.5218\nEpoch 996/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5281 - mse: 30.5281\nEpoch 997/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\nEpoch 998/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5184 - mse: 30.5184\nEpoch 999/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5257 - mse: 30.5257\nEpoch 1000/1000\n16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f2efc8fc950>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Keras로 학습된 모델을 이용하여 주택 가격 예측 수행. ","metadata":{}},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE'] = predicted\nbostonDF.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T00:42:36.188128Z","iopub.execute_input":"2022-01-08T00:42:36.188488Z","iopub.status.idle":"2022-01-08T00:42:36.316487Z","shell.execute_reply.started":"2022-01-08T00:42:36.188435Z","shell.execute_reply":"2022-01-08T00:42:36.315794Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n\n   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  KERAS_PREDICTED_PRICE  \n0     15.3  396.90   4.98   24.0        26.322000              28.962353  \n1     17.8  396.90   9.14   21.6        24.281207              25.488424  \n2     17.8  392.83   4.03   34.7        28.830886              32.618553  \n3     18.7  394.63   2.94   33.4        28.557652              32.396496  \n4     18.7  396.90   5.33   36.2        28.229120              31.583378  \n5     18.7  394.12   5.21   28.7        25.724125              28.090538  \n6     15.2  395.60  12.43   22.9        21.646539              21.310619  \n7     15.2  396.90  19.15   27.1        19.802559              17.734711  \n8     15.2  386.63  29.93   16.5        14.009758               8.025681  \n9     15.2  386.71  17.10   18.9        19.941503              18.231962  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n      <th>PREDICTED_PRICE</th>\n      <th>KERAS_PREDICTED_PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n      <td>26.322000</td>\n      <td>28.962353</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n      <td>24.281207</td>\n      <td>25.488424</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n      <td>28.830886</td>\n      <td>32.618553</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n      <td>28.557652</td>\n      <td>32.396496</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n      <td>28.229120</td>\n      <td>31.583378</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.02985</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.430</td>\n      <td>58.7</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.12</td>\n      <td>5.21</td>\n      <td>28.7</td>\n      <td>25.724125</td>\n      <td>28.090538</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.08829</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.012</td>\n      <td>66.6</td>\n      <td>5.5605</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>395.60</td>\n      <td>12.43</td>\n      <td>22.9</td>\n      <td>21.646539</td>\n      <td>21.310619</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.14455</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.172</td>\n      <td>96.1</td>\n      <td>5.9505</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>396.90</td>\n      <td>19.15</td>\n      <td>27.1</td>\n      <td>19.802559</td>\n      <td>17.734711</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.21124</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>5.631</td>\n      <td>100.0</td>\n      <td>6.0821</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.63</td>\n      <td>29.93</td>\n      <td>16.5</td>\n      <td>14.009758</td>\n      <td>8.025681</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.17004</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.004</td>\n      <td>85.9</td>\n      <td>6.5921</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.71</td>\n      <td>17.10</td>\n      <td>18.9</td>\n      <td>19.941503</td>\n      <td>18.231962</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T02:01:48.288801Z","iopub.execute_input":"2022-01-08T02:01:48.289324Z","iopub.status.idle":"2022-01-08T02:01:49.241348Z","shell.execute_reply.started":"2022-01-08T02:01:48.289225Z","shell.execute_reply":"2022-01-08T02:01:49.240672Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(506, 14)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  PRICE  \n0     15.3  396.90   4.98   24.0  \n1     17.8  396.90   9.14   21.6  \n2     17.8  392.83   4.03   34.7  \n3     18.7  394.63   2.94   33.4  \n4     18.7  396.90   5.33   36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### SGD 기반으로 Weight/Bias update 값 구하기","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_sgd.shape[0] # 1건만 가져옴. N=1\n    # 예측 값. \n    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n    # 실제값과 예측값의 차이 \n    diff_sgd = target_sgd - predicted_sgd\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"execution":{"iopub.status.busy":"2022-01-08T02:01:52.333135Z","iopub.execute_input":"2022-01-08T02:01:52.333704Z","iopub.status.idle":"2022-01-08T02:01:52.340457Z","shell.execute_reply.started":"2022-01-08T02:01:52.333666Z","shell.execute_reply":"2022-01-08T02:01:52.339789Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### SGD 수행하기","metadata":{}},{"cell_type":"code","source":"print(bostonDF['PRICE'].values.shape)\nprint(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\nprint(np.random.choice(506, 1))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T02:02:05.019194Z","iopub.execute_input":"2022-01-08T02:02:05.019737Z","iopub.status.idle":"2022-01-08T02:02:05.028256Z","shell.execute_reply.started":"2022-01-08T02:02:05.019686Z","shell.execute_reply":"2022-01-08T02:02:05.027420Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(506,)\n[447]\n[15]\n","output_type":"stream"}]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # ★★  iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택. \n        # ★★  이 부분이 차이점!!\n        stochastic_index = np.random.choice(target.shape[0], 1) # target.shape는 (506, ), [0]로 506만 뽑음. choice로 0~505 中 인덱스 하나 가져옴.\n        rm_sgd = rm[stochastic_index]\n        lstat_sgd = lstat[stochastic_index]\n        target_sgd = target[stochastic_index]\n        # SGD 기반으로 Weight/Bias의 Update를 구함.  \n        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n        \n        # SGD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # ★★  Loss는 전체 학습 데이터 기반으로 구해야 함. 그래서 다시 계산. \n            predicted = w1 * rm + w2*lstat + bias # 506크기\n            diff = target - predicted # 506크기\n            mse_loss = np.mean(np.square(diff)) # 506크기\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n\nw1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_SGD'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_batch.shape[0]\n    # 예측 값. \n    predicted_batch = w1 * rm_batch+ w2 * lstat_batch + bias\n    # 실제값과 예측값의 차이 \n    diff_batch = target_batch - predicted_batch\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_batch.T, diff_batch))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_batch.T, diff_batch))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_batch))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_indexes = np.random.choice(506, 30)\nprint(batch_indexes)\n\nbostonDF['RM'].values[batch_indexes]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_random_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 갯수만큼 데이터를 임의로 선택. \n        batch_indexes = np.random.choice(target.shape[0], batch_size)\n        rm_batch = rm[batch_indexes]\n        lstat_batch = lstat[batch_indexes]\n        target_batch = target[batch_indexes]\n        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n        \n        # Batch GD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_random_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH_RANDOM'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"for batch_step in range(0, 506, 30):\n    print(batch_step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bostonDF['PRICE'].values[480:510]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n        for batch_step in range(0, target.shape[0], batch_size):\n            # batch_size만큼 순차적인 데이터를 가져옴. \n            rm_batch = rm[batch_step:batch_step + batch_size]\n            lstat_batch = lstat[batch_step:batch_step + batch_size]\n            target_batch = target[batch_step:batch_step + batch_size]\n        \n            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n\n            # Batch GD로 구한 weight/bias의 update 적용. \n            w1 = w1 - w1_update\n            w2 = w2 - w2_update\n            bias = bias - bias_update\n        \n            if verbose:\n                print('Epoch:', i+1,'/', iter_epochs, 'batch step:', batch_step)\n                # Loss는 전체 학습 데이터 기반으로 구해야 함.\n                predicted = w1 * rm + w2*lstat + bias\n                diff = target - predicted\n                mse_loss = np.mean(np.square(diff))\n                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mini BATCH GD를 Keras로 수행\n* Keras는 기본적으로 Mini Batch GD를 수행","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n\n# Keras는 반드시 Batch GD를 적용함. batch_size가 None이면 32를 할당. \nmodel.fit(scaled_features, bostonDF['PRICE'].values, batch_size=30, epochs=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}