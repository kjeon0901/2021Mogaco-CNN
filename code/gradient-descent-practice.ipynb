{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T02:59:41.931847Z","iopub.execute_input":"2021-12-27T02:59:41.932182Z","iopub.status.idle":"2021-12-27T02:59:41.940988Z","shell.execute_reply.started":"2021-12-27T02:59:41.932150Z","shell.execute_reply":"2021-12-27T02:59:41.938197Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n * 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nprint(boston)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T02:59:52.925071Z","iopub.execute_input":"2021-12-27T02:59:52.925726Z","iopub.status.idle":"2021-12-27T02:59:52.950030Z","shell.execute_reply.started":"2021-12-27T02:59:52.925665Z","shell.execute_reply":"2021-12-27T02:59:52.948952Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n        4.9800e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n        9.1400e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n        4.0300e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        5.6400e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n        6.4800e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\", 'filename': '/opt/conda/lib/python3.7/site-packages/sklearn/datasets/data/boston_house_prices.csv'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target #target값\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T02:59:55.739390Z","iopub.execute_input":"2021-12-27T02:59:55.740391Z","iopub.status.idle":"2021-12-27T02:59:55.795308Z","shell.execute_reply.started":"2021-12-27T02:59:55.740344Z","shell.execute_reply":"2021-12-27T02:59:55.794293Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(506, 14)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  PRICE  \n0     15.3  396.90   4.98   24.0  \n1     17.8  396.90   9.14   21.6  \n2     17.8  392.83   4.03   34.7  \n3     18.7  394.63   2.94   33.4  \n4     18.7  396.90   5.33   36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n* w1은 RM(방의 계수) 피처의 Weight 값\n* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n* bias는 Bias\n* N은 입력 데이터 건수\n![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n","metadata":{}},{"cell_type":"code","source":"# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨. \n# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n'''\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n    # 데이터 건수\n    N = len(target)\n    # 예측 값. \n    predicted = w1 * rm + w2*lstat + bias\n    # 실제값과 예측값의 차이 \n    diff = target - predicted\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산. (위 내용 참고, 미분 취하는 계산임)\n    w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))\n    \n    # Mean Squared Error값을 계산. \n    mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환. \n    return bias_update, w1_update, w2_update, mse_loss\n'''\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate):\n    # rm, lstat의 영향이 가장 커서 이 두 개로만 연습할 것임\n    # w1, w2가 들어오는 이유는 예측값 계산하기 위함\n    \n    N = len(target) # def의 인자로 들어오는 건 전부 array(배열) => sum 속도빠름. loop돌리지 않고 numpy 써서 더 좋음.\n    # 주택가격 전체 건수\n    \n    predicted = w1*rm + w2*lstat + bias\n    # 예측값i\n    \n    diff = target-predicted # array 인수 506개를 한번에 계산\n    # 차이i = 실제값i-예측값i\n    \n    w1_update = -learning_rate*(2/N)*np.dot(rm.T, diff) # np.dot()위해 앞에있는 rm 전치행렬로 바꿈\n    w2_update = -learning_rate*(2/N)*np.dot(lstat.T, diff)\n    # η(dLoss(W)/dw1)\n    # η = learning rate = step\n    \n    bias_update1 = -learning_rate*(2/N)*np.sum(diff)\n    bias_factors = np.ones((N,))\n    bias_update = -learning_rate*(2/N)*np.dot(bias_factors.T, diff)\n    bias_update1\n    bias_update\n    \n    mse_loss = np.mean(np.square(diff)) # 차이의 제곱의 평균\n    \n    return bias_update, w1_update, w2_update, mse_loss # 업데이트할 bias와 weight, mse 리턴","metadata":{"execution":{"iopub.status.busy":"2021-12-27T02:59:59.271953Z","iopub.execute_input":"2021-12-27T02:59:59.272224Z","iopub.status.idle":"2021-12-27T02:59:59.286491Z","shell.execute_reply.started":"2021-12-27T02:59:59.272194Z","shell.execute_reply":"2021-12-27T02:59:59.285466Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 를 적용하는 함수 생성\n* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. ","metadata":{}},{"cell_type":"code","source":"np.zeros((3,)), np.ones((2,))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T02:06:02.129758Z","iopub.execute_input":"2021-12-27T02:06:02.130854Z","iopub.status.idle":"2021-12-27T02:06:02.141942Z","shell.execute_reply.started":"2021-12-27T02:06:02.130804Z","shell.execute_reply":"2021-12-27T02:06:02.140782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.ones((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0] # 현재 features 안에는 bostonDF에서 rm, lstat 두 피처만 뽑혀있었음\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행 (get_update_weights_value 함수) \n    for i in range(iter_epochs):\n        # weight/bias update 값 계산 \n        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n        # weight/bias의 update 적용 (이전 셀 참고)\n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', loss) #loss값이 계속 줄어드나 확인용\n        \n    return w1, w2, bias","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:00:04.556353Z","iopub.execute_input":"2021-12-27T03:00:04.556638Z","iopub.status.idle":"2021-12-27T03:00:04.567307Z","shell.execute_reply.started":"2021-12-27T03:00:04.556602Z","shell.execute_reply":"2021-12-27T03:00:04.566280Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 적용\n* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() # default값 안 주면 0~1 사이 값으로 바꿔줌\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']]) #bostonDF에서 rm, lstat 2개의 피처만 가져올것임\nprint(scaled_features[:15])\nprint(bostonDF['PRICE'])\nprint(bostonDF['PRICE'].values)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T02:20:13.506243Z","iopub.execute_input":"2021-12-27T02:20:13.50656Z","iopub.status.idle":"2021-12-27T02:20:13.531757Z","shell.execute_reply.started":"2021-12-27T02:20:13.506529Z","shell.execute_reply":"2021-12-27T02:20:13.53096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() # default값 안 주면 0~1 사이 값으로 바꿔줌\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']]) #bostonDF에서 rm, lstat 2개의 피처만 가져올것임\n\nw1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\n# bostonDF['PRICE'].values를 해줘서 value값만 들어있는 array배열이 됨\n# iter_epochs=1000 => 1000번돌리는것. \nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)\n'''\n최초 w1, w2, bias: [0.] [0.] [1.]\nEpoch: 1 / 1000\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\nEpoch: 2 / 1000\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\nEpoch: 3 / 1000\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\n~~~\nEpoch: 1000 / 1000\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\n##### 최종 w1, w2, bias #######\n[18.54730416] [-13.01074165] [16.77763613]\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:00:13.224507Z","iopub.execute_input":"2021-12-27T03:00:13.225250Z","iopub.status.idle":"2021-12-27T03:00:16.928619Z","shell.execute_reply.started":"2021-12-27T03:00:13.225217Z","shell.execute_reply":"2021-12-27T03:00:16.927606Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"최초 w1, w2, bias: [0.] [0.] [1.]\nEpoch: 1 / 1000\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\nEpoch: 2 / 1000\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\nEpoch: 3 / 1000\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\nEpoch: 4 / 1000\nw1: [0.93124998] w2: [0.39159032] bias: [2.65334123] loss: 476.7031232605375\nEpoch: 5 / 1000\nw1: [1.14941104] w2: [0.48110116] bias: [3.03885015] loss: 455.41666565492966\nEpoch: 6 / 1000\nw1: [1.36202867] w2: [0.56744065] bias: [3.41383227] loss: 435.2718794853261\nEpoch: 7 / 1000\nw1: [1.56925388] w2: [0.65069613] bias: [3.7785751] loss: 416.20726135905875\nEpoch: 8 / 1000\nw1: [1.77123356] w2: [0.73095251] bias: [4.13335831] loss: 398.1646216743121\nEpoch: 9 / 1000\nw1: [1.96811059] w2: [0.8082924] bias: [4.47845392] loss: 381.0889060727257\nEpoch: 10 / 1000\nw1: [2.16002396] w2: [0.88279616] bias: [4.81412652] loss: 364.9280265121499\nEpoch: 11 / 1000\nw1: [2.34710885] w2: [0.95454195] bias: [5.14063347] loss: 349.6327014412229\nEpoch: 12 / 1000\nw1: [2.52949675] w2: [1.02360578] bias: [5.45822511] loss: 335.1563045853578\nEpoch: 13 / 1000\nw1: [2.70731556] w2: [1.0900616] bias: [5.76714493] loss: 321.45472188016\nEpoch: 14 / 1000\nw1: [2.8806897] w2: [1.15398133] bias: [6.06762979] loss: 308.4862161132872\nEpoch: 15 / 1000\nw1: [3.04974017] w2: [1.21543494] bias: [6.35991006] loss: 296.21129885942213\nEpoch: 16 / 1000\nw1: [3.21458467] w2: [1.27449047] bias: [6.64420982] loss: 284.59260931540365\nEpoch: 17 / 1000\nw1: [3.37533771] w2: [1.33121411] bias: [6.92074705] loss: 273.594799663735\nEpoch: 18 / 1000\nw1: [3.53211064] w2: [1.38567023] bias: [7.18973376] loss: 263.1844266127188\nEpoch: 19 / 1000\nw1: [3.6850118] w2: [1.43792147] bias: [7.45137615] loss: 253.32984878042316\nEpoch: 20 / 1000\nw1: [3.83414653] w2: [1.48802872] bias: [7.70587484] loss: 244.00112960761027\nEpoch: 21 / 1000\nw1: [3.97961734] w2: [1.53605124] bias: [7.95342492] loss: 235.16994550172836\nEpoch: 22 / 1000\nw1: [4.1215239] w2: [1.58204665] bias: [8.19421617] loss: 226.80949893011575\nEpoch: 23 / 1000\nw1: [4.2599632] w2: [1.626071] bias: [8.4284332] loss: 218.89443619575283\nEpoch: 24 / 1000\nw1: [4.39502954] w2: [1.66817882] bias: [8.65625556] loss: 211.40076964326613\nEpoch: 25 / 1000\nw1: [4.52681466] w2: [1.70842313] bias: [8.87785789] loss: 204.30580405648178\nEpoch: 26 / 1000\nw1: [4.65540781] w2: [1.74685552] bias: [9.09341009] loss: 197.58806702168727\nEpoch: 27 / 1000\nw1: [4.78089579] w2: [1.78352615] bias: [9.30307739] loss: 191.22724304292856\nEpoch: 28 / 1000\nw1: [4.90336302] w2: [1.81848384] bias: [9.50702053] loss: 185.20411120718256\nEpoch: 29 / 1000\nw1: [5.02289163] w2: [1.85177604] bias: [9.70539583] loss: 179.50048620813746\nEpoch: 30 / 1000\nw1: [5.13956149] w2: [1.88344893] bias: [9.89835537] loss: 174.0991625476179\nEpoch: 31 / 1000\nw1: [5.25345032] w2: [1.91354741] bias: [10.08604706] loss: 168.9838617434446\nEpoch: 32 / 1000\nw1: [5.36463368] w2: [1.94211517] bias: [10.26861478] loss: 164.13918238174\nEpoch: 33 / 1000\nw1: [5.4731851] w2: [1.96919469] bias: [10.44619847] loss: 159.5505528604214\nEpoch: 34 / 1000\nw1: [5.57917608] w2: [1.99482731] bias: [10.61893425] loss: 155.20418667888006\nEpoch: 35 / 1000\nw1: [5.68267616] w2: [2.01905321] bias: [10.78695453] loss: 151.08704013665792\nEpoch: 36 / 1000\nw1: [5.78375302] w2: [2.0419115] bias: [10.95038809] loss: 147.18677231132403\nEpoch: 37 / 1000\nw1: [5.88247245] w2: [2.06344021] bias: [11.10936022] loss: 143.49170719274753\nEpoch: 38 / 1000\nw1: [5.97889846] w2: [2.08367632] bias: [11.26399275] loss: 139.9907978575804\nEpoch: 39 / 1000\nw1: [6.07309332] w2: [2.10265582] bias: [11.4144042] loss: 136.67359257402353\nEpoch: 40 / 1000\nw1: [6.16511758] w2: [2.12041371] bias: [11.56070987] loss: 133.53020273287106\nEpoch: 41 / 1000\nw1: [6.25503017] w2: [2.13698402] bias: [11.70302189] loss: 130.55127250643457\nEpoch: 42 / 1000\nw1: [6.34288837] w2: [2.15239986] bias: [11.84144932] loss: 127.727950142247\nEpoch: 43 / 1000\nw1: [6.42874791] w2: [2.16669344] bias: [11.97609827] loss: 125.05186080346596\nEpoch: 44 / 1000\nw1: [6.51266302] w2: [2.17989608] bias: [12.10707193] loss: 122.51508087263839\nEpoch: 45 / 1000\nw1: [6.59468643] w2: [2.19203825] bias: [12.23447067] loss: 120.11011363998176\nEpoch: 46 / 1000\nw1: [6.67486943] w2: [2.20314959] bias: [12.35839213] loss: 117.82986630158324\nEpoch: 47 / 1000\nw1: [6.7532619] w2: [2.21325891] bias: [12.47893128] loss: 115.66762819693858\nEpoch: 48 / 1000\nw1: [6.82991239] w2: [2.22239427] bias: [12.59618049] loss: 113.61705021905472\nEpoch: 49 / 1000\nw1: [6.90486809] w2: [2.23058292] bias: [12.71022962] loss: 111.67212533393861\nEpoch: 50 / 1000\nw1: [6.97817492] w2: [2.2378514] bias: [12.82116606] loss: 109.82717014969786\nEpoch: 51 / 1000\nw1: [7.04987754] w2: [2.2442255] bias: [12.92907483] loss: 108.07680747870059\nEpoch: 52 / 1000\nw1: [7.1200194] w2: [2.24973032] bias: [13.03403861] loss: 106.41594983928782\nEpoch: 53 / 1000\nw1: [7.18864275] w2: [2.25439027] bias: [13.13613783] loss: 104.83978384641571\nEpoch: 54 / 1000\nw1: [7.25578868] w2: [2.25822908] bias: [13.23545073] loss: 103.34375544333149\nEpoch: 55 / 1000\nw1: [7.3214972] w2: [2.26126986] bias: [13.33205341] loss: 101.9235559289684\nEpoch: 56 / 1000\nw1: [7.38580717] w2: [2.26353506] bias: [13.42601987] loss: 100.57510873818626\nEpoch: 57 / 1000\nw1: [7.44875643] w2: [2.26504653] bias: [13.51742212] loss: 99.29455693429396\nEpoch: 58 / 1000\nw1: [7.51038178] w2: [2.26582552] bias: [13.6063302] loss: 98.07825137547643\nEpoch: 59 / 1000\nw1: [7.57071899] w2: [2.2658927] bias: [13.6928122] loss: 96.9227395188156\nEpoch: 60 / 1000\nw1: [7.62980289] w2: [2.26526817] bias: [13.7769344] loss: 95.82475482755216\nEpoch: 61 / 1000\nw1: [7.68766733] w2: [2.26397149] bias: [13.85876124] loss: 94.78120674908494\nEpoch: 62 / 1000\nw1: [7.74434526] w2: [2.26202167] bias: [13.9383554] loss: 93.7891712329572\nEpoch: 63 / 1000\nw1: [7.7998687] w2: [2.25943721] bias: [14.01577787] loss: 92.8458817597344\nEpoch: 64 / 1000\nw1: [7.85426883] w2: [2.25623611] bias: [14.09108795] loss: 91.94872085324727\nEpoch: 65 / 1000\nw1: [7.90757595] w2: [2.25243586] bias: [14.16434333] loss: 91.09521205015614\nEpoch: 66 / 1000\nw1: [7.95981955] w2: [2.2480535] bias: [14.23560012] loss: 90.28301230219593\nEpoch: 67 / 1000\nw1: [8.01102831] w2: [2.24310557] bias: [14.30491291] loss: 89.50990478778893\nEpoch: 68 / 1000\nw1: [8.06123013] w2: [2.23760818] bias: [14.37233478] loss: 88.77379211096893\nEpoch: 69 / 1000\nw1: [8.11045214] w2: [2.23157699] bias: [14.43791738] loss: 88.07268986674764\nEpoch: 70 / 1000\nw1: [8.15872074] w2: [2.22502725] bias: [14.50171094] loss: 87.40472055318025\nEpoch: 71 / 1000\nw1: [8.20606159] w2: [2.21797376] bias: [14.56376431] loss: 86.76810781144925\nEpoch: 72 / 1000\nw1: [8.25249968] w2: [2.21043094] bias: [14.62412502] loss: 86.16117097629328\nEpoch: 73 / 1000\nw1: [8.2980593] w2: [2.2024128] bias: [14.68283929] loss: 85.58231992005935\nEpoch: 74 / 1000\nw1: [8.34276406] w2: [2.193933] bias: [14.73995209] loss: 85.03005017455823\nEpoch: 75 / 1000\nw1: [8.38663696] w2: [2.18500477] bias: [14.79550715] loss: 84.50293831575453\nEpoch: 76 / 1000\nw1: [8.42970036] w2: [2.17564104] bias: [14.84954701] loss: 83.99963759713036\nEpoch: 77 / 1000\nw1: [8.47197598] w2: [2.16585433] bias: [14.90211305] loss: 83.51887381832368\nEpoch: 78 / 1000\nw1: [8.513485] w2: [2.15565685] bias: [14.95324552] loss: 83.05944141636493\nEpoch: 79 / 1000\nw1: [8.55424798] w2: [2.14506048] bias: [15.00298356] loss: 82.6201997675181\nEpoch: 80 / 1000\nw1: [8.59428494] w2: [2.13407676] bias: [15.05136527] loss: 82.20006968837905\nEpoch: 81 / 1000\nw1: [8.63361534] w2: [2.12271691] bias: [15.09842767] loss: 81.79803012549498\nEpoch: 82 / 1000\nw1: [8.67225811] w2: [2.11099186] bias: [15.14420679] loss: 81.4131150233476\nEpoch: 83 / 1000\nw1: [8.71023168] w2: [2.09891223] bias: [15.18873769] loss: 81.04441036108945\nEpoch: 84 / 1000\nw1: [8.74755397] w2: [2.08648835] bias: [15.23205444] loss: 80.69105134894139\nEpoch: 85 / 1000\nw1: [8.78424239] w2: [2.07373027] bias: [15.2741902] loss: 80.3522197756484\nEpoch: 86 / 1000\nw1: [8.8203139] w2: [2.06064776] bias: [15.31517722] loss: 80.02714149885448\nEpoch: 87 / 1000\nw1: [8.85578499] w2: [2.04725033] bias: [15.35504688] loss: 79.71508407069689\nEpoch: 88 / 1000\nw1: [8.8906717] w2: [2.03354722] bias: [15.39382968] loss: 79.41535449133292\nEpoch: 89 / 1000\nw1: [8.92498962] w2: [2.01954743] bias: [15.4315553] loss: 79.12729708350739\nEpoch: 90 / 1000\nw1: [8.95875394] w2: [2.0052597] bias: [15.46825261] loss: 78.85029148163837\nEpoch: 91 / 1000\nw1: [8.99197941] w2: [1.99069254] bias: [15.5039497] loss: 78.58375072925126\nEpoch: 92 / 1000\nw1: [9.02468039] w2: [1.97585422] bias: [15.53867387] loss: 78.32711947892314\nEpoch: 93 / 1000\nw1: [9.05687085] w2: [1.9607528] bias: [15.57245169] loss: 78.07987228921439\nEpoch: 94 / 1000\nw1: [9.08856437] w2: [1.94539609] bias: [15.605309] loss: 77.84151201336168\nEpoch: 95 / 1000\nw1: [9.11977417] w2: [1.92979171] bias: [15.63727095] loss: 77.6115682747885\nEpoch: 96 / 1000\nw1: [9.1505131] w2: [1.91394707] bias: [15.66836197] loss: 77.38959602475494\nEpoch: 97 / 1000\nw1: [9.18079366] w2: [1.89786936] bias: [15.69860585] loss: 77.17517417772183\nEpoch: 98 / 1000\nw1: [9.21062801] w2: [1.8815656] bias: [15.72802573] loss: 76.96790432024123\nEpoch: 99 / 1000\nw1: [9.240028] w2: [1.86504258] bias: [15.7566441] loss: 76.76740948941219\nEpoch: 100 / 1000\nw1: [9.26900511] w2: [1.84830694] bias: [15.78448284] loss: 76.57333301715327\nEpoch: 101 / 1000\nw1: [9.29757055] w2: [1.83136512] bias: [15.81156325] loss: 76.38533743674583\nEpoch: 102 / 1000\nw1: [9.32573521] w2: [1.81422338] bias: [15.83790603] loss: 76.20310344829278\nEpoch: 103 / 1000\nw1: [9.35350967] w2: [1.79688783] bias: [15.86353133] loss: 76.02632893991849\nEpoch: 104 / 1000\nw1: [9.38090424] w2: [1.77936439] bias: [15.88845873] loss: 75.85472806170637\nEpoch: 105 / 1000\nw1: [9.40792893] w2: [1.76165882] bias: [15.91270728] loss: 75.68803034953318\nEpoch: 106 / 1000\nw1: [9.4345935] w2: [1.74377674] bias: [15.93629554] loss: 75.52597989611057\nEpoch: 107 / 1000\nw1: [9.46090743] w2: [1.7257236] bias: [15.95924151] loss: 75.36833456669144\nEpoch: 108 / 1000\nw1: [9.48687994] w2: [1.70750469] bias: [15.98156274] loss: 75.2148652570338\nEpoch: 109 / 1000\nw1: [9.51252] w2: [1.68912519] bias: [16.0032763] loss: 75.06535519134566\nEpoch: 110 / 1000\nw1: [9.53783634] w2: [1.67059011] bias: [16.02439876] loss: 74.91959925805673\nEpoch: 111 / 1000\nw1: [9.56283744] w2: [1.65190433] bias: [16.04494626] loss: 74.77740338137866\nEpoch: 112 / 1000\nw1: [9.58753157] w2: [1.63307258] bias: [16.06493452] loss: 74.63858392672564\nEpoch: 113 / 1000\nw1: [9.61192676] w2: [1.6140995] bias: [16.08437878] loss: 74.50296713817104\nEpoch: 114 / 1000\nw1: [9.63603082] w2: [1.59498957] bias: [16.10329391] loss: 74.37038860621374\nEpoch: 115 / 1000\nw1: [9.65985134] w2: [1.57574714] bias: [16.12169436] loss: 74.24069276422141\nEpoch: 116 / 1000\nw1: [9.68339573] w2: [1.55637648] bias: [16.13959417] loss: 74.11373241200526\nEpoch: 117 / 1000\nw1: [9.70667116] w2: [1.53688171] bias: [16.15700701] loss: 73.98936826506468\nEpoch: 118 / 1000\nw1: [9.72968465] w2: [1.51726684] bias: [16.17394618] loss: 73.86746852811852\nEpoch: 119 / 1000\nw1: [9.75244299] w2: [1.49753578] bias: [16.1904246] loss: 73.7479084916145\nEpoch: 120 / 1000\nw1: [9.7749528] w2: [1.47769233] bias: [16.20645486] loss: 73.63057014997872\nEpoch: 121 / 1000\nw1: [9.79722052] w2: [1.45774018] bias: [16.2220492] loss: 73.51534184043389\nEpoch: 122 / 1000\nw1: [9.81925241] w2: [1.43768293] bias: [16.2372195] loss: 73.40211790127799\nEpoch: 123 / 1000\nw1: [9.84105457] w2: [1.41752407] bias: [16.25197736] loss: 73.29079834857474\nEpoch: 124 / 1000\nw1: [9.86263292] w2: [1.39726699] bias: [16.26633401] loss: 73.18128857026421\nEpoch: 125 / 1000\nw1: [9.88399322] w2: [1.37691501] bias: [16.28030043] loss: 73.0734990367546\nEpoch: 126 / 1000\nw1: [9.90514108] w2: [1.35647133] bias: [16.29388726] loss: 72.9673450271072\nEpoch: 127 / 1000\nw1: [9.92608196] w2: [1.33593907] bias: [16.30710486] loss: 72.86274636997473\nEpoch: 128 / 1000\nw1: [9.94682116] w2: [1.31532129] bias: [16.31996332] loss: 72.75962719849768\nEpoch: 129 / 1000\nw1: [9.96736385] w2: [1.29462093] bias: [16.33247243] loss: 72.65791571840701\nEpoch: 130 / 1000\nw1: [9.98771504] w2: [1.27384088] bias: [16.34464173] loss: 72.55754398862112\nEpoch: 131 / 1000\nw1: [10.00787962] w2: [1.25298391] bias: [16.3564805] loss: 72.45844771366467\nEpoch: 132 / 1000\nw1: [10.02786234] w2: [1.23205276] bias: [16.36799776] loss: 72.36056604727146\nEpoch: 133 / 1000\nw1: [10.04766782] w2: [1.21105006] bias: [16.37920228] loss: 72.26384140656936\nEpoch: 134 / 1000\nw1: [10.06730056] w2: [1.18997839] bias: [16.3901026] loss: 72.16821929627685\nEpoch: 135 / 1000\nw1: [10.08676492] w2: [1.16884025] bias: [16.40070703] loss: 72.07364814237184\nEpoch: 136 / 1000\nw1: [10.10606517] w2: [1.14763806] bias: [16.41102363] loss: 71.98007913472244\nEpoch: 137 / 1000\nw1: [10.12520544] w2: [1.12637419] bias: [16.42106027] loss: 71.88746607819654\nEpoch: 138 / 1000\nw1: [10.14418976] w2: [1.10505095] bias: [16.43082459] loss: 71.79576525179384\nEpoch: 139 / 1000\nw1: [10.16302204] w2: [1.08367056] bias: [16.44032401] loss: 71.70493527536757\nEpoch: 140 / 1000\nw1: [10.18170609] w2: [1.0622352] bias: [16.44956577] loss: 71.61493698352744\nEpoch: 141 / 1000\nw1: [10.20024562] w2: [1.04074698] bias: [16.45855689] loss: 71.52573330633642\nEpoch: 142 / 1000\nw1: [10.21864423] w2: [1.01920796] bias: [16.46730423] loss: 71.4372891564357\nEpoch: 143 / 1000\nw1: [10.23690542] w2: [0.99762014] bias: [16.47581443] loss: 71.34957132225128\nEpoch: 144 / 1000\nw1: [10.25503262] w2: [0.97598546] bias: [16.48409395] loss: 71.2625483669545\nEpoch: 145 / 1000\nw1: [10.27302913] w2: [0.95430581] bias: [16.49214911] loss: 71.17619053286658\nEpoch: 146 / 1000\nw1: [10.29089819] w2: [0.93258303] bias: [16.49998601] loss: 71.09046965101383\nEpoch: 147 / 1000\nw1: [10.30864293] w2: [0.91081891] bias: [16.50761062] loss: 71.00535905555587\nEpoch: 148 / 1000\nw1: [10.32626641] w2: [0.88901518] bias: [16.51502873] loss: 70.92083350282455\nEpoch: 149 / 1000\nw1: [10.34377161] w2: [0.86717353] bias: [16.52224597] loss: 70.83686909472493\nEpoch: 150 / 1000\nw1: [10.3611614] w2: [0.84529561] bias: [16.52926782] loss: 70.75344320626338\nEpoch: 151 / 1000\nw1: [10.37843861] w2: [0.82338301] bias: [16.53609961] loss: 70.67053441698039\nEpoch: 152 / 1000\nw1: [10.39560596] w2: [0.80143729] bias: [16.54274653] loss: 70.58812244607789\nEpoch: 153 / 1000\nw1: [10.41266613] w2: [0.77945995] bias: [16.54921363] loss: 70.50618809104161\nEpoch: 154 / 1000\nw1: [10.42962169] w2: [0.75745247] bias: [16.5555058] loss: 70.42471316957067\nEpoch: 155 / 1000\nw1: [10.44647518] w2: [0.73541627] bias: [16.56162782] loss: 70.34368046463578\nEpoch: 156 / 1000\nw1: [10.46322902] w2: [0.71335275] bias: [16.56758434] loss: 70.26307367249775\nEpoch: 157 / 1000\nw1: [10.47988562] w2: [0.69126324] bias: [16.57337986] loss: 70.18287735352673\nEpoch: 158 / 1000\nw1: [10.49644729] w2: [0.66914907] bias: [16.57901878] loss: 70.10307688567114\nEpoch: 159 / 1000\nw1: [10.51291628] w2: [0.6470115] bias: [16.58450536] loss: 70.02365842043383\nEpoch: 160 / 1000\nw1: [10.52929479] w2: [0.62485178] bias: [16.58984377] loss: 69.94460884122006\nEpoch: 161 / 1000\nw1: [10.54558496] w2: [0.60267112] bias: [16.59503805] loss: 69.86591572392965\nEpoch: 162 / 1000\nw1: [10.56178885] w2: [0.58047068] bias: [16.60009213] loss: 69.78756729967246\nEpoch: 163 / 1000\nw1: [10.5779085] w2: [0.55825162] bias: [16.60500982] loss: 69.70955241949265\nEpoch: 164 / 1000\nw1: [10.59394585] w2: [0.53601502] bias: [16.60979486] loss: 69.63186052099357\nEpoch: 165 / 1000\nw1: [10.60990284] w2: [0.51376198] bias: [16.61445085] loss: 69.55448159676082\nEpoch: 166 / 1000\nw1: [10.62578132] w2: [0.49149354] bias: [16.61898132] loss: 69.47740616448675\nEpoch: 167 / 1000\nw1: [10.64158309] w2: [0.46921071] bias: [16.62338969] loss: 69.40062523870435\nEpoch: 168 / 1000\nw1: [10.65730992] w2: [0.44691449] bias: [16.62767928] loss: 69.3241303040444\nEpoch: 169 / 1000\nw1: [10.67296352] w2: [0.42460584] bias: [16.63185334] loss: 69.24791328993315\nEpoch: 170 / 1000\nw1: [10.68854556] w2: [0.4022857] bias: [16.63591502] loss: 69.17196654665355\nEpoch: 171 / 1000\nw1: [10.70405765] w2: [0.37995496] bias: [16.63986738] loss: 69.09628282269593\nEpoch: 172 / 1000\nw1: [10.71950138] w2: [0.35761453] bias: [16.64371341] loss: 69.02085524332935\nEpoch: 173 / 1000\nw1: [10.73487828] w2: [0.33526525] bias: [16.64745599] loss: 68.94567729032708\nEpoch: 174 / 1000\nw1: [10.75018984] w2: [0.31290796] bias: [16.65109795] loss: 68.87074278278475\nEpoch: 175 / 1000\nw1: [10.76543752] w2: [0.29054348] bias: [16.65464203] loss: 68.7960458589718\nEpoch: 176 / 1000\nw1: [10.78062272] w2: [0.26817258] bias: [16.6580909] loss: 68.72158095916072\nEpoch: 177 / 1000\nw1: [10.79574683] w2: [0.24579605] bias: [16.66144716] loss: 68.64734280938153\nEpoch: 178 / 1000\nw1: [10.81081118] w2: [0.22341462] bias: [16.66471332] loss: 68.57332640605132\nEpoch: 179 / 1000\nw1: [10.82581706] w2: [0.20102902] bias: [16.66789185] loss: 68.49952700143216\nEpoch: 180 / 1000\nw1: [10.84076576] w2: [0.17863995] bias: [16.67098513] loss: 68.42594008987219\nEpoch: 181 / 1000\nw1: [10.85565849] w2: [0.15624809] bias: [16.67399549] loss: 68.3525613947883\nEpoch: 182 / 1000\nw1: [10.87049646] w2: [0.13385412] bias: [16.67692518] loss: 68.27938685634992\nEpoch: 183 / 1000\nw1: [10.88528084] w2: [0.11145868] bias: [16.6797764] loss: 68.20641261982634\nEpoch: 184 / 1000\nw1: [10.90001275] w2: [0.08906239] bias: [16.68255129] loss: 68.13363502456207\nEpoch: 185 / 1000\nw1: [10.9146933] w2: [0.06666586] bias: [16.68525193] loss: 68.06105059354589\nEpoch: 186 / 1000\nw1: [10.92932357] w2: [0.0442697] bias: [16.68788033] loss: 67.98865602354198\nEpoch: 187 / 1000\nw1: [10.9439046] w2: [0.02187447] bias: [16.69043848] loss: 67.91644817575282\nEpoch: 188 / 1000\nw1: [10.95843741] w2: [-0.00051926] bias: [16.69292828] loss: 67.84442406698494\nEpoch: 189 / 1000\nw1: [10.97292298] w2: [-0.02291096] bias: [16.69535159] loss: 67.77258086129083\nEpoch: 190 / 1000\nw1: [10.98736229] w2: [-0.04530008] bias: [16.69771023] loss: 67.70091586206088\nEpoch: 191 / 1000\nw1: [11.00175626] w2: [-0.06768612] bias: [16.70000595] loss: 67.62942650454166\nEpoch: 192 / 1000\nw1: [11.0161058] w2: [-0.09006859] bias: [16.70224047] loss: 67.55811034875691\nEpoch: 193 / 1000\nw1: [11.03041181] w2: [-0.11244698] bias: [16.70441545] loss: 67.4869650728103\nEpoch: 194 / 1000\nw1: [11.04467515] w2: [-0.13482084] bias: [16.70653251] loss: 67.41598846654864\nEpoch: 195 / 1000\nw1: [11.05889665] w2: [-0.1571897] bias: [16.70859324] loss: 67.34517842556666\nEpoch: 196 / 1000\nw1: [11.07307713] w2: [-0.17955313] bias: [16.71059916] loss: 67.27453294553469\nEpoch: 197 / 1000\nw1: [11.0872174] w2: [-0.20191068] bias: [16.71255176] loss: 67.20405011683204\nEpoch: 198 / 1000\nw1: [11.10131821] w2: [-0.22426194] bias: [16.71445251] loss: 67.13372811946925\nEpoch: 199 / 1000\nw1: [11.11538033] w2: [-0.2466065] bias: [16.71630279] loss: 67.0635652182841\nEpoch: 200 / 1000\nw1: [11.12940449] w2: [-0.26894397] bias: [16.718104] loss: 66.99355975839633\nEpoch: 201 / 1000\nw1: [11.14339139] w2: [-0.29127396] bias: [16.71985746] loss: 66.92371016090718\nEpoch: 202 / 1000\nw1: [11.15734174] w2: [-0.31359609] bias: [16.72156448] loss: 66.85401491883069\nEpoch: 203 / 1000\nw1: [11.17125621] w2: [-0.33591001] bias: [16.72322631] loss: 66.78447259324402\nEpoch: 204 / 1000\nw1: [11.18513545] w2: [-0.35821536] bias: [16.72484419] loss: 66.7150818096453\nEpoch: 205 / 1000\nw1: [11.19898011] w2: [-0.3805118] bias: [16.72641931] loss: 66.64584125450754\nEpoch: 206 / 1000\nw1: [11.2127908] w2: [-0.402799] bias: [16.72795283] loss: 66.57674967201841\nEpoch: 207 / 1000\nw1: [11.22656813] w2: [-0.42507663] bias: [16.72944588] loss: 66.50780586099518\nEpoch: 208 / 1000\nw1: [11.2403127] w2: [-0.44734438] bias: [16.73089957] loss: 66.43900867196629\nEpoch: 209 / 1000\nw1: [11.25402507] w2: [-0.46960195] bias: [16.73231495] loss: 66.37035700440994\nEpoch: 210 / 1000\nw1: [11.2677058] w2: [-0.49184905] bias: [16.73369309] loss: 66.30184980414137\nEpoch: 211 / 1000\nw1: [11.28135543] w2: [-0.51408538] bias: [16.73503498] loss: 66.23348606084087\nEpoch: 212 / 1000\nw1: [11.2949745] w2: [-0.53631067] bias: [16.73634161] loss: 66.16526480571514\nEpoch: 213 / 1000\nw1: [11.30856352] w2: [-0.55852464] bias: [16.73761393] loss: 66.09718510928428\nEpoch: 214 / 1000\nw1: [11.32212299] w2: [-0.58072704] bias: [16.73885289] loss: 66.02924607928819\nEpoch: 215 / 1000\nw1: [11.33565339] w2: [-0.60291761] bias: [16.74005938] loss: 65.96144685870571\nEpoch: 216 / 1000\nw1: [11.34915521] w2: [-0.6250961] bias: [16.74123429] loss: 65.89378662388037\nEpoch: 217 / 1000\nw1: [11.36262889] w2: [-0.64726228] bias: [16.74237848] loss: 65.82626458274714\nEpoch: 218 / 1000\nw1: [11.3760749] w2: [-0.6694159] bias: [16.74349277] loss: 65.75887997315465\nEpoch: 219 / 1000\nw1: [11.38949367] w2: [-0.69155676] bias: [16.74457798] loss: 65.69163206127773\nEpoch: 220 / 1000\nw1: [11.40288562] w2: [-0.71368461] bias: [16.7456349] loss: 65.62452014011546\nEpoch: 221 / 1000\nw1: [11.41625117] w2: [-0.73579927] bias: [16.74666429] loss: 65.55754352807024\nEpoch: 222 / 1000\nw1: [11.42959072] w2: [-0.75790051] bias: [16.74766691] loss: 65.49070156760305\nEpoch: 223 / 1000\nw1: [11.44290467] w2: [-0.77998813] bias: [16.74864348] loss: 65.42399362396145\nEpoch: 224 / 1000\nw1: [11.45619339] w2: [-0.80206196] bias: [16.74959469] loss: 65.35741908397587\nEpoch: 225 / 1000\nw1: [11.46945727] w2: [-0.82412178] bias: [16.75052125] loss: 65.29097735492081\nEpoch: 226 / 1000\nw1: [11.48269665] w2: [-0.84616743] bias: [16.75142382] loss: 65.2246678634373\nEpoch: 227 / 1000\nw1: [11.4959119] w2: [-0.86819873] bias: [16.75230305] loss: 65.1584900545133\nEpoch: 228 / 1000\nw1: [11.50910336] w2: [-0.8902155] bias: [16.75315957] loss: 65.09244339051924\nEpoch: 229 / 1000\nw1: [11.52227136] w2: [-0.91221757] bias: [16.753994] loss: 65.0265273502952\nEpoch: 230 / 1000\nw1: [11.53541623] w2: [-0.9342048] bias: [16.75480693] loss: 64.9607414282874\nEpoch: 231 / 1000\nw1: [11.54853828] w2: [-0.95617701] bias: [16.75559895] loss: 64.89508513373102\nEpoch: 232 / 1000\nw1: [11.56163782] w2: [-0.97813407] bias: [16.75637061] loss: 64.82955798987723\nEpoch: 233 / 1000\nw1: [11.57471516] w2: [-1.00007581] bias: [16.75712249] loss: 64.76415953326165\nEpoch: 234 / 1000\nw1: [11.58777058] w2: [-1.02200211] bias: [16.7578551] loss: 64.69888931301207\nEpoch: 235 / 1000\nw1: [11.60080437] w2: [-1.04391281] bias: [16.75856897] loss: 64.63374689019388\nEpoch: 236 / 1000\nw1: [11.61381681] w2: [-1.0658078] bias: [16.7592646] loss: 64.56873183719011\nEpoch: 237 / 1000\nw1: [11.62680817] w2: [-1.08768693] bias: [16.7599425] loss: 64.50384373711536\nEpoch: 238 / 1000\nw1: [11.63977871] w2: [-1.10955009] bias: [16.76060313] loss: 64.4390821832609\nEpoch: 239 / 1000\nw1: [11.65272868] w2: [-1.13139715] bias: [16.76124696] loss: 64.37444677856983\nEpoch: 240 / 1000\nw1: [11.66565834] w2: [-1.15322799] bias: [16.76187446] loss: 64.30993713514043\nEpoch: 241 / 1000\nw1: [11.67856793] w2: [-1.17504251] bias: [16.76248605] loss: 64.24555287375615\nEpoch: 242 / 1000\nw1: [11.69145768] w2: [-1.19684059] bias: [16.76308217] loss: 64.18129362344095\nEpoch: 243 / 1000\nw1: [11.70432783] w2: [-1.21862212] bias: [16.76366323] loss: 64.11715902103856\nEpoch: 244 / 1000\nw1: [11.7171786] w2: [-1.240387] bias: [16.76422965] loss: 64.0531487108143\nEpoch: 245 / 1000\nw1: [11.7300102] w2: [-1.26213513] bias: [16.76478181] loss: 63.98926234407838\nEpoch: 246 / 1000\nw1: [11.74282286] w2: [-1.28386641] bias: [16.7653201] loss: 63.92549957882945\nEpoch: 247 / 1000\nw1: [11.75561677] w2: [-1.30558076] bias: [16.76584489] loss: 63.86186007941725\nEpoch: 248 / 1000\nw1: [11.76839215] w2: [-1.32727808] bias: [16.76635656] loss: 63.798343516223404\nEpoch: 249 / 1000\nw1: [11.78114918] w2: [-1.34895828] bias: [16.76685544] loss: 63.734949565359436\nEpoch: 250 / 1000\nw1: [11.79388806] w2: [-1.37062128] bias: [16.76734189] loss: 63.671677908380985\nEpoch: 251 / 1000\nw1: [11.80660898] w2: [-1.392267] bias: [16.76781623] loss: 63.60852823201734\nEpoch: 252 / 1000\nw1: [11.81931212] w2: [-1.41389536] bias: [16.7682788] loss: 63.54550022791546\nEpoch: 253 / 1000\nw1: [11.83199764] w2: [-1.43550628] bias: [16.76872992] loss: 63.482593592397954\nEpoch: 254 / 1000\nw1: [11.84466574] w2: [-1.45709968] bias: [16.76916988] loss: 63.419808026233746\nEpoch: 255 / 1000\nw1: [11.85731657] w2: [-1.47867551] bias: [16.76959898] loss: 63.35714323442133\nEpoch: 256 / 1000\nw1: [11.8699503] w2: [-1.50023368] bias: [16.77001753] loss: 63.29459892598353\nEpoch: 257 / 1000\nw1: [11.88256709] w2: [-1.52177414] bias: [16.7704258] loss: 63.23217481377323\nEpoch: 258 / 1000\nw1: [11.8951671] w2: [-1.54329682] bias: [16.77082407] loss: 63.16987061428967\nEpoch: 259 / 1000\nw1: [11.90775047] w2: [-1.56480166] bias: [16.7712126] loss: 63.107686047504494\nEpoch: 260 / 1000\nw1: [11.92031736] w2: [-1.58628859] bias: [16.77159167] loss: 63.04562083669712\nEpoch: 261 / 1000\nw1: [11.9328679] w2: [-1.60775757] bias: [16.77196151] loss: 62.983674708299006\nEpoch: 262 / 1000\nw1: [11.94540224] w2: [-1.62920853] bias: [16.77232238] loss: 62.921847391746184\nEpoch: 263 / 1000\nw1: [11.95792052] w2: [-1.65064142] bias: [16.77267451] loss: 62.86013861933982\nEpoch: 264 / 1000\nw1: [11.97042287] w2: [-1.6720562] bias: [16.77301815] loss: 62.798548126114085\nEpoch: 265 / 1000\nw1: [11.98290942] w2: [-1.69345281] bias: [16.77335351] loss: 62.73707564971119\nEpoch: 266 / 1000\nw1: [11.9953803] w2: [-1.7148312] bias: [16.77368083] loss: 62.675720930263154\nEpoch: 267 / 1000\nw1: [12.00783562] w2: [-1.73619134] bias: [16.7740003] loss: 62.61448371027983\nEpoch: 268 / 1000\nw1: [12.02027552] w2: [-1.75753317] bias: [16.77431215] loss: 62.553363734542906\nEpoch: 269 / 1000\nw1: [12.03270011] w2: [-1.77885665] bias: [16.77461658] loss: 62.492360750005645\nEpoch: 270 / 1000\nw1: [12.04510949] w2: [-1.80016175] bias: [16.77491377] loss: 62.431474505697935\nEpoch: 271 / 1000\nw1: [12.0575038] w2: [-1.82144842] bias: [16.77520394] loss: 62.370704752636435\nEpoch: 272 / 1000\nw1: [12.06988313] w2: [-1.84271663] bias: [16.77548725] loss: 62.310051243739544\nEpoch: 273 / 1000\nw1: [12.08224758] w2: [-1.86396634] bias: [16.7757639] loss: 62.249513733746795\nEpoch: 274 / 1000\nw1: [12.09459728] w2: [-1.88519753] bias: [16.77603406] loss: 62.18909197914273\nEpoch: 275 / 1000\nw1: [12.1069323] w2: [-1.90641014] bias: [16.77629791] loss: 62.12878573808469\nEpoch: 276 / 1000\nw1: [12.11925276] w2: [-1.92760416] bias: [16.7765556] loss: 62.06859477033449\nEpoch: 277 / 1000\nw1: [12.13155876] w2: [-1.94877955] bias: [16.77680731] loss: 62.008518837193805\nEpoch: 278 / 1000\nw1: [12.14385037] w2: [-1.96993629] bias: [16.7770532] loss: 61.948557701442844\nEpoch: 279 / 1000\nw1: [12.15612771] w2: [-1.99107435] bias: [16.77729341] loss: 61.88871112728251\nEpoch: 280 / 1000\nw1: [12.16839085] w2: [-2.01219369] bias: [16.77752809] loss: 61.828978880279365\nEpoch: 281 / 1000\nw1: [12.18063988] w2: [-2.03329431] bias: [16.7777574] loss: 61.76936072731376\nEpoch: 282 / 1000\nw1: [12.19287489] w2: [-2.05437616] bias: [16.77798147] loss: 61.709856436530615\nEpoch: 283 / 1000\nw1: [12.20509596] w2: [-2.07543924] bias: [16.77820045] loss: 61.65046577729271\nEpoch: 284 / 1000\nw1: [12.21730317] w2: [-2.09648352] bias: [16.77841446] loss: 61.5911885201367\nEpoch: 285 / 1000\nw1: [12.22949661] w2: [-2.11750898] bias: [16.77862364] loss: 61.53202443673122\nEpoch: 286 / 1000\nw1: [12.24167634] w2: [-2.13851559] bias: [16.77882811] loss: 61.47297329983731\nEpoch: 287 / 1000\nw1: [12.25384245] w2: [-2.15950335] bias: [16.77902801] loss: 61.41403488327097\nEpoch: 288 / 1000\nw1: [12.26599501] w2: [-2.18047223] bias: [16.77922343] loss: 61.35520896186758\nEpoch: 289 / 1000\nw1: [12.27813409] w2: [-2.20142222] bias: [16.77941452] loss: 61.2964953114483\nEpoch: 290 / 1000\nw1: [12.29025976] w2: [-2.22235331] bias: [16.77960137] loss: 61.237893708788185\nEpoch: 291 / 1000\nw1: [12.30237209] w2: [-2.24326547] bias: [16.7797841] loss: 61.17940393158601\nEpoch: 292 / 1000\nw1: [12.31447114] w2: [-2.2641587] bias: [16.77996282] loss: 61.12102575843564\nEpoch: 293 / 1000\nw1: [12.32655699] w2: [-2.28503298] bias: [16.78013763] loss: 61.062758968798946\nEpoch: 294 / 1000\nw1: [12.3386297] w2: [-2.3058883] bias: [16.78030863] loss: 61.0046033429801\nEpoch: 295 / 1000\nw1: [12.35068932] w2: [-2.32672465] bias: [16.78047593] loss: 60.946558662101225\nEpoch: 296 / 1000\nw1: [12.36273593] w2: [-2.34754203] bias: [16.78063961] loss: 60.88862470807938\nEpoch: 297 / 1000\nw1: [12.37476957] w2: [-2.36834041] bias: [16.78079977] loss: 60.83080126360464\nEpoch: 298 / 1000\nw1: [12.38679031] w2: [-2.38911979] bias: [16.78095651] loss: 60.77308811211937\nEpoch: 299 / 1000\nw1: [12.39879821] w2: [-2.40988017] bias: [16.78110991] loss: 60.71548503779861\nEpoch: 300 / 1000\nw1: [12.41079332] w2: [-2.43062154] bias: [16.78126006] loss: 60.657991825531376\nEpoch: 301 / 1000\nw1: [12.4227757] w2: [-2.45134389] bias: [16.78140704] loss: 60.60060826090307\nEpoch: 302 / 1000\nw1: [12.43474539] w2: [-2.47204721] bias: [16.78155094] loss: 60.543334130178685\nEpoch: 303 / 1000\nw1: [12.44670246] w2: [-2.4927315] bias: [16.78169183] loss: 60.48616922028689\nEpoch: 304 / 1000\nw1: [12.45864695] w2: [-2.51339676] bias: [16.78182979] loss: 60.429113318805\nEpoch: 305 / 1000\nw1: [12.47057891] w2: [-2.53404298] bias: [16.78196489] loss: 60.37216621394464\nEpoch: 306 / 1000\nw1: [12.48249839] w2: [-2.55467016] bias: [16.78209721] loss: 60.31532769453818\nEpoch: 307 / 1000\nw1: [12.49440545] w2: [-2.5752783] bias: [16.78222683] loss: 60.25859755002588\nEpoch: 308 / 1000\nw1: [12.50630012] w2: [-2.59586739] bias: [16.7823538] loss: 60.201975570443594\nEpoch: 309 / 1000\nw1: [12.51818245] w2: [-2.61643743] bias: [16.78247819] loss: 60.145461546411255\nEpoch: 310 / 1000\nw1: [12.53005248] w2: [-2.63698842] bias: [16.78260008] loss: 60.08905526912183\nEpoch: 311 / 1000\nw1: [12.54191027] w2: [-2.65752037] bias: [16.78271953] loss: 60.03275653033076\nEpoch: 312 / 1000\nw1: [12.55375585] w2: [-2.67803326] bias: [16.78283659] loss: 59.9765651223462\nEpoch: 313 / 1000\nw1: [12.56558927] w2: [-2.69852711] bias: [16.78295133] loss: 59.92048083801941\nEpoch: 314 / 1000\nw1: [12.57741056] w2: [-2.71900191] bias: [16.7830638] loss: 59.86450347073584\nEpoch: 315 / 1000\nw1: [12.58921977] w2: [-2.73945767] bias: [16.78317407] loss: 59.808632814406664\nEpoch: 316 / 1000\nw1: [12.60101694] w2: [-2.75989438] bias: [16.78328219] loss: 59.752868663460575\nEpoch: 317 / 1000\nw1: [12.6128021] w2: [-2.78031205] bias: [16.78338821] loss: 59.69721081283612\nEpoch: 318 / 1000\nw1: [12.62457529] w2: [-2.80071068] bias: [16.78349218] loss: 59.64165905797439\nEpoch: 319 / 1000\nw1: [12.63633656] w2: [-2.82109028] bias: [16.78359416] loss: 59.586213194812004\nEpoch: 320 / 1000\nw1: [12.64808593] w2: [-2.84145084] bias: [16.78369419] loss: 59.5308730197745\nEpoch: 321 / 1000\nw1: [12.65982344] w2: [-2.86179238] bias: [16.78379233] loss: 59.475638329769986\nEpoch: 322 / 1000\nw1: [12.67154913] w2: [-2.8821149] bias: [16.78388862] loss: 59.42050892218317\nEpoch: 323 / 1000\nw1: [12.68326303] w2: [-2.9024184] bias: [16.78398311] loss: 59.36548459486956\nEpoch: 324 / 1000\nw1: [12.69496518] w2: [-2.92270288] bias: [16.78407583] loss: 59.310565146150104\nEpoch: 325 / 1000\nw1: [12.70665561] w2: [-2.94296836] bias: [16.78416685] loss: 59.2557503748059\nEpoch: 326 / 1000\nw1: [12.71833434] w2: [-2.96321483] bias: [16.78425618] loss: 59.201040080073255\nEpoch: 327 / 1000\nw1: [12.73000142] w2: [-2.98344231] bias: [16.78434389] loss: 59.14643406163898\nEpoch: 328 / 1000\nw1: [12.74165687] w2: [-3.0036508] bias: [16.78443] loss: 59.091932119635835\nEpoch: 329 / 1000\nw1: [12.75330073] w2: [-3.02384031] bias: [16.78451456] loss: 59.03753405463831\nEpoch: 330 / 1000\nw1: [12.76493302] w2: [-3.04401085] bias: [16.7845976] loss: 58.98323966765836\nEpoch: 331 / 1000\nw1: [12.77655377] w2: [-3.06416241] bias: [16.78467916] loss: 58.92904876014163\nEpoch: 332 / 1000\nw1: [12.78816302] w2: [-3.08429502] bias: [16.78475928] loss: 58.87496113396362\nEpoch: 333 / 1000\nw1: [12.79976079] w2: [-3.10440867] bias: [16.78483798] loss: 58.82097659142605\nEpoch: 334 / 1000\nw1: [12.8113471] w2: [-3.12450339] bias: [16.78491532] loss: 58.76709493525355\nEpoch: 335 / 1000\nw1: [12.822922] w2: [-3.14457916] bias: [16.78499131] loss: 58.7133159685903\nEpoch: 336 / 1000\nw1: [12.83448549] w2: [-3.16463601] bias: [16.78506598] loss: 58.659639494996824\nEpoch: 337 / 1000\nw1: [12.84603762] w2: [-3.18467395] bias: [16.78513938] loss: 58.6060653184471\nEpoch: 338 / 1000\nw1: [12.8575784] w2: [-3.20469297] bias: [16.78521153] loss: 58.5525932433256\nEpoch: 339 / 1000\nw1: [12.86910787] w2: [-3.2246931] bias: [16.78528246] loss: 58.49922307442452\nEpoch: 340 / 1000\nw1: [12.88062604] w2: [-3.24467433] bias: [16.7853522] loss: 58.44595461694115\nEpoch: 341 / 1000\nw1: [12.89213294] w2: [-3.26463669] bias: [16.78542077] loss: 58.39278767647528\nEpoch: 342 / 1000\nw1: [12.9036286] w2: [-3.28458018] bias: [16.78548821] loss: 58.33972205902679\nEpoch: 343 / 1000\nw1: [12.91511304] w2: [-3.30450481] bias: [16.78555453] loss: 58.286757570993295\nEpoch: 344 / 1000\nw1: [12.92658628] w2: [-3.32441059] bias: [16.78561977] loss: 58.23389401916784\nEpoch: 345 / 1000\nw1: [12.93804834] w2: [-3.34429754] bias: [16.78568395] loss: 58.18113121073679\nEpoch: 346 / 1000\nw1: [12.94949926] w2: [-3.36416566] bias: [16.7857471] loss: 58.1284689532776\nEpoch: 347 / 1000\nw1: [12.96093905] w2: [-3.38401496] bias: [16.78580923] loss: 58.07590705475696\nEpoch: 348 / 1000\nw1: [12.97236773] w2: [-3.40384546] bias: [16.78587038] loss: 58.023445323528755\nEpoch: 349 / 1000\nw1: [12.98378533] w2: [-3.42365717] bias: [16.78593056] loss: 57.97108356833219\nEpoch: 350 / 1000\nw1: [12.99519186] w2: [-3.4434501] bias: [16.78598979] loss: 57.918821598289945\nEpoch: 351 / 1000\nw1: [13.00658736] w2: [-3.46322425] bias: [16.7860481] loss: 57.866659222906506\nEpoch: 352 / 1000\nw1: [13.01797183] w2: [-3.48297966] bias: [16.78610551] loss: 57.814596252066416\nEpoch: 353 / 1000\nw1: [13.0293453] w2: [-3.50271631] bias: [16.78616203] loss: 57.7626324960326\nEpoch: 354 / 1000\nw1: [13.0407078] w2: [-3.52243423] bias: [16.78621769] loss: 57.71076776544485\nEpoch: 355 / 1000\nw1: [13.05205933] w2: [-3.54213344] bias: [16.78627251] loss: 57.659001871318274\nEpoch: 356 / 1000\nw1: [13.06339992] w2: [-3.56181393] bias: [16.7863265] loss: 57.607334625041766\nEpoch: 357 / 1000\nw1: [13.07472959] w2: [-3.58147573] bias: [16.78637968] loss: 57.555765838376544\nEpoch: 358 / 1000\nw1: [13.08604836] w2: [-3.60111884] bias: [16.78643207] loss: 57.504295323454855\nEpoch: 359 / 1000\nw1: [13.09735624] w2: [-3.62074329] bias: [16.78648369] loss: 57.45292289277849\nEpoch: 360 / 1000\nw1: [13.10865327] w2: [-3.64034908] bias: [16.78653455] loss: 57.40164835921752\nEpoch: 361 / 1000\nw1: [13.11993944] w2: [-3.65993623] bias: [16.78658467] loss: 57.35047153600902\nEpoch: 362 / 1000\nw1: [13.13121479] w2: [-3.67950474] bias: [16.78663406] loss: 57.299392236755736\nEpoch: 363 / 1000\nw1: [13.14247932] w2: [-3.69905464] bias: [16.78668274] loss: 57.248410275424966\nEpoch: 364 / 1000\nw1: [13.15373307] w2: [-3.71858594] bias: [16.78673073] loss: 57.197525466347294\nEpoch: 365 / 1000\nw1: [13.16497604] w2: [-3.73809865] bias: [16.78677803] loss: 57.14673762421545\nEpoch: 366 / 1000\nw1: [13.17620825] w2: [-3.75759278] bias: [16.78682467] loss: 57.09604656408315\nEpoch: 367 / 1000\nw1: [13.18742973] w2: [-3.77706834] bias: [16.78687065] loss: 57.04545210136407\nEpoch: 368 / 1000\nw1: [13.19864047] w2: [-3.79652536] bias: [16.786916] loss: 56.99495405183064\nEpoch: 369 / 1000\nw1: [13.20984052] w2: [-3.81596385] bias: [16.78696072] loss: 56.944552231613045\nEpoch: 370 / 1000\nw1: [13.22102987] w2: [-3.83538381] bias: [16.78700482] loss: 56.89424645719823\nEpoch: 371 / 1000\nw1: [13.23220854] w2: [-3.85478527] bias: [16.78704832] loss: 56.844036545428814\nEpoch: 372 / 1000\nw1: [13.24337656] w2: [-3.87416824] bias: [16.78709123] loss: 56.793922313502094\nEpoch: 373 / 1000\nw1: [13.25453394] w2: [-3.89353273] bias: [16.78713356] loss: 56.7439035789691\nEpoch: 374 / 1000\nw1: [13.26568069] w2: [-3.91287875] bias: [16.78717532] loss: 56.69398015973363\nEpoch: 375 / 1000\nw1: [13.27681683] w2: [-3.93220633] bias: [16.78721653] loss: 56.64415187405132\nEpoch: 376 / 1000\nw1: [13.28794237] w2: [-3.95151547] bias: [16.78725718] loss: 56.594418540528636\nEpoch: 377 / 1000\nw1: [13.29905733] w2: [-3.97080619] bias: [16.78729731] loss: 56.54477997812209\nEpoch: 378 / 1000\nw1: [13.31016173] w2: [-3.99007851] bias: [16.78733691] loss: 56.49523600613724\nEpoch: 379 / 1000\nw1: [13.32125558] w2: [-4.00933244] bias: [16.78737599] loss: 56.445786444227856\nEpoch: 380 / 1000\nw1: [13.33233889] w2: [-4.02856799] bias: [16.78741457] loss: 56.396431112395014\nEpoch: 381 / 1000\nw1: [13.34341168] w2: [-4.04778518] bias: [16.78745265] loss: 56.34716983098632\nEpoch: 382 / 1000\nw1: [13.35447396] w2: [-4.06698402] bias: [16.78749024] loss: 56.298002420694964\nEpoch: 383 / 1000\nw1: [13.36552576] w2: [-4.08616454] bias: [16.78752735] loss: 56.248928702558956\nEpoch: 384 / 1000\nw1: [13.37656707] w2: [-4.10532673] bias: [16.78756399] loss: 56.19994849796031\nEpoch: 385 / 1000\nw1: [13.38759792] w2: [-4.12447063] bias: [16.78760017] loss: 56.15106162862413\nEpoch: 386 / 1000\nw1: [13.39861832] w2: [-4.14359624] bias: [16.7876359] loss: 56.10226791661796\nEpoch: 387 / 1000\nw1: [13.40962829] w2: [-4.16270358] bias: [16.78767118] loss: 56.0535671843509\nEpoch: 388 / 1000\nw1: [13.42062783] w2: [-4.18179267] bias: [16.78770602] loss: 56.004959254572825\nEpoch: 389 / 1000\nw1: [13.43161697] w2: [-4.20086352] bias: [16.78774043] loss: 55.95644395037362\nEpoch: 390 / 1000\nw1: [13.44259571] w2: [-4.21991614] bias: [16.78777442] loss: 55.90802109518247\nEpoch: 391 / 1000\nw1: [13.45356407] w2: [-4.23895055] bias: [16.78780799] loss: 55.85969051276699\nEpoch: 392 / 1000\nw1: [13.46452206] w2: [-4.25796677] bias: [16.78784115] loss: 55.81145202723258\nEpoch: 393 / 1000\nw1: [13.47546969] w2: [-4.27696481] bias: [16.78787391] loss: 55.76330546302163\nEpoch: 394 / 1000\nw1: [13.48640698] w2: [-4.29594469] bias: [16.78790627] loss: 55.71525064491278\nEpoch: 395 / 1000\nw1: [13.49733395] w2: [-4.31490642] bias: [16.78793825] loss: 55.66728739802021\nEpoch: 396 / 1000\nw1: [13.5082506] w2: [-4.33385002] bias: [16.78796984] loss: 55.619415547792904\nEpoch: 397 / 1000\nw1: [13.51915694] w2: [-4.3527755] bias: [16.78800105] loss: 55.57163492001395\nEpoch: 398 / 1000\nw1: [13.530053] w2: [-4.37168288] bias: [16.78803189] loss: 55.52394534079979\nEpoch: 399 / 1000\nw1: [13.54093877] w2: [-4.39057218] bias: [16.78806236] loss: 55.47634663659953\nEpoch: 400 / 1000\nw1: [13.55181428] w2: [-4.4094434] bias: [16.78809247] loss: 55.4288386341943\nEpoch: 401 / 1000\nw1: [13.56267954] w2: [-4.42829658] bias: [16.78812223] loss: 55.381421160696405\nEpoch: 402 / 1000\nw1: [13.57353455] w2: [-4.44713171] bias: [16.78815164] loss: 55.334094043548845\nEpoch: 403 / 1000\nw1: [13.58437934] w2: [-4.46594883] bias: [16.78818071] loss: 55.28685711052442\nEpoch: 404 / 1000\nw1: [13.59521391] w2: [-4.48474793] bias: [16.78820943] loss: 55.239710189725166\nEpoch: 405 / 1000\nw1: [13.60603828] w2: [-4.50352905] bias: [16.78823782] loss: 55.19265310958164\nEpoch: 406 / 1000\nw1: [13.61685245] w2: [-4.52229219] bias: [16.78826588] loss: 55.145685698852326\nEpoch: 407 / 1000\nw1: [13.62765645] w2: [-4.54103738] bias: [16.78829362] loss: 55.09880778662277\nEpoch: 408 / 1000\nw1: [13.63845027] w2: [-4.55976462] bias: [16.78832103] loss: 55.05201920230514\nEpoch: 409 / 1000\nw1: [13.64923394] w2: [-4.57847393] bias: [16.78834813] loss: 55.00531977563738\nEpoch: 410 / 1000\nw1: [13.66000746] w2: [-4.59716533] bias: [16.78837492] loss: 54.95870933668269\nEpoch: 411 / 1000\nw1: [13.67077085] w2: [-4.61583884] bias: [16.7884014] loss: 54.91218771582875\nEpoch: 412 / 1000\nw1: [13.68152412] w2: [-4.63449447] bias: [16.78842757] loss: 54.865754743787186\nEpoch: 413 / 1000\nw1: [13.69226728] w2: [-4.65313223] bias: [16.78845345] loss: 54.819410251592814\nEpoch: 414 / 1000\nw1: [13.70300034] w2: [-4.67175215] bias: [16.78847902] loss: 54.77315407060303\nEpoch: 415 / 1000\nw1: [13.71372331] w2: [-4.69035423] bias: [16.78850431] loss: 54.726986032497194\nEpoch: 416 / 1000\nw1: [13.72443621] w2: [-4.7089385] bias: [16.78852931] loss: 54.68090596927598\nEpoch: 417 / 1000\nw1: [13.73513904] w2: [-4.72750497] bias: [16.78855402] loss: 54.634913713260666\nEpoch: 418 / 1000\nw1: [13.74583182] w2: [-4.74605366] bias: [16.78857845] loss: 54.58900909709261\nEpoch: 419 / 1000\nw1: [13.75651456] w2: [-4.76458458] bias: [16.7886026] loss: 54.543191953732546\nEpoch: 420 / 1000\nw1: [13.76718726] w2: [-4.78309775] bias: [16.78862648] loss: 54.49746211645995\nEpoch: 421 / 1000\nw1: [13.77784994] w2: [-4.80159318] bias: [16.78865008] loss: 54.451819418872454\nEpoch: 422 / 1000\nw1: [13.78850262] w2: [-4.8200709] bias: [16.78867342] loss: 54.40626369488518\nEpoch: 423 / 1000\nw1: [13.7991453] w2: [-4.83853091] bias: [16.78869649] loss: 54.36079477873014\nEpoch: 424 / 1000\nw1: [13.80977799] w2: [-4.85697323] bias: [16.7887193] loss: 54.3154125049556\nEpoch: 425 / 1000\nw1: [13.8204007] w2: [-4.87539789] bias: [16.78874185] loss: 54.270116708425476\nEpoch: 426 / 1000\nw1: [13.83101345] w2: [-4.89380489] bias: [16.78876414] loss: 54.22490722431866\nEpoch: 427 / 1000\nw1: [13.84161624] w2: [-4.91219425] bias: [16.78878617] loss: 54.179783888128526\nEpoch: 428 / 1000\nw1: [13.85220909] w2: [-4.93056598] bias: [16.78880796] loss: 54.13474653566218\nEpoch: 429 / 1000\nw1: [13.86279201] w2: [-4.94892012] bias: [16.78882949] loss: 54.08979500303993\nEpoch: 430 / 1000\nw1: [13.873365] w2: [-4.96725666] bias: [16.78885078] loss: 54.044929126694655\nEpoch: 431 / 1000\nw1: [13.88392808] w2: [-4.98557562] bias: [16.78887183] loss: 54.00014874337119\nEpoch: 432 / 1000\nw1: [13.89448126] w2: [-5.00387703] bias: [16.78889263] loss: 53.95545369012575\nEpoch: 433 / 1000\nw1: [13.90502455] w2: [-5.0221609] bias: [16.78891319] loss: 53.910843804325275\nEpoch: 434 / 1000\nw1: [13.91555796] w2: [-5.04042724] bias: [16.78893352] loss: 53.86631892364685\nEpoch: 435 / 1000\nw1: [13.9260815] w2: [-5.05867607] bias: [16.78895361] loss: 53.82187888607716\nEpoch: 436 / 1000\nw1: [13.93659517] w2: [-5.0769074] bias: [16.78897347] loss: 53.77752352991176\nEpoch: 437 / 1000\nw1: [13.947099] w2: [-5.09512126] bias: [16.7889931] loss: 53.733252693754636\nEpoch: 438 / 1000\nw1: [13.95759299] w2: [-5.11331766] bias: [16.7890125] loss: 53.689066216517496\nEpoch: 439 / 1000\nw1: [13.96807715] w2: [-5.13149661] bias: [16.78903167] loss: 53.644963937419185\nEpoch: 440 / 1000\nw1: [13.97855149] w2: [-5.14965814] bias: [16.78905062] loss: 53.60094569598515\nEpoch: 441 / 1000\nw1: [13.98901603] w2: [-5.16780225] bias: [16.78906934] loss: 53.55701133204684\nEpoch: 442 / 1000\nw1: [13.99947076] w2: [-5.18592896] bias: [16.78908785] loss: 53.51316068574103\nEpoch: 443 / 1000\nw1: [14.00991571] w2: [-5.2040383] bias: [16.78910613] loss: 53.469393597509324\nEpoch: 444 / 1000\nw1: [14.02035088] w2: [-5.22213026] bias: [16.7891242] loss: 53.42570990809755\nEpoch: 445 / 1000\nw1: [14.03077628] w2: [-5.24020488] bias: [16.78914205] loss: 53.38210945855515\nEpoch: 446 / 1000\nw1: [14.04119192] w2: [-5.25826217] bias: [16.78915969] loss: 53.33859209023461\nEpoch: 447 / 1000\nw1: [14.05159782] w2: [-5.27630214] bias: [16.78917712] loss: 53.29515764479089\nEpoch: 448 / 1000\nw1: [14.06199398] w2: [-5.29432481] bias: [16.78919434] loss: 53.25180596418082\nEpoch: 449 / 1000\nw1: [14.07238041] w2: [-5.3123302] bias: [16.78921134] loss: 53.20853689066252\nEpoch: 450 / 1000\nw1: [14.08275712] w2: [-5.33031832] bias: [16.78922814] loss: 53.165350266794846\nEpoch: 451 / 1000\nw1: [14.09312412] w2: [-5.34828918] bias: [16.78924474] loss: 53.122245935436794\nEpoch: 452 / 1000\nw1: [14.10348143] w2: [-5.36624282] bias: [16.78926112] loss: 53.079223739746915\nEpoch: 453 / 1000\nw1: [14.11382905] w2: [-5.38417923] bias: [16.78927731] loss: 53.036283523182775\nEpoch: 454 / 1000\nw1: [14.12416699] w2: [-5.40209843] bias: [16.78929329] loss: 52.99342512950037\nEpoch: 455 / 1000\nw1: [14.13449526] w2: [-5.42000045] bias: [16.78930908] loss: 52.9506484027535\nEpoch: 456 / 1000\nw1: [14.14481387] w2: [-5.4378853] bias: [16.78932466] loss: 52.90795318729327\nEpoch: 457 / 1000\nw1: [14.15512283] w2: [-5.455753] bias: [16.78934005] loss: 52.8653393277675\nEpoch: 458 / 1000\nw1: [14.16542215] w2: [-5.47360355] bias: [16.78935524] loss: 52.822806669120155\nEpoch: 459 / 1000\nw1: [14.17571185] w2: [-5.49143698] bias: [16.78937023] loss: 52.780355056590736\nEpoch: 460 / 1000\nw1: [14.18599192] w2: [-5.5092533] bias: [16.78938503] loss: 52.737984335713776\nEpoch: 461 / 1000\nw1: [14.19626238] w2: [-5.52705253] bias: [16.78939964] loss: 52.69569435231826\nEpoch: 462 / 1000\nw1: [14.20652324] w2: [-5.54483469] bias: [16.78941405] loss: 52.65348495252704\nEpoch: 463 / 1000\nw1: [14.21677452] w2: [-5.56259979] bias: [16.78942828] loss: 52.611355982756294\nEpoch: 464 / 1000\nw1: [14.2270162] w2: [-5.58034784] bias: [16.78944231] loss: 52.569307289714935\nEpoch: 465 / 1000\nw1: [14.23724832] w2: [-5.59807887] bias: [16.78945616] loss: 52.52733872040408\nEpoch: 466 / 1000\nw1: [14.24747088] w2: [-5.61579288] bias: [16.78946982] loss: 52.485450122116525\nEpoch: 467 / 1000\nw1: [14.25768388] w2: [-5.6334899] bias: [16.78948329] loss: 52.443641342436074\nEpoch: 468 / 1000\nw1: [14.26788734] w2: [-5.65116994] bias: [16.78949658] loss: 52.401912229237105\nEpoch: 469 / 1000\nw1: [14.27808126] w2: [-5.66883302] bias: [16.78950968] loss: 52.36026263068394\nEpoch: 470 / 1000\nw1: [14.28826566] w2: [-5.68647915] bias: [16.7895226] loss: 52.31869239523035\nEpoch: 471 / 1000\nw1: [14.29844054] w2: [-5.70410835] bias: [16.78953533] loss: 52.277201371618936\nEpoch: 472 / 1000\nw1: [14.30860592] w2: [-5.72172064] bias: [16.78954789] loss: 52.23578940888059\nEpoch: 473 / 1000\nw1: [14.3187618] w2: [-5.73931602] bias: [16.78956026] loss: 52.194456356334015\nEpoch: 474 / 1000\nw1: [14.3289082] w2: [-5.75689452] bias: [16.78957245] loss: 52.1532020635851\nEpoch: 475 / 1000\nw1: [14.33904512] w2: [-5.77445615] bias: [16.78958447] loss: 52.11202638052637\nEpoch: 476 / 1000\nw1: [14.34917257] w2: [-5.79200093] bias: [16.7895963] loss: 52.070929157336515\nEpoch: 477 / 1000\nw1: [14.35929056] w2: [-5.80952888] bias: [16.78960796] loss: 52.029910244479744\nEpoch: 478 / 1000\nw1: [14.3693991] w2: [-5.82704] bias: [16.78961944] loss: 51.98896949270531\nEpoch: 479 / 1000\nw1: [14.3794982] w2: [-5.84453433] bias: [16.78963074] loss: 51.94810675304694\nEpoch: 480 / 1000\nw1: [14.38958787] w2: [-5.86201186] bias: [16.78964187] loss: 51.907321876822316\nEpoch: 481 / 1000\nw1: [14.39966812] w2: [-5.87947262] bias: [16.78965282] loss: 51.866614715632466\nEpoch: 482 / 1000\nw1: [14.40973895] w2: [-5.89691662] bias: [16.78966361] loss: 51.82598512136131\nEpoch: 483 / 1000\nw1: [14.41980038] w2: [-5.91434389] bias: [16.78967421] loss: 51.785432946175085\nEpoch: 484 / 1000\nw1: [14.42985242] w2: [-5.93175442] bias: [16.78968465] loss: 51.744958042521766\nEpoch: 485 / 1000\nw1: [14.43989507] w2: [-5.94914825] bias: [16.78969491] loss: 51.70456026313058\nEpoch: 486 / 1000\nw1: [14.44992835] w2: [-5.96652539] bias: [16.789705] loss: 51.66423946101149\nEpoch: 487 / 1000\nw1: [14.45995226] w2: [-5.98388585] bias: [16.78971492] loss: 51.62399548945456\nEpoch: 488 / 1000\nw1: [14.46996682] w2: [-6.00122965] bias: [16.78972468] loss: 51.583828202029515\nEpoch: 489 / 1000\nw1: [14.47997202] w2: [-6.0185568] bias: [16.78973426] loss: 51.54373745258519\nEpoch: 490 / 1000\nw1: [14.48996789] w2: [-6.03586732] bias: [16.78974367] loss: 51.50372309524896\nEpoch: 491 / 1000\nw1: [14.49995442] w2: [-6.05316122] bias: [16.78975292] loss: 51.46378498442624\nEpoch: 492 / 1000\nw1: [14.50993164] w2: [-6.07043853] bias: [16.789762] loss: 51.42392297479994\nEpoch: 493 / 1000\nw1: [14.51989954] w2: [-6.08769926] bias: [16.78977091] loss: 51.384136921329976\nEpoch: 494 / 1000\nw1: [14.52985814] w2: [-6.10494341] bias: [16.78977965] loss: 51.34442667925266\nEpoch: 495 / 1000\nw1: [14.53980744] w2: [-6.12217102] bias: [16.78978823] loss: 51.30479210408025\nEpoch: 496 / 1000\nw1: [14.54974746] w2: [-6.13938209] bias: [16.78979665] loss: 51.26523305160039\nEpoch: 497 / 1000\nw1: [14.55967821] w2: [-6.15657664] bias: [16.7898049] loss: 51.22574937787557\nEpoch: 498 / 1000\nw1: [14.56959969] w2: [-6.17375468] bias: [16.78981299] loss: 51.18634093924267\nEpoch: 499 / 1000\nw1: [14.57951191] w2: [-6.19091624] bias: [16.78982091] loss: 51.14700759231233\nEpoch: 500 / 1000\nw1: [14.58941488] w2: [-6.20806132] bias: [16.78982867] loss: 51.10774919396853\nEpoch: 501 / 1000\nw1: [14.59930861] w2: [-6.22518995] bias: [16.78983626] loss: 51.06856560136799\nEpoch: 502 / 1000\nw1: [14.60919311] w2: [-6.24230213] bias: [16.7898437] loss: 51.02945667193974\nEpoch: 503 / 1000\nw1: [14.61906839] w2: [-6.25939789] bias: [16.78985097] loss: 50.99042226338448\nEpoch: 504 / 1000\nw1: [14.62893445] w2: [-6.27647723] bias: [16.78985808] loss: 50.95146223367418\nEpoch: 505 / 1000\nw1: [14.63879131] w2: [-6.29354018] bias: [16.78986503] loss: 50.912576441051456\nEpoch: 506 / 1000\nw1: [14.64863898] w2: [-6.31058675] bias: [16.78987182] loss: 50.87376474402915\nEpoch: 507 / 1000\nw1: [14.65847745] w2: [-6.32761696] bias: [16.78987846] loss: 50.83502700138978\nEpoch: 508 / 1000\nw1: [14.66830676] w2: [-6.34463082] bias: [16.78988493] loss: 50.79636307218499\nEpoch: 509 / 1000\nw1: [14.67812689] w2: [-6.36162834] bias: [16.78989124] loss: 50.75777281573508\nEpoch: 510 / 1000\nw1: [14.68793786] w2: [-6.37860955] bias: [16.78989739] loss: 50.71925609162849\nEpoch: 511 / 1000\nw1: [14.69773968] w2: [-6.39557446] bias: [16.78990339] loss: 50.68081275972124\nEpoch: 512 / 1000\nw1: [14.70753236] w2: [-6.41252308] bias: [16.78990923] loss: 50.6424426801365\nEpoch: 513 / 1000\nw1: [14.71731591] w2: [-6.42945542] bias: [16.78991491] loss: 50.604145713264025\nEpoch: 514 / 1000\nw1: [14.72709033] w2: [-6.44637152] bias: [16.78992043] loss: 50.56592171975967\nEpoch: 515 / 1000\nw1: [14.73685563] w2: [-6.46327137] bias: [16.7899258] loss: 50.52777056054484\nEpoch: 516 / 1000\nw1: [14.74661183] w2: [-6.48015499] bias: [16.78993101] loss: 50.48969209680606\nEpoch: 517 / 1000\nw1: [14.75635893] w2: [-6.49702241] bias: [16.78993606] loss: 50.4516861899944\nEpoch: 518 / 1000\nw1: [14.76609694] w2: [-6.51387364] bias: [16.78994096] loss: 50.413752701825004\nEpoch: 519 / 1000\nw1: [14.77582587] w2: [-6.53070868] bias: [16.78994571] loss: 50.375891494276594\nEpoch: 520 / 1000\nw1: [14.78554573] w2: [-6.54752756] bias: [16.7899503] loss: 50.338102429590926\nEpoch: 521 / 1000\nw1: [14.79525652] w2: [-6.5643303] bias: [16.78995473] loss: 50.30038537027233\nEpoch: 522 / 1000\nw1: [14.80495826] w2: [-6.5811169] bias: [16.78995902] loss: 50.26274017908722\nEpoch: 523 / 1000\nw1: [14.81465095] w2: [-6.59788738] bias: [16.78996314] loss: 50.22516671906353\nEpoch: 524 / 1000\nw1: [14.82433461] w2: [-6.61464177] bias: [16.78996712] loss: 50.187664853490276\nEpoch: 525 / 1000\nw1: [14.83400924] w2: [-6.63138006] bias: [16.78997094] loss: 50.15023444591703\nEpoch: 526 / 1000\nw1: [14.84367484] w2: [-6.64810229] bias: [16.78997461] loss: 50.11287536015345\nEpoch: 527 / 1000\nw1: [14.85333144] w2: [-6.66480846] bias: [16.78997812] loss: 50.075587460268764\nEpoch: 528 / 1000\nw1: [14.86297903] w2: [-6.68149859] bias: [16.78998149] loss: 50.03837061059124\nEpoch: 529 / 1000\nw1: [14.87261763] w2: [-6.69817269] bias: [16.7899847] loss: 50.00122467570775\nEpoch: 530 / 1000\nw1: [14.88224724] w2: [-6.71483078] bias: [16.78998776] loss: 49.964149520463295\nEpoch: 531 / 1000\nw1: [14.89186788] w2: [-6.73147288] bias: [16.78999067] loss: 49.92714500996041\nEpoch: 532 / 1000\nw1: [14.90147955] w2: [-6.748099] bias: [16.78999343] loss: 49.89021100955879\nEpoch: 533 / 1000\nw1: [14.91108226] w2: [-6.76470916] bias: [16.78999604] loss: 49.85334738487471\nEpoch: 534 / 1000\nw1: [14.92067602] w2: [-6.78130336] bias: [16.7899985] loss: 49.81655400178059\nEpoch: 535 / 1000\nw1: [14.93026083] w2: [-6.79788163] bias: [16.79000081] loss: 49.779830726404484\nEpoch: 536 / 1000\nw1: [14.93983671] w2: [-6.81444399] bias: [16.79000296] loss: 49.7431774251296\nEpoch: 537 / 1000\nw1: [14.94940367] w2: [-6.83099043] bias: [16.79000497] loss: 49.70659396459382\nEpoch: 538 / 1000\nw1: [14.95896171] w2: [-6.847521] bias: [16.79000683] loss: 49.6700802116892\nEpoch: 539 / 1000\nw1: [14.96851084] w2: [-6.86403568] bias: [16.79000854] loss: 49.6336360335615\nEpoch: 540 / 1000\nw1: [14.97805107] w2: [-6.88053451] bias: [16.79001011] loss: 49.59726129760968\nEpoch: 541 / 1000\nw1: [14.98758241] w2: [-6.8970175] bias: [16.79001152] loss: 49.56095587148541\nEpoch: 542 / 1000\nw1: [14.99710487] w2: [-6.91348465] bias: [16.79001279] loss: 49.52471962309267\nEpoch: 543 / 1000\nw1: [15.00661845] w2: [-6.929936] bias: [16.7900139] loss: 49.48855242058716\nEpoch: 544 / 1000\nw1: [15.01612317] w2: [-6.94637155] bias: [16.79001487] loss: 49.45245413237587\nEpoch: 545 / 1000\nw1: [15.02561903] w2: [-6.96279131] bias: [16.7900157] loss: 49.41642462711662\nEpoch: 546 / 1000\nw1: [15.03510604] w2: [-6.9791953] bias: [16.79001637] loss: 49.38046377371753\nEpoch: 547 / 1000\nw1: [15.04458421] w2: [-6.99558355] bias: [16.7900169] loss: 49.34457144133661\nEpoch: 548 / 1000\nw1: [15.05405355] w2: [-7.01195605] bias: [16.79001728] loss: 49.30874749938122\nEpoch: 549 / 1000\nw1: [15.06351407] w2: [-7.02831284] bias: [16.79001752] loss: 49.27299181750763\nEpoch: 550 / 1000\nw1: [15.07296577] w2: [-7.04465391] bias: [16.79001761] loss: 49.23730426562055\nEpoch: 551 / 1000\nw1: [15.08240866] w2: [-7.06097929] bias: [16.79001756] loss: 49.20168471387261\nEpoch: 552 / 1000\nw1: [15.09184276] w2: [-7.07728899] bias: [16.79001735] loss: 49.166133032663964\nEpoch: 553 / 1000\nw1: [15.10126806] w2: [-7.09358304] bias: [16.79001701] loss: 49.130649092641754\nEpoch: 554 / 1000\nw1: [15.11068459] w2: [-7.10986143] bias: [16.79001652] loss: 49.095232764699645\nEpoch: 555 / 1000\nw1: [15.12009234] w2: [-7.12612419] bias: [16.79001588] loss: 49.05988391997741\nEpoch: 556 / 1000\nw1: [15.12949133] w2: [-7.14237133] bias: [16.7900151] loss: 49.02460242986039\nEpoch: 557 / 1000\nw1: [15.13888156] w2: [-7.15860287] bias: [16.79001417] loss: 48.98938816597906\nEpoch: 558 / 1000\nw1: [15.14826304] w2: [-7.17481882] bias: [16.7900131] loss: 48.954241000208576\nEpoch: 559 / 1000\nw1: [15.15763578] w2: [-7.19101919] bias: [16.79001189] loss: 48.919160804668294\nEpoch: 560 / 1000\nw1: [15.16699979] w2: [-7.20720401] bias: [16.79001053] loss: 48.88414745172129\nEpoch: 561 / 1000\nw1: [15.17635508] w2: [-7.22337328] bias: [16.79000903] loss: 48.849200813973894\nEpoch: 562 / 1000\nw1: [15.18570166] w2: [-7.23952702] bias: [16.79000738] loss: 48.81432076427528\nEpoch: 563 / 1000\nw1: [15.19503952] w2: [-7.25566525] bias: [16.7900056] loss: 48.77950717571692\nEpoch: 564 / 1000\nw1: [15.20436869] w2: [-7.27178798] bias: [16.79000366] loss: 48.74475992163221\nEpoch: 565 / 1000\nw1: [15.21368917] w2: [-7.28789523] bias: [16.79000159] loss: 48.71007887559594\nEpoch: 566 / 1000\nw1: [15.22300097] w2: [-7.303987] bias: [16.78999937] loss: 48.67546391142385\nEpoch: 567 / 1000\nw1: [15.2323041] w2: [-7.32006332] bias: [16.78999702] loss: 48.640914903172195\nEpoch: 568 / 1000\nw1: [15.24159856] w2: [-7.3361242] bias: [16.78999452] loss: 48.60643172513728\nEpoch: 569 / 1000\nw1: [15.25088436] w2: [-7.35216965] bias: [16.78999187] loss: 48.57201425185497\nEpoch: 570 / 1000\nw1: [15.26016152] w2: [-7.3681997] bias: [16.78998909] loss: 48.53766235810026\nEpoch: 571 / 1000\nw1: [15.26943003] w2: [-7.38421434] bias: [16.78998616] loss: 48.50337591888686\nEpoch: 572 / 1000\nw1: [15.27868991] w2: [-7.40021361] bias: [16.7899831] loss: 48.46915480946662\nEpoch: 573 / 1000\nw1: [15.28794117] w2: [-7.4161975] bias: [16.78997989] loss: 48.434998905329216\nEpoch: 574 / 1000\nw1: [15.29718382] w2: [-7.43216605] bias: [16.78997654] loss: 48.4009080822016\nEpoch: 575 / 1000\nw1: [15.30641786] w2: [-7.44811926] bias: [16.78997305] loss: 48.36688221604762\nEpoch: 576 / 1000\nw1: [15.31564329] w2: [-7.46405714] bias: [16.78996942] loss: 48.332921183067455\nEpoch: 577 / 1000\nw1: [15.32486014] w2: [-7.47997971] bias: [16.78996565] loss: 48.29902485969732\nEpoch: 578 / 1000\nw1: [15.3340684] w2: [-7.495887] bias: [16.78996173] loss: 48.26519312260889\nEpoch: 579 / 1000\nw1: [15.34326809] w2: [-7.511779] bias: [16.78995768] loss: 48.2314258487089\nEpoch: 580 / 1000\nw1: [15.35245922] w2: [-7.52765573] bias: [16.78995349] loss: 48.19772291513872\nEpoch: 581 / 1000\nw1: [15.36164178] w2: [-7.54351722] bias: [16.78994916] loss: 48.16408419927386\nEpoch: 582 / 1000\nw1: [15.3708158] w2: [-7.55936347] bias: [16.78994469] loss: 48.1305095787235\nEpoch: 583 / 1000\nw1: [15.37998127] w2: [-7.5751945] bias: [16.78994008] loss: 48.096998931330184\nEpoch: 584 / 1000\nw1: [15.38913821] w2: [-7.59101033] bias: [16.78993534] loss: 48.06355213516922\nEpoch: 585 / 1000\nw1: [15.39828663] w2: [-7.60681096] bias: [16.78993045] loss: 48.030169068548304\nEpoch: 586 / 1000\nw1: [15.40742652] w2: [-7.62259641] bias: [16.78992542] loss: 47.996849610007075\nEpoch: 587 / 1000\nw1: [15.41655791] w2: [-7.6383667] bias: [16.78992026] loss: 47.96359363831666\nEpoch: 588 / 1000\nw1: [15.42568079] w2: [-7.65412184] bias: [16.78991496] loss: 47.930401032479274\nEpoch: 589 / 1000\nw1: [15.43479519] w2: [-7.66986185] bias: [16.78990952] loss: 47.8972716717277\nEpoch: 590 / 1000\nw1: [15.44390109] w2: [-7.68558674] bias: [16.78990394] loss: 47.86420543552492\nEpoch: 591 / 1000\nw1: [15.45299852] w2: [-7.70129652] bias: [16.78989822] loss: 47.83120220356366\nEpoch: 592 / 1000\nw1: [15.46208749] w2: [-7.71699121] bias: [16.78989237] loss: 47.79826185576592\nEpoch: 593 / 1000\nw1: [15.47116799] w2: [-7.73267083] bias: [16.78988638] loss: 47.76538427228258\nEpoch: 594 / 1000\nw1: [15.48024003] w2: [-7.74833538] bias: [16.78988025] loss: 47.73256933349295\nEpoch: 595 / 1000\nw1: [15.48930364] w2: [-7.76398489] bias: [16.78987398] loss: 47.6998169200043\nEpoch: 596 / 1000\nw1: [15.4983588] w2: [-7.77961936] bias: [16.78986758] loss: 47.6671269126515\nEpoch: 597 / 1000\nw1: [15.50740554] w2: [-7.79523881] bias: [16.78986104] loss: 47.634499192496484\nEpoch: 598 / 1000\nw1: [15.51644386] w2: [-7.81084326] bias: [16.78985436] loss: 47.60193364082794\nEpoch: 599 / 1000\nw1: [15.52547376] w2: [-7.82643272] bias: [16.78984755] loss: 47.569430139160765\nEpoch: 600 / 1000\nw1: [15.53449526] w2: [-7.8420072] bias: [16.7898406] loss: 47.53698856923569\nEpoch: 601 / 1000\nw1: [15.54350836] w2: [-7.85756672] bias: [16.78983352] loss: 47.504608813018855\nEpoch: 602 / 1000\nw1: [15.55251307] w2: [-7.87311129] bias: [16.7898263] loss: 47.47229075270136\nEpoch: 603 / 1000\nw1: [15.56150941] w2: [-7.88864092] bias: [16.78981894] loss: 47.44003427069883\nEpoch: 604 / 1000\nw1: [15.57049737] w2: [-7.90415564] bias: [16.78981145] loss: 47.40783924965102\nEpoch: 605 / 1000\nw1: [15.57947696] w2: [-7.91965546] bias: [16.78980382] loss: 47.37570557242133\nEpoch: 606 / 1000\nw1: [15.5884482] w2: [-7.93514038] bias: [16.78979606] loss: 47.34363312209646\nEpoch: 607 / 1000\nw1: [15.59741109] w2: [-7.95061043] bias: [16.78978816] loss: 47.31162178198593\nEpoch: 608 / 1000\nw1: [15.60636564] w2: [-7.96606561] bias: [16.78978013] loss: 47.2796714356216\nEpoch: 609 / 1000\nw1: [15.61531185] w2: [-7.98150595] bias: [16.78977196] loss: 47.24778196675744\nEpoch: 610 / 1000\nw1: [15.62424974] w2: [-7.99693145] bias: [16.78976366] loss: 47.21595325936883\nEpoch: 611 / 1000\nw1: [15.63317932] w2: [-8.01234214] bias: [16.78975523] loss: 47.184185197652404\nEpoch: 612 / 1000\nw1: [15.64210058] w2: [-8.02773801] bias: [16.78974665] loss: 47.152477666025455\nEpoch: 613 / 1000\nw1: [15.65101354] w2: [-8.0431191] bias: [16.78973795] loss: 47.120830549125564\nEpoch: 614 / 1000\nw1: [15.65991821] w2: [-8.05848541] bias: [16.78972911] loss: 47.08924373181021\nEpoch: 615 / 1000\nw1: [15.66881459] w2: [-8.07383695] bias: [16.78972014] loss: 47.057717099156314\nEpoch: 616 / 1000\nw1: [15.6777027] w2: [-8.08917375] bias: [16.78971103] loss: 47.026250536459834\nEpoch: 617 / 1000\nw1: [15.68658253] w2: [-8.10449581] bias: [16.7897018] loss: 46.994843929235365\nEpoch: 618 / 1000\nw1: [15.69545411] w2: [-8.11980315] bias: [16.78969242] loss: 46.96349716321567\nEpoch: 619 / 1000\nw1: [15.70431743] w2: [-8.13509578] bias: [16.78968292] loss: 46.93221012435132\nEpoch: 620 / 1000\nw1: [15.7131725] w2: [-8.15037372] bias: [16.78967328] loss: 46.90098269881025\nEpoch: 621 / 1000\nw1: [15.72201933] w2: [-8.16563699] bias: [16.78966351] loss: 46.86981477297738\nEpoch: 622 / 1000\nw1: [15.73085794] w2: [-8.18088558] bias: [16.7896536] loss: 46.83870623345412\nEpoch: 623 / 1000\nw1: [15.73968832] w2: [-8.19611953] bias: [16.78964357] loss: 46.80765696705802\nEpoch: 624 / 1000\nw1: [15.74851049] w2: [-8.21133884] bias: [16.7896334] loss: 46.776666860822424\nEpoch: 625 / 1000\nw1: [15.75732445] w2: [-8.22654352] bias: [16.7896231] loss: 46.7457358019959\nEpoch: 626 / 1000\nw1: [15.76613021] w2: [-8.2417336] bias: [16.78961266] loss: 46.71486367804193\nEpoch: 627 / 1000\nw1: [15.77492777] w2: [-8.25690908] bias: [16.7896021] loss: 46.68405037663851\nEpoch: 628 / 1000\nw1: [15.78371716] w2: [-8.27206998] bias: [16.7895914] loss: 46.65329578567767\nEpoch: 629 / 1000\nw1: [15.79249837] w2: [-8.28721631] bias: [16.78958057] loss: 46.62259979326519\nEpoch: 630 / 1000\nw1: [15.80127141] w2: [-8.30234809] bias: [16.78956961] loss: 46.59196228772003\nEpoch: 631 / 1000\nw1: [15.81003629] w2: [-8.31746533] bias: [16.78955852] loss: 46.561383157574035\nEpoch: 632 / 1000\nw1: [15.81879301] w2: [-8.33256804] bias: [16.7895473] loss: 46.53086229157153\nEpoch: 633 / 1000\nw1: [15.8275416] w2: [-8.34765625] bias: [16.78953595] loss: 46.50039957866884\nEpoch: 634 / 1000\nw1: [15.83628204] w2: [-8.36272995] bias: [16.78952446] loss: 46.46999490803397\nEpoch: 635 / 1000\nw1: [15.84501436] w2: [-8.37778917] bias: [16.78951285] loss: 46.439648169046144\nEpoch: 636 / 1000\nw1: [15.85373855] w2: [-8.39283392] bias: [16.78950111] loss: 46.40935925129544\nEpoch: 637 / 1000\nw1: [15.86245463] w2: [-8.40786422] bias: [16.78948923] loss: 46.37912804458234\nEpoch: 638 / 1000\nw1: [15.8711626] w2: [-8.42288007] bias: [16.78947722] loss: 46.348954438917396\nEpoch: 639 / 1000\nw1: [15.87986248] w2: [-8.43788149] bias: [16.78946509] loss: 46.31883832452074\nEpoch: 640 / 1000\nw1: [15.88855426] w2: [-8.4528685] bias: [16.78945282] loss: 46.28877959182181\nEpoch: 641 / 1000\nw1: [15.89723796] w2: [-8.4678411] bias: [16.78944043] loss: 46.258778131458826\nEpoch: 642 / 1000\nw1: [15.90591359] w2: [-8.48279932] bias: [16.7894279] loss: 46.228833834278454\nEpoch: 643 / 1000\nw1: [15.91458114] w2: [-8.49774317] bias: [16.78941525] loss: 46.19894659133542\nEpoch: 644 / 1000\nw1: [15.92324064] w2: [-8.51267265] bias: [16.78940247] loss: 46.16911629389206\nEpoch: 645 / 1000\nw1: [15.93189208] w2: [-8.52758779] bias: [16.78938955] loss: 46.139342833418\nEpoch: 646 / 1000\nw1: [15.94053548] w2: [-8.5424886] bias: [16.78937651] loss: 46.109626101589654\nEpoch: 647 / 1000\nw1: [15.94917085] w2: [-8.55737508] bias: [16.78936334] loss: 46.07996599028998\nEpoch: 648 / 1000\nw1: [15.95779818] w2: [-8.57224726] bias: [16.78935004] loss: 46.05036239160791\nEpoch: 649 / 1000\nw1: [15.96641749] w2: [-8.58710515] bias: [16.78933661] loss: 46.020815197838125\nEpoch: 650 / 1000\nw1: [15.97502879] w2: [-8.60194876] bias: [16.78932306] loss: 45.99132430148054\nEpoch: 651 / 1000\nw1: [15.98363208] w2: [-8.61677811] bias: [16.78930937] loss: 45.961889595239974\nEpoch: 652 / 1000\nw1: [15.99222737] w2: [-8.63159321] bias: [16.78929556] loss: 45.93251097202571\nEpoch: 653 / 1000\nw1: [16.00081466] w2: [-8.64639407] bias: [16.78928162] loss: 45.90318832495122\nEpoch: 654 / 1000\nw1: [16.00939398] w2: [-8.6611807] bias: [16.78926755] loss: 45.873921547333616\nEpoch: 655 / 1000\nw1: [16.01796532] w2: [-8.67595313] bias: [16.78925335] loss: 45.84471053269337\nEpoch: 656 / 1000\nw1: [16.02652869] w2: [-8.69071135] bias: [16.78923903] loss: 45.8155551747539\nEpoch: 657 / 1000\nw1: [16.0350841] w2: [-8.7054554] bias: [16.78922458] loss: 45.78645536744118\nEpoch: 658 / 1000\nw1: [16.04363155] w2: [-8.72018528] bias: [16.78921] loss: 45.75741100488334\nEpoch: 659 / 1000\nw1: [16.05217106] w2: [-8.734901] bias: [16.78919529] loss: 45.72842198141032\nEpoch: 660 / 1000\nw1: [16.06070263] w2: [-8.74960257] bias: [16.78918046] loss: 45.69948819155344\nEpoch: 661 / 1000\nw1: [16.06922627] w2: [-8.76429002] bias: [16.7891655] loss: 45.67060953004505\nEpoch: 662 / 1000\nw1: [16.07774199] w2: [-8.77896336] bias: [16.78915042] loss: 45.641785891818124\nEpoch: 663 / 1000\nw1: [16.08624979] w2: [-8.79362259] bias: [16.7891352] loss: 45.61301717200588\nEpoch: 664 / 1000\nw1: [16.09474968] w2: [-8.80826773] bias: [16.78911986] loss: 45.58430326594144\nEpoch: 665 / 1000\nw1: [16.10324167] w2: [-8.8228988] bias: [16.7891044] loss: 45.55564406915739\nEpoch: 666 / 1000\nw1: [16.11172577] w2: [-8.8375158] bias: [16.78908881] loss: 45.527039477385394\nEpoch: 667 / 1000\nw1: [16.12020198] w2: [-8.85211876] bias: [16.78907309] loss: 45.4984893865559\nEpoch: 668 / 1000\nw1: [16.12867031] w2: [-8.86670769] bias: [16.78905725] loss: 45.46999369279768\nEpoch: 669 / 1000\nw1: [16.13713078] w2: [-8.88128259] bias: [16.78904128] loss: 45.44155229243747\nEpoch: 670 / 1000\nw1: [16.14558338] w2: [-8.89584348] bias: [16.78902518] loss: 45.41316508199962\nEpoch: 671 / 1000\nw1: [16.15402812] w2: [-8.91039038] bias: [16.78900896] loss: 45.384831958205694\nEpoch: 672 / 1000\nw1: [16.16246501] w2: [-8.9249233] bias: [16.78899262] loss: 45.35655281797409\nEpoch: 673 / 1000\nw1: [16.17089407] w2: [-8.93944225] bias: [16.78897615] loss: 45.328327558419666\nEpoch: 674 / 1000\nw1: [16.17931529] w2: [-8.95394724] bias: [16.78895955] loss: 45.30015607685338\nEpoch: 675 / 1000\nw1: [16.18772868] w2: [-8.9684383] bias: [16.78894283] loss: 45.272038270781934\nEpoch: 676 / 1000\nw1: [16.19613426] w2: [-8.98291542] bias: [16.78892599] loss: 45.243974037907336\nEpoch: 677 / 1000\nw1: [16.20453202] w2: [-8.99737864] bias: [16.78890902] loss: 45.21596327612659\nEpoch: 678 / 1000\nw1: [16.21292198] w2: [-9.01182795] bias: [16.78889193] loss: 45.18800588353128\nEpoch: 679 / 1000\nw1: [16.22130415] w2: [-9.02626337] bias: [16.78887471] loss: 45.160101758407265\nEpoch: 680 / 1000\nw1: [16.22967852] w2: [-9.04068492] bias: [16.78885736] loss: 45.13225079923422\nEpoch: 681 / 1000\nw1: [16.23804512] w2: [-9.0550926] bias: [16.7888399] loss: 45.10445290468534\nEpoch: 682 / 1000\nw1: [16.24640393] w2: [-9.06948644] bias: [16.78882231] loss: 45.07670797362692\nEpoch: 683 / 1000\nw1: [16.25475499] w2: [-9.08386644] bias: [16.78880459] loss: 45.04901590511803\nEpoch: 684 / 1000\nw1: [16.26309828] w2: [-9.09823262] bias: [16.78878676] loss: 45.02137659841011\nEpoch: 685 / 1000\nw1: [16.27143382] w2: [-9.11258499] bias: [16.7887688] loss: 44.99378995294663\nEpoch: 686 / 1000\nw1: [16.27976162] w2: [-9.12692357] bias: [16.78875071] loss: 44.966255868362715\nEpoch: 687 / 1000\nw1: [16.28808168] w2: [-9.14124836] bias: [16.78873251] loss: 44.938774244484776\nEpoch: 688 / 1000\nw1: [16.296394] w2: [-9.15555938] bias: [16.78871417] loss: 44.91134498133016\nEpoch: 689 / 1000\nw1: [16.30469861] w2: [-9.16985665] bias: [16.78869572] loss: 44.883967979106735\nEpoch: 690 / 1000\nw1: [16.3129955] w2: [-9.18414018] bias: [16.78867715] loss: 44.856643138212625\nEpoch: 691 / 1000\nw1: [16.32128468] w2: [-9.19840997] bias: [16.78865845] loss: 44.829370359235746\nEpoch: 692 / 1000\nw1: [16.32956617] w2: [-9.21266605] bias: [16.78863963] loss: 44.802149542953494\nEpoch: 693 / 1000\nw1: [16.33783996] w2: [-9.22690842] bias: [16.78862068] loss: 44.7749805903324\nEpoch: 694 / 1000\nw1: [16.34610606] w2: [-9.24113711] bias: [16.78860162] loss: 44.74786340252771\nEpoch: 695 / 1000\nw1: [16.35436448] w2: [-9.25535211] bias: [16.78858243] loss: 44.72079788088309\nEpoch: 696 / 1000\nw1: [16.36261523] w2: [-9.26955346] bias: [16.78856312] loss: 44.693783926930244\nEpoch: 697 / 1000\nw1: [16.37085832] w2: [-9.28374115] bias: [16.78854369] loss: 44.66682144238852\nEpoch: 698 / 1000\nw1: [16.37909375] w2: [-9.2979152] bias: [16.78852414] loss: 44.63991032916459\nEpoch: 699 / 1000\nw1: [16.38732154] w2: [-9.31207562] bias: [16.78850446] loss: 44.613050489352126\nEpoch: 700 / 1000\nw1: [16.39554168] w2: [-9.32622244] bias: [16.78848466] loss: 44.586241825231355\nEpoch: 701 / 1000\nw1: [16.40375418] w2: [-9.34035565] bias: [16.78846475] loss: 44.55948423926876\nEpoch: 702 / 1000\nw1: [16.41195906] w2: [-9.35447528] bias: [16.78844471] loss: 44.53277763411674\nEpoch: 703 / 1000\nw1: [16.42015631] w2: [-9.36858134] bias: [16.78842455] loss: 44.50612191261321\nEpoch: 704 / 1000\nw1: [16.42834596] w2: [-9.38267383] bias: [16.78840427] loss: 44.47951697778129\nEpoch: 705 / 1000\nw1: [16.43652799] w2: [-9.39675278] bias: [16.78838387] loss: 44.452962732828894\nEpoch: 706 / 1000\nw1: [16.44470243] w2: [-9.41081819] bias: [16.78836335] loss: 44.426459081148465\nEpoch: 707 / 1000\nw1: [16.45286928] w2: [-9.42487008] bias: [16.78834271] loss: 44.400005926316545\nEpoch: 708 / 1000\nw1: [16.46102854] w2: [-9.43890846] bias: [16.78832194] loss: 44.37360317209344\nEpoch: 709 / 1000\nw1: [16.46918022] w2: [-9.45293335] bias: [16.78830106] loss: 44.34725072242291\nEpoch: 710 / 1000\nw1: [16.47732434] w2: [-9.46694475] bias: [16.78828006] loss: 44.320948481431806\nEpoch: 711 / 1000\nw1: [16.48546089] w2: [-9.48094268] bias: [16.78825894] loss: 44.29469635342965\nEpoch: 712 / 1000\nw1: [16.49358988] w2: [-9.49492716] bias: [16.78823769] loss: 44.2684942429084\nEpoch: 713 / 1000\nw1: [16.50171133] w2: [-9.50889819] bias: [16.78821633] loss: 44.24234205454201\nEpoch: 714 / 1000\nw1: [16.50982524] w2: [-9.52285579] bias: [16.78819485] loss: 44.21623969318615\nEpoch: 715 / 1000\nw1: [16.51793161] w2: [-9.53679997] bias: [16.78817325] loss: 44.19018706387779\nEpoch: 716 / 1000\nw1: [16.52603045] w2: [-9.55073074] bias: [16.78815153] loss: 44.16418407183493\nEpoch: 717 / 1000\nw1: [16.53412178] w2: [-9.56464812] bias: [16.78812969] loss: 44.1382306224562\nEpoch: 718 / 1000\nw1: [16.54220559] w2: [-9.57855212] bias: [16.78810773] loss: 44.11232662132054\nEpoch: 719 / 1000\nw1: [16.5502819] w2: [-9.59244276] bias: [16.78808566] loss: 44.08647197418686\nEpoch: 720 / 1000\nw1: [16.55835071] w2: [-9.60632003] bias: [16.78806346] loss: 44.060666586993634\nEpoch: 721 / 1000\nw1: [16.56641202] w2: [-9.62018397] bias: [16.78804115] loss: 44.034910365858714\nEpoch: 722 / 1000\nw1: [16.57446586] w2: [-9.63403458] bias: [16.78801871] loss: 44.009203217078785\nEpoch: 723 / 1000\nw1: [16.58251221] w2: [-9.64787187] bias: [16.78799616] loss: 43.98354504712917\nEpoch: 724 / 1000\nw1: [16.5905511] w2: [-9.66169585] bias: [16.78797349] loss: 43.95793576266344\nEpoch: 725 / 1000\nw1: [16.59858252] w2: [-9.67550655] bias: [16.78795071] loss: 43.93237527051309\nEpoch: 726 / 1000\nw1: [16.60660649] w2: [-9.68930396] bias: [16.7879278] loss: 43.90686347768717\nEpoch: 727 / 1000\nw1: [16.614623] w2: [-9.70308811] bias: [16.78790478] loss: 43.88140029137195\nEpoch: 728 / 1000\nw1: [16.62263208] w2: [-9.71685901] bias: [16.78788164] loss: 43.85598561893064\nEpoch: 729 / 1000\nw1: [16.63063372] w2: [-9.73061667] bias: [16.78785838] loss: 43.83061936790297\nEpoch: 730 / 1000\nw1: [16.63862794] w2: [-9.7443611] bias: [16.78783501] loss: 43.80530144600491\nEpoch: 731 / 1000\nw1: [16.64661473] w2: [-9.75809231] bias: [16.78781151] loss: 43.78003176112834\nEpoch: 732 / 1000\nw1: [16.65459411] w2: [-9.77181032] bias: [16.7877879] loss: 43.75481022134065\nEpoch: 733 / 1000\nw1: [16.66256609] w2: [-9.78551514] bias: [16.78776418] loss: 43.72963673488447\nEpoch: 734 / 1000\nw1: [16.67053066] w2: [-9.79920678] bias: [16.78774033] loss: 43.704511210177316\nEpoch: 735 / 1000\nw1: [16.67848784] w2: [-9.81288526] bias: [16.78771637] loss: 43.67943355581124\nEpoch: 736 / 1000\nw1: [16.68643764] w2: [-9.82655059] bias: [16.7876923] loss: 43.654403680552534\nEpoch: 737 / 1000\nw1: [16.69438006] w2: [-9.84020277] bias: [16.7876681] loss: 43.62942149334134\nEpoch: 738 / 1000\nw1: [16.70231511] w2: [-9.85384183] bias: [16.78764379] loss: 43.6044869032914\nEpoch: 739 / 1000\nw1: [16.71024279] w2: [-9.86746778] bias: [16.78761937] loss: 43.57959981968965\nEpoch: 740 / 1000\nw1: [16.71816311] w2: [-9.88108062] bias: [16.78759483] loss: 43.554760151995914\nEpoch: 741 / 1000\nw1: [16.72607609] w2: [-9.89468037] bias: [16.78757017] loss: 43.529967809842596\nEpoch: 742 / 1000\nw1: [16.73398172] w2: [-9.90826704] bias: [16.7875454] loss: 43.50522270303434\nEpoch: 743 / 1000\nw1: [16.74188002] w2: [-9.92184065] bias: [16.78752051] loss: 43.48052474154769\nEpoch: 744 / 1000\nw1: [16.74977099] w2: [-9.93540121] bias: [16.7874955] loss: 43.45587383553073\nEpoch: 745 / 1000\nw1: [16.75765463] w2: [-9.94894873] bias: [16.78747038] loss: 43.431269895302876\nEpoch: 746 / 1000\nw1: [16.76553096] w2: [-9.96248322] bias: [16.78744515] loss: 43.4067128313544\nEpoch: 747 / 1000\nw1: [16.77339998] w2: [-9.9760047] bias: [16.78741979] loss: 43.382202554346186\nEpoch: 748 / 1000\nw1: [16.7812617] w2: [-9.98951317] bias: [16.78739433] loss: 43.357738975109434\nEpoch: 749 / 1000\nw1: [16.78911612] w2: [-10.00300865] bias: [16.78736875] loss: 43.333322004645254\nEpoch: 750 / 1000\nw1: [16.79696326] w2: [-10.01649116] bias: [16.78734305] loss: 43.308951554124384\nEpoch: 751 / 1000\nw1: [16.80480311] w2: [-10.0299607] bias: [16.78731724] loss: 43.28462753488688\nEpoch: 752 / 1000\nw1: [16.81263569] w2: [-10.04341728] bias: [16.78729132] loss: 43.26034985844177\nEpoch: 753 / 1000\nw1: [16.82046101] w2: [-10.05686093] bias: [16.78726528] loss: 43.23611843646675\nEpoch: 754 / 1000\nw1: [16.82827906] w2: [-10.07029165] bias: [16.78723912] loss: 43.21193318080783\nEpoch: 755 / 1000\nw1: [16.83608987] w2: [-10.08370945] bias: [16.78721286] loss: 43.18779400347907\nEpoch: 756 / 1000\nw1: [16.84389342] w2: [-10.09711435] bias: [16.78718648] loss: 43.1637008166622\nEpoch: 757 / 1000\nw1: [16.85168974] w2: [-10.11050636] bias: [16.78715998] loss: 43.13965353270631\nEpoch: 758 / 1000\nw1: [16.85947882] w2: [-10.12388549] bias: [16.78713337] loss: 43.115652064127595\nEpoch: 759 / 1000\nw1: [16.86726068] w2: [-10.13725175] bias: [16.78710665] loss: 43.09169632360895\nEpoch: 760 / 1000\nw1: [16.87503532] w2: [-10.15060516] bias: [16.78707981] loss: 43.067786223999704\nEpoch: 761 / 1000\nw1: [16.88280274] w2: [-10.16394572] bias: [16.78705286] loss: 43.04392167831529\nEpoch: 762 / 1000\nw1: [16.89056297] w2: [-10.17727346] bias: [16.7870258] loss: 43.02010259973694\nEpoch: 763 / 1000\nw1: [16.89831599] w2: [-10.19058838] bias: [16.78699862] loss: 42.996328901611314\nEpoch: 764 / 1000\nw1: [16.90606182] w2: [-10.20389049] bias: [16.78697133] loss: 42.97260049745028\nEpoch: 765 / 1000\nw1: [16.91380047] w2: [-10.21717981] bias: [16.78694393] loss: 42.94891730093052\nEpoch: 766 / 1000\nw1: [16.92153194] w2: [-10.23045635] bias: [16.78691642] loss: 42.925279225893235\nEpoch: 767 / 1000\nw1: [16.92925624] w2: [-10.24372012] bias: [16.78688879] loss: 42.901686186343866\nEpoch: 768 / 1000\nw1: [16.93697337] w2: [-10.25697114] bias: [16.78686105] loss: 42.87813809645173\nEpoch: 769 / 1000\nw1: [16.94468335] w2: [-10.2702094] bias: [16.7868332] loss: 42.85463487054973\nEpoch: 770 / 1000\nw1: [16.95238618] w2: [-10.28343494] bias: [16.78680523] loss: 42.83117642313408\nEpoch: 771 / 1000\nw1: [16.96008186] w2: [-10.29664776] bias: [16.78677716] loss: 42.807762668863916\nEpoch: 772 / 1000\nw1: [16.96777041] w2: [-10.30984787] bias: [16.78674897] loss: 42.78439352256105\nEpoch: 773 / 1000\nw1: [16.97545182] w2: [-10.32303528] bias: [16.78672067] loss: 42.76106889920961\nEpoch: 774 / 1000\nw1: [16.98312612] w2: [-10.33621001] bias: [16.78669226] loss: 42.737788713955844\nEpoch: 775 / 1000\nw1: [16.99079329] w2: [-10.34937207] bias: [16.78666373] loss: 42.714552882107604\nEpoch: 776 / 1000\nw1: [16.99845336] w2: [-10.36252147] bias: [16.7866351] loss: 42.69136131913426\nEpoch: 777 / 1000\nw1: [17.00610632] w2: [-10.37565822] bias: [16.78660635] loss: 42.66821394066621\nEpoch: 778 / 1000\nw1: [17.01375219] w2: [-10.38878234] bias: [16.78657749] loss: 42.64511066249474\nEpoch: 779 / 1000\nw1: [17.02139097] w2: [-10.40189383] bias: [16.78654852] loss: 42.62205140057156\nEpoch: 780 / 1000\nw1: [17.02902266] w2: [-10.41499272] bias: [16.78651944] loss: 42.59903607100862\nEpoch: 781 / 1000\nw1: [17.03664728] w2: [-10.428079] bias: [16.78649025] loss: 42.57606459007772\nEpoch: 782 / 1000\nw1: [17.04426483] w2: [-10.4411527] bias: [16.78646095] loss: 42.55313687421027\nEpoch: 783 / 1000\nw1: [17.05187531] w2: [-10.45421382] bias: [16.78643154] loss: 42.53025283999691\nEpoch: 784 / 1000\nw1: [17.05947874] w2: [-10.46726238] bias: [16.78640202] loss: 42.50741240418732\nEpoch: 785 / 1000\nw1: [17.06707512] w2: [-10.48029838] bias: [16.78637238] loss: 42.4846154836898\nEpoch: 786 / 1000\nw1: [17.07466446] w2: [-10.49332185] bias: [16.78634264] loss: 42.46186199557103\nEpoch: 787 / 1000\nw1: [17.08224676] w2: [-10.50633279] bias: [16.78631279] loss: 42.439151857055755\nEpoch: 788 / 1000\nw1: [17.08982204] w2: [-10.51933122] bias: [16.78628282] loss: 42.41648498552649\nEpoch: 789 / 1000\nw1: [17.09739029] w2: [-10.53231714] bias: [16.78625275] loss: 42.393861298523206\nEpoch: 790 / 1000\nw1: [17.10495152] w2: [-10.54529057] bias: [16.78622257] loss: 42.37128071374305\nEpoch: 791 / 1000\nw1: [17.11250574] w2: [-10.55825152] bias: [16.78619227] loss: 42.34874314904001\nEpoch: 792 / 1000\nw1: [17.12005297] w2: [-10.57120001] bias: [16.78616187] loss: 42.32624852242467\nEpoch: 793 / 1000\nw1: [17.12759319] w2: [-10.58413604] bias: [16.78613136] loss: 42.303796752063846\nEpoch: 794 / 1000\nw1: [17.13512643] w2: [-10.59705962] bias: [16.78610074] loss: 42.28138775628035\nEpoch: 795 / 1000\nw1: [17.14265268] w2: [-10.60997078] bias: [16.78607001] loss: 42.259021453552656\nEpoch: 796 / 1000\nw1: [17.15017196] w2: [-10.62286951] bias: [16.78603917] loss: 42.236697762514595\nEpoch: 797 / 1000\nw1: [17.15768427] w2: [-10.63575584] bias: [16.78600822] loss: 42.214416601955115\nEpoch: 798 / 1000\nw1: [17.16518962] w2: [-10.64862977] bias: [16.78597716] loss: 42.192177890817916\nEpoch: 799 / 1000\nw1: [17.17268801] w2: [-10.66149131] bias: [16.785946] loss: 42.1699815482012\nEpoch: 800 / 1000\nw1: [17.18017945] w2: [-10.67434049] bias: [16.78591473] loss: 42.14782749335735\nEpoch: 801 / 1000\nw1: [17.18766394] w2: [-10.6871773] bias: [16.78588334] loss: 42.12571564569266\nEpoch: 802 / 1000\nw1: [17.1951415] w2: [-10.70000176] bias: [16.78585185] loss: 42.103645924767015\nEpoch: 803 / 1000\nw1: [17.20261214] w2: [-10.71281389] bias: [16.78582025] loss: 42.08161825029365\nEpoch: 804 / 1000\nw1: [17.21007584] w2: [-10.72561369] bias: [16.78578855] loss: 42.05963254213878\nEpoch: 805 / 1000\nw1: [17.21753263] w2: [-10.73840117] bias: [16.78575673] loss: 42.03768872032138\nEpoch: 806 / 1000\nw1: [17.22498252] w2: [-10.75117636] bias: [16.78572481] loss: 42.01578670501283\nEpoch: 807 / 1000\nw1: [17.23242549] w2: [-10.76393925] bias: [16.78569278] loss: 41.99392641653668\nEpoch: 808 / 1000\nw1: [17.23986157] w2: [-10.77668987] bias: [16.78566064] loss: 41.97210777536835\nEpoch: 809 / 1000\nw1: [17.24729076] w2: [-10.78942822] bias: [16.7856284] loss: 41.950330702134806\nEpoch: 810 / 1000\nw1: [17.25471307] w2: [-10.80215432] bias: [16.78559605] loss: 41.9285951176143\nEpoch: 811 / 1000\nw1: [17.2621285] w2: [-10.81486817] bias: [16.78556359] loss: 41.90690094273609\nEpoch: 812 / 1000\nw1: [17.26953706] w2: [-10.82756979] bias: [16.78553102] loss: 41.885248098580114\nEpoch: 813 / 1000\nw1: [17.27693875] w2: [-10.84025919] bias: [16.78549835] loss: 41.863636506376736\nEpoch: 814 / 1000\nw1: [17.28433359] w2: [-10.85293638] bias: [16.78546557] loss: 41.84206608750647\nEpoch: 815 / 1000\nw1: [17.29172157] w2: [-10.86560138] bias: [16.78543268] loss: 41.82053676349962\nEpoch: 816 / 1000\nw1: [17.29910271] w2: [-10.87825419] bias: [16.78539968] loss: 41.79904845603611\nEpoch: 817 / 1000\nw1: [17.30647701] w2: [-10.89089483] bias: [16.78536658] loss: 41.77760108694512\nEpoch: 818 / 1000\nw1: [17.31384448] w2: [-10.9035233] bias: [16.78533338] loss: 41.75619457820477\nEpoch: 819 / 1000\nw1: [17.32120513] w2: [-10.91613963] bias: [16.78530007] loss: 41.734828851941955\nEpoch: 820 / 1000\nw1: [17.32855895] w2: [-10.92874381] bias: [16.78526665] loss: 41.713503830431954\nEpoch: 821 / 1000\nw1: [17.33590597] w2: [-10.94133588] bias: [16.78523312] loss: 41.692219436098185\nEpoch: 822 / 1000\nw1: [17.34324618] w2: [-10.95391582] bias: [16.78519949] loss: 41.670975591511905\nEpoch: 823 / 1000\nw1: [17.35057958] w2: [-10.96648366] bias: [16.78516575] loss: 41.64977221939199\nEpoch: 824 / 1000\nw1: [17.3579062] w2: [-10.97903941] bias: [16.78513191] loss: 41.62860924260458\nEpoch: 825 / 1000\nw1: [17.36522603] w2: [-10.99158308] bias: [16.78509797] loss: 41.607486584162835\nEpoch: 826 / 1000\nw1: [17.37253908] w2: [-11.00411468] bias: [16.78506391] loss: 41.58640416722664\nEpoch: 827 / 1000\nw1: [17.37984535] w2: [-11.01663422] bias: [16.78502975] loss: 41.56536191510232\nEpoch: 828 / 1000\nw1: [17.38714486] w2: [-11.02914172] bias: [16.78499549] loss: 41.54435975124242\nEpoch: 829 / 1000\nw1: [17.39443761] w2: [-11.04163718] bias: [16.78496112] loss: 41.52339759924532\nEpoch: 830 / 1000\nw1: [17.4017236] w2: [-11.05412062] bias: [16.78492665] loss: 41.50247538285505\nEpoch: 831 / 1000\nw1: [17.40900285] w2: [-11.06659205] bias: [16.78489207] loss: 41.48159302596099\nEpoch: 832 / 1000\nw1: [17.41627535] w2: [-11.07905147] bias: [16.78485739] loss: 41.46075045259755\nEpoch: 833 / 1000\nw1: [17.42354112] w2: [-11.09149891] bias: [16.7848226] loss: 41.43994758694394\nEpoch: 834 / 1000\nw1: [17.43080016] w2: [-11.10393437] bias: [16.78478771] loss: 41.41918435332387\nEpoch: 835 / 1000\nw1: [17.43805248] w2: [-11.11635787] bias: [16.78475271] loss: 41.398460676205325\nEpoch: 836 / 1000\nw1: [17.44529808] w2: [-11.12876941] bias: [16.78471761] loss: 41.37777648020019\nEpoch: 837 / 1000\nw1: [17.45253697] w2: [-11.14116901] bias: [16.78468241] loss: 41.35713169006405\nEpoch: 838 / 1000\nw1: [17.45976916] w2: [-11.15355667] bias: [16.7846471] loss: 41.33652623069595\nEpoch: 839 / 1000\nw1: [17.46699465] w2: [-11.16593242] bias: [16.78461169] loss: 41.31596002713801\nEpoch: 840 / 1000\nw1: [17.47421345] w2: [-11.17829626] bias: [16.78457617] loss: 41.29543300457525\nEpoch: 841 / 1000\nw1: [17.48142557] w2: [-11.1906482] bias: [16.78454055] loss: 41.274945088335265\nEpoch: 842 / 1000\nw1: [17.48863101] w2: [-11.20298825] bias: [16.78450483] loss: 41.254496203887996\nEpoch: 843 / 1000\nw1: [17.49582978] w2: [-11.21531643] bias: [16.784469] loss: 41.23408627684538\nEpoch: 844 / 1000\nw1: [17.50302188] w2: [-11.22763275] bias: [16.78443307] loss: 41.21371523296121\nEpoch: 845 / 1000\nw1: [17.51020732] w2: [-11.23993721] bias: [16.78439704] loss: 41.19338299813071\nEpoch: 846 / 1000\nw1: [17.51738612] w2: [-11.25222983] bias: [16.78436091] loss: 41.1730894983904\nEpoch: 847 / 1000\nw1: [17.52455826] w2: [-11.26451063] bias: [16.78432467] loss: 41.15283465991774\nEpoch: 848 / 1000\nw1: [17.53172377] w2: [-11.2767796] bias: [16.78428833] loss: 41.13261840903089\nEpoch: 849 / 1000\nw1: [17.53888264] w2: [-11.28903677] bias: [16.78425188] loss: 41.11244067218846\nEpoch: 850 / 1000\nw1: [17.54603489] w2: [-11.30128215] bias: [16.78421533] loss: 41.09230137598923\nEpoch: 851 / 1000\nw1: [17.55318051] w2: [-11.31351573] bias: [16.78417869] loss: 41.07220044717183\nEpoch: 852 / 1000\nw1: [17.56031952] w2: [-11.32573755] bias: [16.78414193] loss: 41.05213781261458\nEpoch: 853 / 1000\nw1: [17.56745192] w2: [-11.3379476] bias: [16.78410508] loss: 41.03211339933514\nEpoch: 854 / 1000\nw1: [17.57457772] w2: [-11.35014591] bias: [16.78406812] loss: 41.01212713449024\nEpoch: 855 / 1000\nw1: [17.58169692] w2: [-11.36233247] bias: [16.78403107] loss: 40.992178945375514\nEpoch: 856 / 1000\nw1: [17.58880954] w2: [-11.37450731] bias: [16.78399391] loss: 40.9722687594251\nEpoch: 857 / 1000\nw1: [17.59591557] w2: [-11.38667043] bias: [16.78395665] loss: 40.95239650421147\nEpoch: 858 / 1000\nw1: [17.60301502] w2: [-11.39882184] bias: [16.78391928] loss: 40.93256210744514\nEpoch: 859 / 1000\nw1: [17.6101079] w2: [-11.41096156] bias: [16.78388182] loss: 40.9127654969744\nEpoch: 860 / 1000\nw1: [17.61719422] w2: [-11.42308959] bias: [16.78384425] loss: 40.89300660078506\nEpoch: 861 / 1000\nw1: [17.62427398] w2: [-11.43520596] bias: [16.78380658] loss: 40.87328534700017\nEpoch: 862 / 1000\nw1: [17.63134719] w2: [-11.44731066] bias: [16.78376882] loss: 40.85360166387977\nEpoch: 863 / 1000\nw1: [17.63841385] w2: [-11.45940371] bias: [16.78373095] loss: 40.83395547982069\nEpoch: 864 / 1000\nw1: [17.64547397] w2: [-11.47148512] bias: [16.78369298] loss: 40.81434672335613\nEpoch: 865 / 1000\nw1: [17.65252756] w2: [-11.48355491] bias: [16.7836549] loss: 40.7947753231556\nEpoch: 866 / 1000\nw1: [17.65957462] w2: [-11.49561308] bias: [16.78361673] loss: 40.77524120802448\nEpoch: 867 / 1000\nw1: [17.66661516] w2: [-11.50765964] bias: [16.78357846] loss: 40.75574430690392\nEpoch: 868 / 1000\nw1: [17.67364919] w2: [-11.51969461] bias: [16.78354009] loss: 40.73628454887042\nEpoch: 869 / 1000\nw1: [17.68067671] w2: [-11.53171799] bias: [16.78350161] loss: 40.716861863135726\nEpoch: 870 / 1000\nw1: [17.68769772] w2: [-11.54372981] bias: [16.78346304] loss: 40.697476179046475\nEpoch: 871 / 1000\nw1: [17.69471224] w2: [-11.55573006] bias: [16.78342437] loss: 40.67812742608395\nEpoch: 872 / 1000\nw1: [17.70172027] w2: [-11.56771876] bias: [16.78338559] loss: 40.658815533863844\nEpoch: 873 / 1000\nw1: [17.70872181] w2: [-11.57969592] bias: [16.78334672] loss: 40.63954043213603\nEpoch: 874 / 1000\nw1: [17.71571688] w2: [-11.59166155] bias: [16.78330774] loss: 40.620302050784225\nEpoch: 875 / 1000\nw1: [17.72270547] w2: [-11.60361567] bias: [16.78326867] loss: 40.601100319825825\nEpoch: 876 / 1000\nw1: [17.7296876] w2: [-11.61555828] bias: [16.7832295] loss: 40.58193516941158\nEpoch: 877 / 1000\nw1: [17.73666327] w2: [-11.62748939] bias: [16.78319023] loss: 40.56280652982536\nEpoch: 878 / 1000\nw1: [17.74363248] w2: [-11.63940902] bias: [16.78315085] loss: 40.54371433148395\nEpoch: 879 / 1000\nw1: [17.75059525] w2: [-11.65131718] bias: [16.78311138] loss: 40.52465850493671\nEpoch: 880 / 1000\nw1: [17.75755158] w2: [-11.66321388] bias: [16.78307181] loss: 40.505638980865406\nEpoch: 881 / 1000\nw1: [17.76450147] w2: [-11.67509912] bias: [16.78303214] loss: 40.486655690083914\nEpoch: 882 / 1000\nw1: [17.77144494] w2: [-11.68697292] bias: [16.78299238] loss: 40.46770856353795\nEpoch: 883 / 1000\nw1: [17.77838198] w2: [-11.69883529] bias: [16.78295251] loss: 40.44879753230484\nEpoch: 884 / 1000\nw1: [17.7853126] w2: [-11.71068625] bias: [16.78291254] loss: 40.42992252759331\nEpoch: 885 / 1000\nw1: [17.79223682] w2: [-11.7225258] bias: [16.78287248] loss: 40.41108348074317\nEpoch: 886 / 1000\nw1: [17.79915463] w2: [-11.73435395] bias: [16.78283232] loss: 40.3922803232251\nEpoch: 887 / 1000\nw1: [17.80606604] w2: [-11.74617071] bias: [16.78279206] loss: 40.373512986640385\nEpoch: 888 / 1000\nw1: [17.81297106] w2: [-11.7579761] bias: [16.7827517] loss: 40.35478140272068\nEpoch: 889 / 1000\nw1: [17.81986969] w2: [-11.76977012] bias: [16.78271124] loss: 40.33608550332776\nEpoch: 890 / 1000\nw1: [17.82676194] w2: [-11.78155279] bias: [16.78267068] loss: 40.31742522045325\nEpoch: 891 / 1000\nw1: [17.83364782] w2: [-11.79332412] bias: [16.78263003] loss: 40.2988004862184\nEpoch: 892 / 1000\nw1: [17.84052733] w2: [-11.80508412] bias: [16.78258928] loss: 40.28021123287384\nEpoch: 893 / 1000\nw1: [17.84740048] w2: [-11.8168328] bias: [16.78254843] loss: 40.26165739279933\nEpoch: 894 / 1000\nw1: [17.85426727] w2: [-11.82857016] bias: [16.78250749] loss: 40.2431388985035\nEpoch: 895 / 1000\nw1: [17.86112771] w2: [-11.84029623] bias: [16.78246644] loss: 40.224655682623606\nEpoch: 896 / 1000\nw1: [17.86798181] w2: [-11.85201101] bias: [16.7824253] loss: 40.206207677925306\nEpoch: 897 / 1000\nw1: [17.87482957] w2: [-11.86371451] bias: [16.78238406] loss: 40.18779481730241\nEpoch: 898 / 1000\nw1: [17.881671] w2: [-11.87540674] bias: [16.78234273] loss: 40.1694170337766\nEpoch: 899 / 1000\nw1: [17.88850611] w2: [-11.88708772] bias: [16.7823013] loss: 40.15107426049724\nEpoch: 900 / 1000\nw1: [17.89533489] w2: [-11.89875745] bias: [16.78225977] loss: 40.13276643074108\nEpoch: 901 / 1000\nw1: [17.90215736] w2: [-11.91041594] bias: [16.78221814] loss: 40.11449347791208\nEpoch: 902 / 1000\nw1: [17.90897352] w2: [-11.92206322] bias: [16.78217642] loss: 40.09625533554109\nEpoch: 903 / 1000\nw1: [17.91578338] w2: [-11.93369928] bias: [16.7821346] loss: 40.078051937285665\nEpoch: 904 / 1000\nw1: [17.92258694] w2: [-11.94532413] bias: [16.78209268] loss: 40.05988321692979\nEpoch: 905 / 1000\nw1: [17.92938422] w2: [-11.9569378] bias: [16.78205067] loss: 40.041749108383684\nEpoch: 906 / 1000\nw1: [17.93617521] w2: [-11.96854028] bias: [16.78200857] loss: 40.02364954568348\nEpoch: 907 / 1000\nw1: [17.94295992] w2: [-11.98013159] bias: [16.78196636] loss: 40.005584462991074\nEpoch: 908 / 1000\nw1: [17.94973836] w2: [-11.99171175] bias: [16.78192406] loss: 39.98755379459383\nEpoch: 909 / 1000\nw1: [17.95651053] w2: [-12.00328075] bias: [16.78188166] loss: 39.96955747490436\nEpoch: 910 / 1000\nw1: [17.96327645] w2: [-12.01483862] bias: [16.78183917] loss: 39.95159543846028\nEpoch: 911 / 1000\nw1: [17.97003611] w2: [-12.02638536] bias: [16.78179659] loss: 39.93366761992395\nEpoch: 912 / 1000\nw1: [17.97678952] w2: [-12.03792098] bias: [16.7817539] loss: 39.915773954082304\nEpoch: 913 / 1000\nw1: [17.98353669] w2: [-12.04944549] bias: [16.78171112] loss: 39.897914375846526\nEpoch: 914 / 1000\nw1: [17.99027762] w2: [-12.06095891] bias: [16.78166825] loss: 39.88008882025189\nEpoch: 915 / 1000\nw1: [17.99701232] w2: [-12.07246124] bias: [16.78162528] loss: 39.86229722245745\nEpoch: 916 / 1000\nw1: [18.0037408] w2: [-12.0839525] bias: [16.78158222] loss: 39.844539517745886\nEpoch: 917 / 1000\nw1: [18.01046306] w2: [-12.09543269] bias: [16.78153906] loss: 39.82681564152318\nEpoch: 918 / 1000\nw1: [18.0171791] w2: [-12.10690183] bias: [16.7814958] loss: 39.809125529318464\nEpoch: 919 / 1000\nw1: [18.02388894] w2: [-12.11835992] bias: [16.78145245] loss: 39.791469116783716\nEpoch: 920 / 1000\nw1: [18.03059258] w2: [-12.12980699] bias: [16.78140901] loss: 39.77384633969358\nEpoch: 921 / 1000\nw1: [18.03729002] w2: [-12.14124303] bias: [16.78136547] loss: 39.7562571339451\nEpoch: 922 / 1000\nw1: [18.04398128] w2: [-12.15266806] bias: [16.78132184] loss: 39.73870143555749\nEpoch: 923 / 1000\nw1: [18.05066635] w2: [-12.16408208] bias: [16.78127811] loss: 39.721179180671925\nEpoch: 924 / 1000\nw1: [18.05734524] w2: [-12.17548512] bias: [16.78123429] loss: 39.70369030555126\nEpoch: 925 / 1000\nw1: [18.06401796] w2: [-12.18687718] bias: [16.78119038] loss: 39.68623474657988\nEpoch: 926 / 1000\nw1: [18.07068452] w2: [-12.19825827] bias: [16.78114637] loss: 39.66881244026334\nEpoch: 927 / 1000\nw1: [18.07734492] w2: [-12.20962839] bias: [16.78110226] loss: 39.65142332322828\nEpoch: 928 / 1000\nw1: [18.08399916] w2: [-12.22098757] bias: [16.78105806] loss: 39.63406733222211\nEpoch: 929 / 1000\nw1: [18.09064725] w2: [-12.23233582] bias: [16.78101377] loss: 39.61674440411275\nEpoch: 930 / 1000\nw1: [18.0972892] w2: [-12.24367313] bias: [16.78096939] loss: 39.59945447588852\nEpoch: 931 / 1000\nw1: [18.10392502] w2: [-12.25499953] bias: [16.78092491] loss: 39.582197484657776\nEpoch: 932 / 1000\nw1: [18.1105547] w2: [-12.26631502] bias: [16.78088034] loss: 39.56497336764877\nEpoch: 933 / 1000\nw1: [18.11717826] w2: [-12.27761961] bias: [16.78083567] loss: 39.54778206220937\nEpoch: 934 / 1000\nw1: [18.1237957] w2: [-12.28891332] bias: [16.78079092] loss: 39.53062350580688\nEpoch: 935 / 1000\nw1: [18.13040703] w2: [-12.30019615] bias: [16.78074606] loss: 39.513497636027786\nEpoch: 936 / 1000\nw1: [18.13701225] w2: [-12.31146811] bias: [16.78070112] loss: 39.4964043905775\nEpoch: 937 / 1000\nw1: [18.14361137] w2: [-12.32272922] bias: [16.78065608] loss: 39.479343707280194\nEpoch: 938 / 1000\nw1: [18.15020439] w2: [-12.33397949] bias: [16.78061095] loss: 39.46231552407853\nEpoch: 939 / 1000\nw1: [18.15679132] w2: [-12.34521892] bias: [16.78056573] loss: 39.445319779033426\nEpoch: 940 / 1000\nw1: [18.16337217] w2: [-12.35644753] bias: [16.78052042] loss: 39.42835641032391\nEpoch: 941 / 1000\nw1: [18.16994694] w2: [-12.36766532] bias: [16.78047501] loss: 39.41142535624678\nEpoch: 942 / 1000\nw1: [18.17651564] w2: [-12.37887231] bias: [16.78042951] loss: 39.39452655521646\nEpoch: 943 / 1000\nw1: [18.18307827] w2: [-12.39006851] bias: [16.78038392] loss: 39.37765994576475\nEpoch: 944 / 1000\nw1: [18.18963484] w2: [-12.40125393] bias: [16.78033823] loss: 39.36082546654061\nEpoch: 945 / 1000\nw1: [18.19618535] w2: [-12.41242857] bias: [16.78029246] loss: 39.34402305630991\nEpoch: 946 / 1000\nw1: [18.20272981] w2: [-12.42359245] bias: [16.78024659] loss: 39.32725265395524\nEpoch: 947 / 1000\nw1: [18.20926823] w2: [-12.43474558] bias: [16.78020063] loss: 39.31051419847569\nEpoch: 948 / 1000\nw1: [18.21580061] w2: [-12.44588797] bias: [16.78015458] loss: 39.293807628986585\nEpoch: 949 / 1000\nw1: [18.22232696] w2: [-12.45701963] bias: [16.78010844] loss: 39.27713288471932\nEpoch: 950 / 1000\nw1: [18.22884728] w2: [-12.46814056] bias: [16.7800622] loss: 39.26048990502108\nEpoch: 951 / 1000\nw1: [18.23536158] w2: [-12.47925079] bias: [16.78001587] loss: 39.243878629354704\nEpoch: 952 / 1000\nw1: [18.24186987] w2: [-12.49035031] bias: [16.77996946] loss: 39.22729899729834\nEpoch: 953 / 1000\nw1: [18.24837215] w2: [-12.50143915] bias: [16.77992295] loss: 39.21075094854534\nEpoch: 954 / 1000\nw1: [18.25486842] w2: [-12.5125173] bias: [16.77987635] loss: 39.19423442290398\nEpoch: 955 / 1000\nw1: [18.26135869] w2: [-12.52358478] bias: [16.77982966] loss: 39.17774936029728\nEpoch: 956 / 1000\nw1: [18.26784297] w2: [-12.53464161] bias: [16.77978288] loss: 39.16129570076272\nEpoch: 957 / 1000\nw1: [18.27432127] w2: [-12.54568778] bias: [16.77973601] loss: 39.144873384452126\nEpoch: 958 / 1000\nw1: [18.28079358] w2: [-12.55672332] bias: [16.77968904] loss: 39.12848235163132\nEpoch: 959 / 1000\nw1: [18.28725992] w2: [-12.56774822] bias: [16.77964199] loss: 39.112122542680034\nEpoch: 960 / 1000\nw1: [18.29372028] w2: [-12.57876251] bias: [16.77959485] loss: 39.095793898091586\nEpoch: 961 / 1000\nw1: [18.30017469] w2: [-12.58976619] bias: [16.77954761] loss: 39.07949635847274\nEpoch: 962 / 1000\nw1: [18.30662313] w2: [-12.60075927] bias: [16.77950029] loss: 39.063229864543445\nEpoch: 963 / 1000\nw1: [18.31306562] w2: [-12.61174176] bias: [16.77945287] loss: 39.04699435713664\nEpoch: 964 / 1000\nw1: [18.31950217] w2: [-12.62271367] bias: [16.77940537] loss: 39.03078977719803\nEpoch: 965 / 1000\nw1: [18.32593277] w2: [-12.63367501] bias: [16.77935778] loss: 39.01461606578587\nEpoch: 966 / 1000\nw1: [18.33235744] w2: [-12.6446258] bias: [16.77931009] loss: 38.99847316407075\nEpoch: 967 / 1000\nw1: [18.33877618] w2: [-12.65556603] bias: [16.77926232] loss: 38.982361013335414\nEpoch: 968 / 1000\nw1: [18.34518899] w2: [-12.66649573] bias: [16.77921445] loss: 38.96627955497447\nEpoch: 969 / 1000\nw1: [18.35159588] w2: [-12.6774149] bias: [16.7791665] loss: 38.95022873049427\nEpoch: 970 / 1000\nw1: [18.35799686] w2: [-12.68832355] bias: [16.77911846] loss: 38.9342084815126\nEpoch: 971 / 1000\nw1: [18.36439193] w2: [-12.6992217] bias: [16.77907033] loss: 38.91821874975857\nEpoch: 972 / 1000\nw1: [18.3707811] w2: [-12.71010934] bias: [16.77902211] loss: 38.90225947707233\nEpoch: 973 / 1000\nw1: [18.37716438] w2: [-12.7209865] bias: [16.7789738] loss: 38.88633060540485\nEpoch: 974 / 1000\nw1: [18.38354176] w2: [-12.73185318] bias: [16.7789254] loss: 38.87043207681779\nEpoch: 975 / 1000\nw1: [18.38991326] w2: [-12.74270939] bias: [16.77887691] loss: 38.8545638334832\nEpoch: 976 / 1000\nw1: [18.39627887] w2: [-12.75355515] bias: [16.77882833] loss: 38.83872581768335\nEpoch: 977 / 1000\nw1: [18.40263862] w2: [-12.76439045] bias: [16.77877967] loss: 38.82291797181052\nEpoch: 978 / 1000\nw1: [18.40899249] w2: [-12.77521532] bias: [16.77873092] loss: 38.807140238366806\nEpoch: 979 / 1000\nw1: [18.4153405] w2: [-12.78602976] bias: [16.77868207] loss: 38.79139255996387\nEpoch: 980 / 1000\nw1: [18.42168265] w2: [-12.79683378] bias: [16.77863314] loss: 38.775674879322736\nEpoch: 981 / 1000\nw1: [18.42801895] w2: [-12.80762739] bias: [16.77858412] loss: 38.75998713927361\nEpoch: 982 / 1000\nw1: [18.43434941] w2: [-12.81841061] bias: [16.77853502] loss: 38.7443292827557\nEpoch: 983 / 1000\nw1: [18.44067402] w2: [-12.82918343] bias: [16.77848582] loss: 38.72870125281692\nEpoch: 984 / 1000\nw1: [18.4469928] w2: [-12.83994588] bias: [16.77843654] loss: 38.71310299261373\nEpoch: 985 / 1000\nw1: [18.45330575] w2: [-12.85069796] bias: [16.77838717] loss: 38.69753444541095\nEpoch: 986 / 1000\nw1: [18.45961288] w2: [-12.86143968] bias: [16.77833771] loss: 38.68199555458154\nEpoch: 987 / 1000\nw1: [18.46591418] w2: [-12.87217106] bias: [16.77828816] loss: 38.66648626360636\nEpoch: 988 / 1000\nw1: [18.47220967] w2: [-12.88289209] bias: [16.77823853] loss: 38.651006516074006\nEpoch: 989 / 1000\nw1: [18.47849936] w2: [-12.89360279] bias: [16.77818881] loss: 38.6355562556806\nEpoch: 990 / 1000\nw1: [18.48478324] w2: [-12.90430318] bias: [16.778139] loss: 38.62013542622956\nEpoch: 991 / 1000\nw1: [18.49106133] w2: [-12.91499326] bias: [16.7780891] loss: 38.60474397163141\nEpoch: 992 / 1000\nw1: [18.49733362] w2: [-12.92567303] bias: [16.77803912] loss: 38.589381835903595\nEpoch: 993 / 1000\nw1: [18.50360013] w2: [-12.93634252] bias: [16.77798905] loss: 38.57404896317024\nEpoch: 994 / 1000\nw1: [18.50986086] w2: [-12.94700173] bias: [16.77793889] loss: 38.558745297661964\nEpoch: 995 / 1000\nw1: [18.51611582] w2: [-12.95765066] bias: [16.77788865] loss: 38.54347078371569\nEpoch: 996 / 1000\nw1: [18.522365] w2: [-12.96828934] bias: [16.77783832] loss: 38.528225365774425\nEpoch: 997 / 1000\nw1: [18.52860842] w2: [-12.97891777] bias: [16.7777879] loss: 38.51300898838708\nEpoch: 998 / 1000\nw1: [18.53484608] w2: [-12.98953596] bias: [16.7777374] loss: 38.49782159620822\nEpoch: 999 / 1000\nw1: [18.54107799] w2: [-13.00014391] bias: [16.77768681] loss: 38.48266313399792\nEpoch: 1000 / 1000\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\n##### 최종 w1, w2, bias #######\n[18.54730416] [-13.01074165] [16.77763613]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n최초 w1, w2, bias: [0.] [0.] [1.]\\nEpoch: 1 / 1000\\nw1: [0.24193162] w2: [0.10311943] bias: [1.43065613] loss: 548.0813043478261\\nEpoch: 2 / 1000\\nw1: [0.47767212] w2: [0.20269304] bias: [1.84955238] loss: 522.964778344195\\nEpoch: 3 / 1000\\nw1: [0.70739021] w2: [0.29881838] bias: [2.25700994] loss: 499.19625820107575\\n~~~\\nEpoch: 1000 / 1000\\nw1: [18.54730416] w2: [-13.01074165] bias: [16.77763613] loss: 38.46753354662152\\n##### 최종 w1, w2, bias #######\\n[18.54730416] [-13.01074165] [16.77763613]\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### 계산된 Weight와 Bias를 이용하여 Price 예측\n* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. ","metadata":{}},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE'] = predicted\nbostonDF.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:00:30.986113Z","iopub.execute_input":"2021-12-27T03:00:30.986377Z","iopub.status.idle":"2021-12-27T03:00:31.023790Z","shell.execute_reply.started":"2021-12-27T03:00:30.986347Z","shell.execute_reply":"2021-12-27T03:00:31.022593Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n\n   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  \n0     15.3  396.90   4.98   24.0        26.322000  \n1     17.8  396.90   9.14   21.6        24.281207  \n2     17.8  392.83   4.03   34.7        28.830886  \n3     18.7  394.63   2.94   33.4        28.557652  \n4     18.7  396.90   5.33   36.2        28.229120  \n5     18.7  394.12   5.21   28.7        25.724125  \n6     15.2  395.60  12.43   22.9        21.646539  \n7     15.2  396.90  19.15   27.1        19.802559  \n8     15.2  386.63  29.93   16.5        14.009758  \n9     15.2  386.71  17.10   18.9        19.941503  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n      <th>PREDICTED_PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n      <td>26.322000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n      <td>24.281207</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n      <td>28.830886</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n      <td>28.557652</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n      <td>28.229120</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.02985</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.430</td>\n      <td>58.7</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.12</td>\n      <td>5.21</td>\n      <td>28.7</td>\n      <td>25.724125</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.08829</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.012</td>\n      <td>66.6</td>\n      <td>5.5605</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>395.60</td>\n      <td>12.43</td>\n      <td>22.9</td>\n      <td>21.646539</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.14455</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.172</td>\n      <td>96.1</td>\n      <td>5.9505</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>396.90</td>\n      <td>19.15</td>\n      <td>27.1</td>\n      <td>19.802559</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.21124</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>5.631</td>\n      <td>100.0</td>\n      <td>6.0821</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.63</td>\n      <td>29.93</td>\n      <td>16.5</td>\n      <td>14.009758</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.17004</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.004</td>\n      <td>85.9</td>\n      <td>6.5921</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.71</td>\n      <td>17.10</td>\n      <td>18.9</td>\n      <td>19.941503</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Keras를 이용하여 보스턴 주택가격 모델 학습 및 예측\n* Dense Layer를 이용하여 퍼셉트론 구현. units는 1로 설정. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\nmodel.fit(scaled_features, bostonDF['PRICE'].values, epochs=1000)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T03:00:35.665562Z","iopub.execute_input":"2021-12-27T03:00:35.665898Z","iopub.status.idle":"2021-12-27T03:00:41.723559Z","shell.execute_reply.started":"2021-12-27T03:00:35.665866Z","shell.execute_reply":"2021-12-27T03:00:41.721587Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2021-12-27 03:00:41.201542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-27 03:00:41.319708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-27 03:00:41.320843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2130656749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ones'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Skip the init in FunctionalModel since model doesn't have input/output yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\n\u001b[0;32m--> 108\u001b[0;31m         name=name, autocast=False)\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sequential'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_masking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_batch_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_model_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_init_batch_counters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# `evaluate`, and `predict`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0magg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONLY_FIRST_REPLICA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     self._predict_counter = tf.Variable(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    241\u001b[0m                         shape=None):\n\u001b[1;32m    242\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1611\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m   def _init_from_args(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1745\u001b[0m             initial_value = ops.convert_to_tensor(initial_value,\n\u001b[1;32m   1746\u001b[0m                                                   \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"initial_value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m                                                   dtype=dtype)\n\u001b[0m\u001b[1;32m   1748\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    534\u001b[0m       \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mconfig_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ContextOptionsSetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_policy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mconfig\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;34m\"\"\"Return the ConfigProto with all runtime deltas applied.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;31m# Ensure physical devices have been discovered and config has been imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m_initialize_physical_devices\u001b[0;34m(self, reinitialize)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_ListPhysicalDevices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       self._physical_devices = [\n\u001b[1;32m   1295\u001b[0m           \u001b[0mPhysicalDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Keras로 학습된 모델을 이용하여 주택 가격 예측 수행. ","metadata":{}},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE'] = predicted\nbostonDF.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SGD 기반으로 Weight/Bias update 값 구하기","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_sgd.shape[0]\n    # 예측 값. \n    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n    # 실제값과 예측값의 차이 \n    diff_sgd = target_sgd - predicted_sgd\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SGD 수행하기","metadata":{}},{"cell_type":"code","source":"print(bostonDF['PRICE'].values.shape)\nprint(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\nprint(np.random.choice(506, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택. \n        stochastic_index = np.random.choice(target.shape[0], 1)\n        rm_sgd = rm[stochastic_index]\n        lstat_sgd = lstat[stochastic_index]\n        target_sgd = target[stochastic_index]\n        # SGD 기반으로 Weight/Bias의 Update를 구함.  \n        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n        \n        # SGD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n\nw1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_SGD'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_batch.shape[0]\n    # 예측 값. \n    predicted_batch = w1 * rm_batch+ w2 * lstat_batch + bias\n    # 실제값과 예측값의 차이 \n    diff_batch = target_batch - predicted_batch\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_batch.T, diff_batch))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_batch.T, diff_batch))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_batch))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_indexes = np.random.choice(506, 30)\nprint(batch_indexes)\n\nbostonDF['RM'].values[batch_indexes]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_random_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 갯수만큼 데이터를 임의로 선택. \n        batch_indexes = np.random.choice(target.shape[0], batch_size)\n        rm_batch = rm[batch_indexes]\n        lstat_batch = lstat[batch_indexes]\n        target_batch = target[batch_indexes]\n        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n        \n        # Batch GD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_random_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH_RANDOM'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"for batch_step in range(0, 506, 30):\n    print(batch_step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bostonDF['PRICE'].values[480:510]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n        for batch_step in range(0, target.shape[0], batch_size):\n            # batch_size만큼 순차적인 데이터를 가져옴. \n            rm_batch = rm[batch_step:batch_step + batch_size]\n            lstat_batch = lstat[batch_step:batch_step + batch_size]\n            target_batch = target[batch_step:batch_step + batch_size]\n        \n            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n\n            # Batch GD로 구한 weight/bias의 update 적용. \n            w1 = w1 - w1_update\n            w2 = w2 - w2_update\n            bias = bias - bias_update\n        \n            if verbose:\n                print('Epoch:', i+1,'/', iter_epochs, 'batch step:', batch_step)\n                # Loss는 전체 학습 데이터 기반으로 구해야 함.\n                predicted = w1 * rm + w2*lstat + bias\n                diff = target - predicted\n                mse_loss = np.mean(np.square(diff))\n                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mini BATCH GD를 Keras로 수행\n* Keras는 기본적으로 Mini Batch GD를 수행","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n\n# Keras는 반드시 Batch GD를 적용함. batch_size가 None이면 32를 할당. \nmodel.fit(scaled_features, bostonDF['PRICE'].values, batch_size=30, epochs=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}